# Notes on RL 20

1. [A Modular Test Bed for Reinforcement Learning Incorporation into Industrial Applications](https://arxiv.org/pdf/2306.01440)
2.  [Adaptive and Explainable Deployment of Navigation Skills via Hierarchical Deep Reinforcement Learning](https://arxiv.org/pdf/2305.19746) INT
3.  [DHRL-FNMR: An Intelligent Multicast Routing Approach Based on Deep Hierarchical Reinforcement Learning in SDN](https://arxiv.org/pdf/2305.19077)
4.  [Simulation and Retargeting of Complex Multi-Character Interactions](https://arxiv.org/pdf/2305.20041)
5. [Deep Q-Learning versus Proximal Policy Optimization: Performance Comparison in a Material Sorting Task](https://arxiv.org/pdf/2306.01451) INT
6.  [Hyperparameters in Reinforcement Learning and How To Tune Them](https://arxiv.org/pdf/2306.01324) INT INT
7.  [Symmetric Exploration in Combinatorial Optimization is Free!](https://arxiv.org/pdf/2306.01276) INT
8.  Explainable/Interpretable Machine Learning
9.  [Interpretable and Explainable Logical Policies via Neurally Guided Symbolic Abstraction](https://arxiv.org/pdf/2306.01439)
10. [Hierarchical Reinforcement Learning for Modeling User Novelty-Seeking Intent in Recommender Systems](https://arxiv.org/pdf/2306.01476)
11. [Identifiability and Generalizability in Constrained Inverse Reinforcement Learning](https://arxiv.org/pdf/2306.00629) INT
12. [NetHack is Hard to Hack](https://arxiv.org/pdf/2305.19240) INT
13. [PAGAR: Imitation Learning with Protagonist Antagonist Guided Adversarial Reward](https://arxiv.org/pdf/2306.01731) INT INT
14. [Offline Meta Reinforcement Learning with In-Distribution Online Adaptation](https://arxiv.org/pdf/2305.19529) INT
15. [Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning](https://arxiv.org/pdf/2305.18459) INT
16. [Safe Offline Reinforcement Learning with Real-Time Budget Constraints](https://arxiv.org/pdf/2306.00603)
17. [Doing the right thing for the right reason: Evaluating artificial moral cognition by probing cost insensitivity](https://arxiv.org/pdf/2305.18269) INT
18. [Intent-aligned AI systems deplete human agency: the need for agency foundations research in AI safety](https://arxiv.org/pdf/2305.19223) INT
19. [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/pdf/2305.18290) INT
20. [Factually Consistent Summarization via Reinforcement Learning with Textual Entailment Feedback](https://arxiv.org/pdf/2306.00186) INT
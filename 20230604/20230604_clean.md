# Notes on RL 20

__89__

Using arxiv scraper

## New papers

1. __Engineering Applications__
   1. Computing and Software Engineering
      1. [BitE : Accelerating Learned Query Optimization in a Mixed-Workload Environment](https://arxiv.org/pdf/2306.00845)
   2. Energy
      1. [Centralised rehearsal of decentralised cooperation: Multi-agent reinforcement learning for the scalable coordination of residential energy flexibility](https://arxiv.org/pdf/2305.18875)
   3. Image & Video
      1. [Controllable Motion Diffusion Model](https://arxiv.org/pdf/2306.00416)
   4. Industrial Applications
      1. [A Modular Test Bed for Reinforcement Learning Incorporationo Industrial Applications](https://arxiv.org/pdf/2306.01440)
      2. [An Architecture for Deploying Reinforcement Learning in Industrial Environments](https://arxiv.org/pdf/2306.01420)
   5.  Navigation
       1.  [Adaptive and Explainable Deployment of Navigation Skills via Hierarchical Deep Reinforcement Learning](https://arxiv.org/pdf/2305.19746)
       2.  [Efficient Learning of Urban Driving Policies Using Bird's-Eye-View State Representations](https://arxiv.org/pdf/2305.19904)
       3.  [Investigating Navigation Strategies in the Morris Water Maze through Deep Reinforcement Learning](https://arxiv.org/pdf/2306.01066)
       4.  [RLAD: Reinforcement Learning from Pixels for Autonomous Driving in Urban Environments](https://arxiv.org/pdf/2305.18510)
   6.  Networks
       1.  [Average AoI Minimization for Energy Harvesting Relay-aided Status Update Network Using Deep Reinforcement Learning](https://arxiv.org/pdf/2306.01251)
       2.  [DHRL-FNMR: Anelligent Multicast Routing Approach Based on Deep Hierarchical Reinforcement Learning in SDN](https://arxiv.org/pdf/2305.19077)
       3.  [Training Terahertz Wireless Systems to Battle I/Q Imbalance](https://arxiv.org/pdf/2306.01611)
   7.  Traffic & Flow
       1.  [Perimeter Control Using Deep Reinforcement Learning: A Model-free Approach towards Homogeneous Flow Rate Optimization](https://arxiv.org/pdf/2305.19291)
   8.  Robotics
       1.  [Active Reinforcement Learning under Limited Visual Observability](https://arxiv.org/pdf/2306.00975)
       2.  [Granular Gym: High Performance Simulation for Robotic Tasks with Granular Materials](https://arxiv.org/pdf/2306.01369)
       3.  [LIV: Language-Image Representations and Rewards for Robotic Control](https://arxiv.org/pdf/2306.00958)
       4.  [Multi-Robot Path Planning Combining Heuristics and Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/2306.01270)
   9.  UAV's
       1.  [A Hybrid Framework of Reinforcement Learning and Convex Optimization for UAV-Based Autonomous Metaverse Data Collection](https://arxiv.org/pdf/2305.18481)
   10. Others
       1.  [Action valuation of on- and off-ball soccer players based on multi-agent deep reinforcement learning](https://arxiv.org/pdf/2305.17886)
       2.  [Simulation and Retargeting of Complex Multi-Charactereractions](https://arxiv.org/pdf/2305.20041)
2.  __Reinforcement Learning Theory__
    1. Actor Critic
       1. [ReLU to the Rescue: Improve Your On-Policy Actor-Critic with Positive Advantages](https://arxiv.org/pdf/2306.01460)
    2. Algorithms
       1. [Deep Q-Learning versus Proximal Policy Optimization: Performance Comparison in a Material Sorting Task](https://arxiv.org/pdf/2306.01451)
       2. [VA-learning as a more efficient alternative to Q-learning](https://arxiv.org/pdf/2305.18161)
    3. Computational Efficiency
       1. [Handling Large Discrete Action Spaces via Dynamic Neighborhood Construction](https://arxiv.org/pdf/2305.19891)
    4.  Continual learning
        1.  [Policy Optimization for Continuous Reinforcement Learning](https://arxiv.org/pdf/2305.18901)
    5.  Control Theory
        1.  [Improving the performance of Learned Controllers in Behavior Trees using Value Function Estimates at Switching Boundaries](https://arxiv.org/pdf/2305.18903)
    6.  Empirical Study of RL
        1.  [Hyperparameters in Reinforcement Learning and How To Tune Them](https://arxiv.org/pdf/2306.01324)
        2.  [Replicability in Reinforcement Learning](https://arxiv.org/pdf/2305.19562)
    7.  Exploration Methods
        1.  [Accelerating Reinforcement Learning with Value-Conditional State Entropy Exploration](https://arxiv.org/pdf/2305.19476)
        2.  [Latent Exploration for Reinforcement Learning](https://arxiv.org/pdf/2305.20065)
        3.  [One Objective to Rule Them All: A Maximization Objective Fusing Estimation and Planning for Exploration](https://arxiv.org/pdf/2305.18258)
        4.  [Provable and Practical: Efficient Exploration in Reinforcement Learning via Langevin Monte Carlo](https://arxiv.org/pdf/2305.18246)
        5.  [Symmetric Exploration in Combinatorial Optimization is Free!](https://arxiv.org/pdf/2306.01276)
    8.  Explainable/Interpretable Machine Learning
        1.  [Interpretable and Explainable Logical Policies via Neurally Guided Symbolic Abstraction](https://arxiv.org/pdf/2306.01439)
    9.  Federated Learning
        1.  [Federated Multi-Sequence Stochastic Approximation with Local Hypergradient Estimation](https://arxiv.org/pdf/2306.01648)
    10. Graph RL
        1.  [CLIPGraphs: Multimodal Graph Networks to Infer Object-Room Affinities](https://arxiv.org/pdf/2306.01540)
        2.  [Subequivariant Graph Reinforcement Learning in 3D Environments](https://arxiv.org/pdf/2305.18951)
    11. Hierarchical RL
        1.  [Hierarchical Reinforcement Learning for Modeling User Novelty-Seekingent in Recommender Systems](https://arxiv.org/pdf/2306.01476)
        2.  [IQL-TD-MPC: Implicit Q-Learning for Hierarchical Model Predictive Control](https://arxiv.org/pdf/2306.00867)
    12. Imitation / Inverse / Demonstration Reinforcement Learning
        1.  [Identifiability and Generalizability in Constrained Inverse Reinforcement Learning](https://arxiv.org/pdf/2306.00629)
        2.  [NetHack is Hard to Hack](https://arxiv.org/pdf/2305.19240)
        3.  [PAGAR: Imitation Learning with Protagonist Antagonist Guided Adversarial Reward](https://arxiv.org/pdf/2306.01731)
    13. Markov Decision Processes / Deep Theory
        1.  [Achieving Fairness in Multi-Agent Markov Decision Processes Using Reinforcement Learning](https://arxiv.org/pdf/2306.00324)
        2.  [Differentially Private Episodic Reinforcement Learning with Heavy-tailed Rewards](https://arxiv.org/pdf/2306.01121)
        3.  [Non-stationary Reinforcement Learning under General Function Approximation](https://arxiv.org/pdf/2306.00861)
    14. Meta Learning
        1.  [Offline Meta Reinforcement Learning with In-Distribution Online Adaptation](https://arxiv.org/pdf/2305.19529)
    15. Model-based
        1.  [Pre-training Contextualized World Models with In-the-wild Videos for Reinforcement Learning](https://arxiv.org/pdf/2305.18499)
    16. Modular RL
        1.  [Augmented Modular Reinforcement Learning based on Heterogeneous Knowledge](https://arxiv.org/pdf/2306.01158)
    17. Multi-Agent RL
        1.  [AccMER: Accelerating Multi-Agent Experience Replay with Cache Locality-aware Prioritization](https://arxiv.org/pdf/2306.00187)
    18. Multi-Task RL
        1.  [Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning](https://arxiv.org/pdf/2305.18459)
        2.  [Independent Component Alignment for Multi-Task Learning](https://arxiv.org/pdf/2305.19000)
    19. Offline RL
        1.  [A Convex Relaxation Approach to Bayesian Regret Minimization in Offline Bandits](https://arxiv.org/pdf/2306.01237)
        2.  [Delphic Offline Reinforcement Learning under Nonidentifiable Hidden Confounding](https://arxiv.org/pdf/2306.01157)
        3.  [Efficient Diffusion Policies for Offline Reinforcement Learning](https://arxiv.org/pdf/2305.20081)
        4.  [Improving and Benchmarking Offline Reinforcement Learning Algorithms](https://arxiv.org/pdf/2306.00972)
        5.  [MetaDiffuser: Diffusion Model as Conditional Planner for Offline Meta-RL](https://arxiv.org/pdf/2305.19923)
    20. Quality Diverse RL
        1.  [Generating Behaviorally Diverse Policies with Latent Diffusion Models](https://arxiv.org/pdf/2305.18738)
    21. Quantum Computing
        1.  [Quafu-RL: The Cloud Quantum Computers based Quantum Reinforcement Learning](https://arxiv.org/pdf/2305.17966)
    22. Reinforcement Learning from Human Preferences/Feedback
        1.  [Fine-Grained Human Feedback Gives Better Rewards for Language Model Training](https://arxiv.org/pdf/2306.01693)
        2.  [How to Query Human Feedback Efficiently in RL?](https://arxiv.org/pdf/2305.18505)
    23. Representation Learning
        1.  [Representation-Driven Reinforcement Learning](https://arxiv.org/pdf/2305.19922)
        2.  [Towards a Better Understanding of Representation Dynamics under TD-learning](https://arxiv.org/pdf/2305.18491)
    24. Risk-sensitive/safe/constrained RL
        1.  [Provably Efficient Generalized Lagrangian Policy Optimization for Safe Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/2306.00212)
        2.  [ROSARL: Reward-Only Safe Reinforcement Learning](https://arxiv.org/pdf/2306.00035)
        3.  [Safe Offline Reinforcement Learning with Real-Time Budget Constraints](https://arxiv.org/pdf/2306.00603)
    25. Robust RL
        1.  [Efficient RL with Impaired Observability: Learning to Act with Delayed and Missing State Observations](https://arxiv.org/pdf/2306.01243)
        2.  [Learning for Edge-Weighted Online Bipartite Matching with Robustness Guarantees](https://arxiv.org/pdf/2306.00172)
        3.  [Solving Robust MDPs through No-Regret Dynamics](https://arxiv.org/pdf/2305.19035)
    26. Sim-to-real
        1.  [Privileged Knowledge Distillation for Sim-to-Real Policy Generalization](https://arxiv.org/pdf/2305.18464)
    27. Transformers
        1.  [Blockwise Parallel Transformer for Long Context Large Models](https://arxiv.org/pdf/2305.19370)
    28. Vision-Language Models
        1.  [Test-Time Adaptation with CLIP Reward for Zero-Shot Generalization in Vision-Language Models](https://arxiv.org/pdf/2305.18010)
    29. Visual RL
        1.  [Normalization Enhances Generalization in Visual Reinforcement Learning](https://arxiv.org/pdf/2306.00656)
    30. Others
        1.  [TorchRL: A data-driven decision-making library for PyTorch](https://arxiv.org/pdf/2306.00577)
3. __Recommender Systems__
   1. [Robust Reinforcement Learning Objectives for Sequential Recommender Systems](https://arxiv.org/pdf/2305.18820)
4. __Games and Game Theory__
   1. [Score-Based Equilibrium Learning in Multi-Player Finite Games with Imperfect Information](https://arxiv.org/pdf/2306.00350)
   2. [What model does MuZero learn?](https://arxiv.org/pdf/2306.00840)
5.  __Philosophy and Ethics__
    1.  [Doing the right thing for the right reason: Evaluating artificial moral cognition by probing cost insensitivity](https://arxiv.org/pdf/2305.18269)
    2.  [Intent-aligned AI systems deplete human agency: the need for agency foundations research in AI safety](https://arxiv.org/pdf/2305.19223)
6.  __Healthcare Applications__
    1.  [Deep Reinforcement Learning Framework for Thoracic Diseases Classification via Prior Knowledge Guidance](https://arxiv.org/pdf/2306.01232)
    2.  [FRAMM: Fair Ranking with Missing Modalities for Clinical Trial Site Selection](https://arxiv.org/pdf/2305.19407)
    3.  [Multi-environment lifelong deep reinforcement learning for medical imaging](https://arxiv.org/pdf/2306.00188)
7.  __Natural Language Processing__
    1.  [Adapting Pre-trained Language Models to Vision-Language Tasks via Dynamic Visual Prompting](https://arxiv.org/pdf/2306.00409)
    2.  [Adversarial learning of neural user simulators for dialogue policy optimisation](https://arxiv.org/pdf/2306.00858)
    3.  [Chatting Makes Perfect -- Chat-based Image Retrieval](https://arxiv.org/pdf/2305.20062)
    4.  [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/pdf/2305.18290)
    5.  [Factually Consistent Summarization via Reinforcement Learning with Textual Entailment Feedback](https://arxiv.org/pdf/2306.00186)
    6.  [Thought Cloning: Learning to Think while Acting by Imitating Human Thinking](https://arxiv.org/pdf/2306.00323)
# Review of Abstracts

## Game Theory

Towards Automating Codenames Spymasters with Deep Reinforcement Learning

    multi-player co-operative games

    Although traditional multi-agent reinforcement learning (RL) techniques tend to work well when RL agents work with each other, they fail to work well when co-operating with humans (Siu et al., 2021; Bakhtin et al., 2021)

    text-based games

    problems: 

    main challenges with text-based games is the large action space available to agents

    common sense about how the world works

    Codenames is a good benchmark for both human-AI co-operation and text-based reinforcement learning,

    VERY interesting

Warmth and Competence in Human-Agent Cooperation

    Human choose the Reinforcement Learning Agent they want to play with based on their interaction

    Using game of Coins

    We recommend human-agent interaction researchers routinely incorporate the measurement of social perception and subjective preferences into their studies

    VERY interesting

Independent and Decentralized Learning in Markov Potential Games

    Uses MARL

    Focus on the independent and decentralized setting, where players can only observe the realized state and their own reward in every stage

    We prove that the policies induced by our learning dynamics converge to a stationary Nash equilibrium in Markov potential games with probability 1.

    VERY interesting

## Reinforcement Learning Theory

Towards Learning Abstractions via Reinforcement Learning

    Synthesis of efficient communication schemes in multi-agent systems trained by RL

    Combination of symbolic methods and machine Learning: neuro-symbolic system

    Reinforcement Learning is interleaved with steps to extend the current language with novel higher level concepts

    We demonstrate that this approach allows agents to converge quickly on a small collaborative construction task

    VERY interesting

Backward Curriculum Reinforcement Learning

    Reverse curriculum learning starts training the agent using the backward trajectory of the episode rather than the original forward trajectory.

    This gives the agent a strong reward signal, so the agent can learn in a more sample-efficient manner.

    our method only requires a minor change in algorithm, which is reversing the order of trajectory before training the agent. Therefore, it can be simply applied to any state-of-art algorithms.

    VERY interesting

Outcome-Driven Reinforcement Learning via Variational Inference

    we view reinforcement learning as inferring policies that achieve desired outcomes, rather than as a problem of maximizing rewards

    VERY interesting
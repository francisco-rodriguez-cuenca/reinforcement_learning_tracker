{
    "A Finite Sample Complexity Bound for Distributionally Robust Q-learning": {
        "abstract": "We consider a reinforcement learning setting in which the deployment environment is different from the training environment. Applying a robust Markov decision processes formulation, we extend the distributionally robust $Q$-learning framework studied in Liu et al. [2022]. Further, we improve the design and analysis of their multi-level Monte Carlo estimator. Assuming access to a simulator, we prove that the worst-case expected sample complexity of our algorithm to learn the optimal robust $Q$-function within an $\u03b5$ error in the sup norm is upper bounded by $\\tilde O(|S||A|(1-\u03b3)^{-5}\u03b5^{-2}p_{\\wedge}^{-6}\u03b4^{-4})$, where $\u03b3$ is the discount rate, $p_{\\wedge}$ is the non-zero minimal support probability of the transition kernels and $\u03b4$ is the uncertainty size. This is the first sample complexity result for the model-free robust RL problem. Simulation studies further validate our theoretical results.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.13203",
        "string": "[A Finite Sample Complexity Bound for Distributionally Robust Q-learning](https://arxiv.org/pdf/2302.13203)"
    },
    "A Human-Centered Safe Robot Reinforcement Learning Framework with Interactive Behaviors": {
        "abstract": "Deployment of reinforcement learning algorithms for robotics applications in the real world requires ensuring the safety of the robot and its environment. Safe robot reinforcement learning (SRRL) is a crucial step towards achieving human-robot coexistence. In this paper, we envision a human-centered SRRL framework consisting of three stages: safe exploration, safety value alignment, and safe collaboration. We examine the research gaps in these areas and propose to leverage interactive behaviors for SRRL. Interactive behaviors enable bi-directional information transfer between humans and robots, such as conversational robot ChatGPT. We argue that interactive behaviors need further attention from the SRRL community. We discuss four open challenges related to the robustness, efficiency, transparency, and adaptability of SRRL with interactive behaviors.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.13137",
        "string": "[A Human-Centered Safe Robot Reinforcement Learning Framework with Interactive Behaviors](https://arxiv.org/pdf/2302.13137)"
    },
    "A Reinforcement Learning Framework for Online Speaker Diarization": {
        "abstract": "Speaker diarization is a task to label an audio or video recording with the identity of the speaker at each given time stamp. In this work, we propose a novel machine learning framework to conduct real-time multi-speaker diarization and recognition without prior registration and pretraining in a fully online and reinforcement learning setting. Our framework combines embedding extraction, clustering, and resegmentation into the same problem as an online decision-making problem. We discuss practical considerations and advanced techniques such as the offline reinforcement learning, semi-supervision, and domain adaptation to address the challenges of limited training data and out-of-distribution environments. Our approach considers speaker diarization as a fully online learning problem of the speaker recognition task, where the agent receives no pretraining from any training set before deployment, and learns to detect speaker identity on the fly through reward feedbacks. The paradigm of the reinforcement learning approach to speaker diarization presents an adaptive, lightweight, and generalizable system that is useful for multi-user teleconferences, where many people might come and go without extensive pre-registration ahead of time. Lastly, we provide a desktop application that uses our proposed approach as a proof of concept. To the best of our knowledge, this is the first approach to apply a reinforcement learning approach to the speaker diarization task.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.10924",
        "string": "[A Reinforcement Learning Framework for Online Speaker Diarization](https://arxiv.org/pdf/2302.10924)"
    },
    "A Supervisory Learning Control Framework for Autonomous & Real-time Task Planning for an Underactuated Cooperative Robotic task": {
        "abstract": "We introduce a framework for cooperative manipulation, applied on an underactuated manipulation problem. Two stationary robotic manipulators are required to cooperate in order to reposition an object within their shared work space. Control of multi-agent systems for manipulation tasks cannot rely on individual control strategies with little to no communication between the agents that serve the common objective through swarming. Instead a coordination strategy is required that queries subtasks to the individual agents. We formulate the problem in a Task And Motion Planning (TAMP) setting, while considering a decomposition strategy that allows us to treat the task and motion planning problems separately. We solve the supervisory planning problem offline using deep Reinforcement Learning techniques resulting into a supervisory policy capable of coordinating the two manipulators into a successful execution of the pick-and-place task. Additionally, a benefit of solving the task planning problem offline is the possibility of real-time (re)planning, demonstrating robustness in the event of subtask execution failure or on-the-fly task changes. The framework achieved zero-shot deployment on the real setup with a success rate that is higher than 90%.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.11491",
        "string": "[A Supervisory Learning Control Framework for Autonomous & Real-time Task Planning for an Underactuated Cooperative Robotic task](https://arxiv.org/pdf/2302.11491)"
    },
    "AC2C: Adaptively Controlled Two-Hop Communication for Multi-Agent Reinforcement Learning": {
        "abstract": "Learning communication strategies in cooperative multi-agent reinforcement learning (MARL) has recently attracted intensive attention. Early studies typically assumed a fully-connected communication topology among agents, which induces high communication costs and may not be feasible. Some recent works have developed adaptive communication strategies to reduce communication overhead, but these methods cannot effectively obtain valuable information from agents that are beyond the communication range. In this paper, we consider a realistic communication model where each agent has a limited communication range, and the communication topology dynamically changes. To facilitate effective agent communication, we propose a novel communication protocol called Adaptively Controlled Two-Hop Communication (AC2C). After an initial local communication round, AC2C employs an adaptive two-hop communication strategy to enable long-range information exchange among agents to boost performance, which is implemented by a communication controller. This controller determines whether each agent should ask for two-hop messages and thus helps to reduce the communication overhead during distributed execution. We evaluate AC2C on three cooperative multi-agent tasks, and the experimental results show that it outperforms relevant baselines with lower communication costs.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.12515",
        "string": "[AC2C: Adaptively Controlled Two-Hop Communication for Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/2302.12515)"
    },
    "Adversarial Model for Offline Reinforcement Learning": {
        "abstract": "We propose a novel model-based offline Reinforcement Learning (RL) framework, called Adversarial Model for Offline Reinforcement Learning (ARMOR), which can robustly learn policies to improve upon an arbitrary reference policy regardless of data coverage. ARMOR is designed to optimize policies for the worst-case performance relative to the reference policy through adversarially training a Markov decision process model. In theory, we prove that ARMOR, with a well-tuned hyperparameter, can compete with the best policy within data coverage when the reference policy is supported by the data. At the same time, ARMOR is robust to hyperparameter choices: the policy learned by ARMOR, with \"any\" admissible hyperparameter, would never degrade the performance of the reference policy, even when the reference policy is not covered by the dataset. To validate these properties in practice, we design a scalable implementation of ARMOR, which by adversarial training, can optimize policies without using model ensembles in contrast to typical model-based methods. We show that ARMOR achieves competent performance with both state-of-the-art offline model-free and model-based RL algorithms and can robustly improve the reference policy over various hyperparameter choices.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.11048",
        "string": "[Adversarial Model for Offline Reinforcement Learning](https://arxiv.org/pdf/2302.11048)"
    },
    "Assessment of Reinforcement Learning for Macro Placement": {
        "abstract": "We provide open, transparent implementation and assessment of Google Brain's deep reinforcement learning approach to macro placement and its Circuit Training (CT) implementation in GitHub. We implement in open source key \"blackbox\" elements of CT, and clarify discrepancies between CT and Nature paper. New testcases on open enablements are developed and released. We assess CT alongside multiple alternative macro placers, with all evaluation flows and related scripts public in GitHub. Our experiments also encompass academic mixed-size placement benchmarks, as well as ablation and stability studies. We comment on the impact of Nature and CT, as well as directions for future research.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.11014",
        "string": "[Assessment of Reinforcement Learning for Macro Placement](https://arxiv.org/pdf/2302.11014)"
    },
    "Asymptotically Unbiased Off-Policy Policy Evaluation when Reusing Old Data in Nonstationary Environments": {
        "abstract": "In this work, we consider the off-policy policy evaluation problem for contextual bandits and finite horizon reinforcement learning in the nonstationary setting. Reusing old data is critical for policy evaluation, but existing estimators that reuse old data introduce large bias such that we can not obtain a valid confidence interval. Inspired from a related field called survey sampling, we introduce a variant of the doubly robust (DR) estimator, called the regression-assisted DR estimator, that can incorporate the past data without introducing a large bias. The estimator unifies several existing off-policy policy evaluation methods and improves on them with the use of auxiliary information and a regression approach. We prove that the new estimator is asymptotically unbiased, and provide a consistent variance estimator to a construct a large sample confidence interval. Finally, we empirically show that the new estimator improves estimation for the current and future policy values, and provides a tight and valid interval estimation in several nonstationary recommendation environments.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.11725",
        "string": "[Asymptotically Unbiased Off-Policy Policy Evaluation when Reusing Old Data in Nonstationary Environments](https://arxiv.org/pdf/2302.11725)"
    },
    "Autonomous Exploration and Mapping for Mobile Robots via Cumulative Curriculum Reinforcement Learning": {
        "abstract": "Deep reinforcement learning (DRL) has been widely applied in autonomous exploration and mapping tasks, but often struggles with the challenges of sampling efficiency, poor adaptability to unknown map sizes, and slow simulation speed. To speed up convergence, we combine curriculum learning (CL) with DRL, and first propose a Cumulative Curriculum Reinforcement Learning (CCRL) training framework to alleviate the issue of catastrophic forgetting faced by general CL. Besides, we present a novel state representation, which considers a local egocentric map and a global exploration map resized to the fixed dimension, so as to flexibly adapt to environments with various sizes and shapes. Additionally, for facilitating the fast training of DRL models, we develop a lightweight grid-based simulator, which can substantially accelerate simulation compared to popular robot simulation platforms such as Gazebo. Based on the customized simulator, comprehensive experiments have been conducted, and the results show that the CCRL framework not only mitigates the catastrophic forgetting problem, but also improves the sample efficiency and generalization of DRL models, compared to general CL as well as without a curriculum. Our code is available at https://github.com/BeamanLi/CCRL_Exploration.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.13025",
        "string": "[Autonomous Exploration and Mapping for Mobile Robots via Cumulative Curriculum Reinforcement Learning](https://arxiv.org/pdf/2302.13025)"
    },
    "Backstepping Temporal Difference Learning": {
        "abstract": "Off-policy learning ability is an important feature of reinforcement learning (RL) for practical applications. However, even one of the most elementary RL algorithms, temporal-difference (TD) learning, is known to suffer form divergence issue when the off-policy scheme is used together with linear function approximation. To overcome the divergent behavior, several off-policy TD-learning algorithms, including gradient-TD learning (GTD), and TD-learning with correction (TDC), have been developed until now. In this work, we provide a unified view of such algorithms from a purely control-theoretic perspective, and propose a new convergent algorithm. Our method relies on the backstepping technique, which is widely used in nonlinear control theory.\n  Finally, convergence of the proposed algorithm is experimentally verified in environments where the standard TD-learning is known to be unstable.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.09875",
        "string": "[Backstepping Temporal Difference Learning](https://arxiv.org/pdf/2302.09875)"
    },
    "Behavior Proximal Policy Optimization": {
        "abstract": "Offline reinforcement learning (RL) is a challenging setting where existing off-policy actor-critic methods perform poorly due to the overestimation of out-of-distribution state-action pairs. Thus, various additional augmentations are proposed to keep the learned policy close to the offline dataset (or the behavior policy). In this work, starting from the analysis of offline monotonic policy improvement, we get a surprising finding that some online on-policy algorithms are naturally able to solve offline RL. Specifically, the inherent conservatism of these on-policy algorithms is exactly what the offline RL method needs to overcome the overestimation. Based on this, we propose Behavior Proximal Policy Optimization (BPPO), which solves offline RL without any extra constraint or regularization introduced compared to PPO. Extensive experiments on the D4RL benchmark indicate this extremely succinct method outperforms state-of-the-art offline RL algorithms. Our implementation is available at https://github.com/Dragon-Zhuang/BPPO.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.11312",
        "string": "[Behavior Proximal Policy Optimization](https://arxiv.org/pdf/2302.11312)"
    },
    "Combining search strategies to improve performance in the calibration of economic ABMs": {
        "abstract": "Calibrating agent-based models (ABMs) in economics and finance typically involves a derivative-free search in a very large parameter space. In this work, we benchmark a number of search methods in the calibration of a well-known macroeconomic ABM on real data, and further assess the performance of \"mixed strategies\" made by combining different methods. We find that methods based on random-forest surrogates are particularly efficient, and that combining search methods generally increases performance since the biases of any single method are mitigated. Moving from these observations, we propose a reinforcement learning (RL) scheme to automatically select and combine search methods on-the-fly during a calibration run. The RL agent keeps exploiting a specific method only as long as this keeps performing well, but explores new strategies when the specific method reaches a performance plateau. The resulting RL search scheme outperforms any other method or method combination tested, and does not rely on any prior information or trial and error procedure.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.11835",
        "string": "[Combining search strategies to improve performance in the calibration of economic ABMs](https://arxiv.org/pdf/2302.11835)"
    },
    "Concept Learning for Interpretable Multi-Agent Reinforcement Learning": {
        "abstract": "Multi-agent robotic systems are increasingly operating in real-world environments in close proximity to humans, yet are largely controlled by policy models with inscrutable deep neural network representations. We introduce a method for incorporating interpretable concepts from a domain expert into models trained through multi-agent reinforcement learning, by requiring the model to first predict such concepts then utilize them for decision making. This allows an expert to both reason about the resulting concept policy models in terms of these high-level concepts at run-time, as well as intervene and correct mispredictions to improve performance. We show that this yields improved interpretability and training stability, with benefits to policy performance and sample efficiency in a simulated and real-world cooperative-competitive multi-agent game.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.12232",
        "string": "[Concept Learning for Interpretable Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/2302.12232)"
    },
    "Conditioning Hierarchical Reinforcement Learning on Flexible Constraints": {
        "abstract": "Safety in goal directed Reinforcement Learning (RL) settings has typically been handled through constraints over trajectories and have demonstrated good performance in primarily short horizon tasks (goal is not too far away). In this paper, we are specifically interested in the problem of solving temporally extended decision making problems such as (1) robots that have to clean different areas in a house while avoiding slippery and unsafe areas (e.g., stairs) and retaining enough charge to move to a charging dock; (2) autonomous electric vehicles that have to reach a far away destination while having to optimize charging locations along the way; in the presence of complex safety constraints. Our key contribution is a (safety) Constrained Planning with Reinforcement Learning (CoP-RL) mechanism that combines a high-level constrained planning agent (which computes a reward maximizing path from a given start to a far away goal state while satisfying cost constraints) with a low-level goal conditioned RL agent (which estimates cost and reward values to move between nearby states). A major advantage of CoP-RL is that it can handle constraints on the cost value distribution (e.g., on Conditional Value at Risk, CVaR, and also on expected value). We perform extensive experiments with different types of safety constraints to demonstrate the utility of our approach over leading best approaches in constrained and hierarchical RL.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.10639",
        "string": "[Conditioning Hierarchical Reinforcement Learning on Flexible Constraints](https://arxiv.org/pdf/2302.10639)"
    },
    "Constrained Reinforcement Learning for Stochastic Dynamic Optimal Power Flow Control": {
        "abstract": "Deep Reinforcement Learning (DRL) has become a popular method for solving control problems in power systems. Conventional DRL encourages the agent to explore various policies encoded in a neural network (NN) with the goal of maximizing the reward function. However, this approach can lead to infeasible solutions that violate physical constraints such as power flow equations, voltage limits, and dynamic constraints. Ensuring these constraints are met is crucial in power systems, as they are a safety critical infrastructure. To address this issue, existing DRL algorithms remedy the problem by projecting the actions onto the feasible set, which can result in sub-optimal solutions. This paper presents a novel primal-dual approach for learning optimal constrained DRL policies for dynamic optimal power flow problems, with the aim of controlling power generations and battery outputs. We also prove the convergence of the critic and actor networks. Our case studies on IEEE standard systems demonstrate the superiority of the proposed approach in dynamically adapting to the environment while maintaining safety constraints.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.10382",
        "string": "[Constrained Reinforcement Learning for Stochastic Dynamic Optimal Power Flow Control](https://arxiv.org/pdf/2302.10382)"
    },
    "Curiosity-driven Exploration in Sparse-reward Multi-agent Reinforcement Learning": {
        "abstract": "Sparsity of rewards while applying a deep reinforcement learning method negatively affects its sample-efficiency. A viable solution to deal with the sparsity of rewards is to learn via intrinsic motivation which advocates for adding an intrinsic reward to the reward function to encourage the agent to explore the environment and expand the sample space. Though intrinsic motivation methods are widely used to improve data-efficient learning in the reinforcement learning model, they also suffer from the so-called detachment problem. In this article, we discuss the limitations of intrinsic curiosity module in sparse-reward multi-agent reinforcement learning and propose a method called I-Go-Explore that combines the intrinsic curiosity module with the Go-Explore framework to alleviate the detachment problem.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.10825",
        "string": "[Curiosity-driven Exploration in Sparse-reward Multi-agent Reinforcement Learning](https://arxiv.org/pdf/2302.10825)"
    },
    "DSL-Assembly: A Robust and Safe Assembly Strategy": {
        "abstract": "A reinforcement learning (RL) based method that enables the robot to accomplish the assembly-type task with safety regulations is proposed. The overall strategy consists of grasping and assembly, and this paper mainly considers the assembly strategy. Force feedback is used instead of visual feedback to perceive the shape and direction of the hole in this paper. Furthermore, multiple models based on different sensors are trained for different environments due to environmental perturbations and equipment failures (failures of cameras and other sensors) in the real world. Then, since the emergency stop is triggered when the force output by the robot is too large, a force-based dynamic safety lock (DSL) is proposed to limit the pressing force of the robot. Finally, we train and test the robot model with a simulator and build ablation experiments to illustrate the effectiveness of our method. The models are independently tested 500 times in the simulator, giving a 58.91% success rate with a 4mm gap. These models are transferred to the real world and deployed on a real robot. Simulation environments: https://github.com/0707yiliu/peg-in-hole-with-RL.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.10842",
        "string": "[DSL-Assembly: A Robust and Safe Assembly Strategy](https://arxiv.org/pdf/2302.10842)"
    },
    "Deep Reinforcement Learning Based on Local GNN for Goal-conditioned Deformable Object Rearranging": {
        "abstract": "Object rearranging is one of the most common deformable manipulation tasks, where the robot needs to rearrange a deformable object into a goal configuration. Previous studies focus on designing an expert system for each specific task by model-based or data-driven approaches and the application scenarios are therefore limited. Some research has been attempting to design a general framework to obtain more advanced manipulation capabilities for deformable rearranging tasks, with lots of progress achieved in simulation. However, transferring from simulation to reality is difficult due to the limitation of the end-to-end CNN architecture. To address these challenges, we design a local GNN (Graph Neural Network) based learning method, which utilizes two representation graphs to encode keypoints detected from images. Self-attention is applied for graph updating and cross-attention is applied for generating manipulation actions. Extensive experiments have been conducted to demonstrate that our framework is effective in multiple 1-D (rope, rope ring) and 2-D (cloth) rearranging tasks in simulation and can be easily transferred to a real robot by fine-tuning a keypoint detector.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.10446",
        "string": "[Deep Reinforcement Learning Based on Local GNN for Goal-conditioned Deformable Object Rearranging](https://arxiv.org/pdf/2302.10446)"
    },
    "Deep Reinforcement Learning for Cost-Effective Medical Diagnosis": {
        "abstract": "Dynamic diagnosis is desirable when medical tests are costly or time-consuming. In this work, we use reinforcement learning (RL) to find a dynamic policy that selects lab test panels sequentially based on previous observations, ensuring accurate testing at a low cost. Clinical diagnostic data are often highly imbalanced; therefore, we aim to maximize the $F_1$ score instead of the error rate. However, optimizing the non-concave $F_1$ score is not a classic RL problem, thus invalidates standard RL methods. To remedy this issue, we develop a reward shaping approach, leveraging properties of the $F_1$ score and duality of policy optimization, to provably find the set of all Pareto-optimal policies for budget-constrained $F_1$ score maximization. To handle the combinatorially complex state space, we propose a Semi-Model-based Deep Diagnosis Policy Optimization (SM-DDPO) framework that is compatible with end-to-end training and online learning. SM-DDPO is tested on diverse clinical tasks: ferritin abnormality detection, sepsis mortality prediction, and acute kidney injury diagnosis. Experiments with real-world data validate that SM-DDPO trains efficiently and identifies all Pareto-front solutions. Across all tasks, SM-DDPO is able to achieve state-of-the-art diagnosis accuracy (in some cases higher than conventional methods) with up to $85\\%$ reduction in testing cost. The code is available at [https://github.com/Zheng321/Blood_Panel].\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.10261",
        "string": "[Deep Reinforcement Learning for Cost-Effective Medical Diagnosis](https://arxiv.org/pdf/2302.10261)"
    },
    "Deep Reinforcement Learning for Robotic Pushing and Picking in Cluttered Environment": {
        "abstract": "In this paper, a novel robotic grasping system is established to automatically pick up objects in cluttered scenes. A composite robotic hand composed of a suction cup and a gripper is designed for grasping the object stably. The suction cup is used for lifting the object from the clutter first and the gripper for grasping the object accordingly. We utilize the affordance map to provide pixel-wise lifting point candidates for the suction cup. To obtain a good affordance map, the active exploration mechanism is introduced to the system. An effective metric is designed to calculate the reward for the current affordance map, and a deep Q-Network (DQN) is employed to guide the robotic hand to actively explore the environment until the generated affordance map is suitable for grasping. Experimental results have demonstrated that the proposed robotic grasping system is able to greatly increase the success rate of the robotic grasping in cluttered scenes.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.10717",
        "string": "[Deep Reinforcement Learning for Robotic Pushing and Picking in Cluttered Environment](https://arxiv.org/pdf/2302.10717)"
    },
    "Deep reinforced learning heuristic tested on spin-glass ground states: The larger picture": {
        "abstract": "In Changjun Fan et al. [Nature Communications https://doi.org/10.1038/s41467-023-36363-w (2023)], the authors present a deep reinforced learning approach to augment combinatorial optimization heuristics. In particular, they present results for several spin glass ground state problems, for which instances on non-planar networks are generally NP-hard, in comparison with several Monte Carlo based methods, such as simulated annealing (SA) or parallel tempering (PT). Indeed, those results demonstrate that the reinforced learning improves the results over those obtained with SA or PT, or at least allows for reduced runtimes for the heuristics before results of comparable quality have been obtained relative to those other methods. To facilitate the conclusion that their method is ''superior'', the authors pursue two basic strategies: (1) A commercial GUROBI solver is called on to procure a sample of exact ground states as a testbed to compare with, and (2) a head-to-head comparison between the heuristics is given for a sample of larger instances where exact ground states are hard to ascertain. Here, we put these studies into a larger context, showing that the claimed superiority is at best marginal for smaller samples and becomes essentially irrelevant with respect to any sensible approximation of true ground states in the larger samples. For example, this method becomes irrelevant as a means to determine stiffness exponents $\u03b8$ in $d>2$, as mentioned by the authors, where the problem is not only NP-hard but requires the subtraction of two almost equal ground-state energies and systemic errors in each of $\\approx 1\\%$ found here are unacceptable. This larger picture on the method arises from a straightforward finite-size corrections study over the spin glass ensembles the authors employ, using data that has been available for decades.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.10848",
        "string": "[Deep reinforced learning heuristic tested on spin-glass ground states: The larger picture](https://arxiv.org/pdf/2302.10848)"
    },
    "DeepCPG Policies for Robot Locomotion": {
        "abstract": "Central Pattern Generators (CPGs) form the neural basis of the observed rhythmic behaviors for locomotion in legged animals. The CPG dynamics organized into networks allow the emergence of complex locomotor behaviors. In this work, we take this inspiration for developing walking behaviors in multi-legged robots. We present novel DeepCPG policies that embed CPGs as a layer in a larger neural network and facilitate end-to-end learning of locomotion behaviors in deep reinforcement learning (DRL) setup. We demonstrate the effectiveness of this approach on physics engine-based insectoid robots. We show that, compared to traditional approaches, DeepCPG policies allow sample-efficient end-to-end learning of effective locomotion strategies even in the case of high-dimensional sensor spaces (vision). We scale the DeepCPG policies using a modular robot configuration and multi-agent DRL. Our results suggest that gradual complexification with embedded priors of these policies in a modular fashion could achieve non-trivial sensor and motor integration on a robot platform. These results also indicate the efficacy of bootstrapping more complex intelligent systems from simpler ones based on biological principles. Finally, we present the experimental results for a proof-of-concept insectoid robot system for which DeepCPG learned policies initially using the simulation engine and these were afterwards transferred to real-world robots without any additional fine-tuning.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.13191",
        "string": "[DeepCPG Policies for Robot Locomotion](https://arxiv.org/pdf/2302.13191)"
    },
    "Demonstration-Guided Reinforcement Learning with Efficient Exploration for Task Automation of Surgical Robot": {
        "abstract": "Task automation of surgical robot has the potentials to improve surgical efficiency. Recent reinforcement learning (RL) based approaches provide scalable solutions to surgical automation, but typically require extensive data collection to solve a task if no prior knowledge is given. This issue is known as the exploration challenge, which can be alleviated by providing expert demonstrations to an RL agent. Yet, how to make effective use of demonstration data to improve exploration efficiency still remains an open challenge. In this work, we introduce Demonstration-guided EXploration (DEX), an efficient reinforcement learning algorithm that aims to overcome the exploration problem with expert demonstrations for surgical automation. To effectively exploit demonstrations, our method estimates expert-like behaviors with higher values to facilitate productive interactions, and adopts non-parametric regression to enable such guidance at states unobserved in demonstration data. Extensive experiments on $10$ surgical manipulation tasks from SurRoL, a comprehensive surgical simulation platform, demonstrate significant improvements in the exploration efficiency and task success rates of our method. Moreover, we also deploy the learned policies to the da Vinci Research Kit (dVRK) platform to show the effectiveness on the real robot. Code is available at https://github.com/med-air/DEX.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.09772",
        "string": "[Demonstration-Guided Reinforcement Learning with Efficient Exploration for Task Automation of Surgical Robot](https://arxiv.org/pdf/2302.09772)"
    },
    "Differentiable Arbitrating in Zero-sum Markov Games": {
        "abstract": "We initiate the study of how to perturb the reward in a zero-sum Markov game with two players to induce a desirable Nash equilibrium, namely arbitrating. Such a problem admits a bi-level optimization formulation. The lower level requires solving the Nash equilibrium under a given reward function, which makes the overall problem challenging to optimize in an end-to-end way. We propose a backpropagation scheme that differentiates through the Nash equilibrium, which provides the gradient feedback for the upper level. In particular, our method only requires a black-box solver for the (regularized) Nash equilibrium (NE). We develop the convergence analysis for the proposed framework with proper black-box NE solvers and demonstrate the empirical successes in two multi-agent reinforcement learning (MARL) environments.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.10058",
        "string": "[Differentiable Arbitrating in Zero-sum Markov Games](https://arxiv.org/pdf/2302.10058)"
    },
    "Diverse Policy Optimization for Structured Action Space": {
        "abstract": "Enhancing the diversity of policies is beneficial for robustness, exploration, and transfer in reinforcement learning (RL). In this paper, we aim to seek diverse policies in an under-explored setting, namely RL tasks with structured action spaces with the two properties of composability and local dependencies. The complex action structure, non-uniform reward landscape, and subtle hyperparameter tuning due to the properties of structured actions prevent existing approaches from scaling well. We propose a simple and effective RL method, Diverse Policy Optimization (DPO), to model the policies in structured action space as the energy-based models (EBM) by following the probabilistic RL framework. A recently proposed novel and powerful generative model, GFlowNet, is introduced as the efficient, diverse EBM-based policy sampler. DPO follows a joint optimization framework: the outer layer uses the diverse policies sampled by the GFlowNet to update the EBM-based policies, which supports the GFlowNet training in the inner layer. Experiments on ATSC and Battle benchmarks demonstrate that DPO can efficiently discover surprisingly diverse policies in challenging scenarios and substantially outperform existing state-of-the-art methods.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.11917",
        "string": "[Diverse Policy Optimization for Structured Action Space](https://arxiv.org/pdf/2302.11917)"
    },
    "Dual Policy Learning for Aggregation Optimization in Graph Neural Network-based Recommender Systems": {
        "abstract": "Graph Neural Networks (GNNs) provide powerful representations for recommendation tasks. GNN-based recommendation systems capture the complex high-order connectivity between users and items by aggregating information from distant neighbors and can improve the performance of recommender systems. Recently, Knowledge Graphs (KGs) have also been incorporated into the user-item interaction graph to provide more abundant contextual information; they are exploited to address cold-start problems and enable more explainable aggregation in GNN-based recommender systems (GNN-Rs). However, due to the heterogeneous nature of users and items, developing an effective aggregation strategy that works across multiple GNN-Rs, such as LightGCN and KGAT, remains a challenge. In this paper, we propose a novel reinforcement learning-based message passing framework for recommender systems, which we call DPAO (Dual Policy framework for Aggregation Optimization). This framework adaptively determines high-order connectivity to aggregate users and items using dual policy learning. Dual policy learning leverages two Deep-Q-Network models to exploit the user- and item-aware feedback from a GNN-R and boost the performance of the target GNN-R. Our proposed framework was evaluated with both non-KG-based and KG-based GNN-R models on six real-world datasets, and their results show that our proposed framework significantly enhances the recent base model, improving nDCG and Recall by up to 63.7% and 42.9%, respectively. Our implementation code is available at https://github.com/steve30572/DPAO/.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.10567",
        "string": "[Dual Policy Learning for Aggregation Optimization in Graph Neural Network-based Recommender Systems](https://arxiv.org/pdf/2302.10567)"
    },
    "EvoTorch: Scalable Evolutionary Computation in Python": {
        "abstract": "Evolutionary computation is an important component within various fields such as artificial intelligence research, reinforcement learning, robotics, industrial automation and/or optimization, engineering design, etc. Considering the increasing computational demands and the dimensionalities of modern optimization problems, the requirement for scalable, re-usable, and practical evolutionary algorithm implementations has been growing. To address this requirement, we present EvoTorch: an evolutionary computation library designed to work with high-dimensional optimization problems, with GPU support and with high parallelization capabilities. EvoTorch is based on and seamlessly works with the PyTorch library, and therefore, allows the users to define their optimization problems using a well-known API.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.12600",
        "string": "[EvoTorch: Scalable Evolutionary Computation in Python](https://arxiv.org/pdf/2302.12600)"
    },
    "Exploration by self-supervised exploitation": {
        "abstract": "Reinforcement learning can solve decision-making problems and train an agent to behave in an environment according to a predesigned reward function. However, such an approach becomes very problematic if the reward is too sparse and the agent does not come across the reward during the environmental exploration. The solution to such a problem may be in equipping the agent with an intrinsic motivation, which will provide informed exploration, during which the agent is likely to also encounter external reward. Novelty detection is one of the promising branches of intrinsic motivation research. We present Self-supervised Network Distillation (SND), a class of internal motivation algorithms based on the distillation error as a novelty indicator, where the target model is trained using self-supervised learning. We adapted three existing self-supervised methods for this purpose and experimentally tested them on a set of ten environments that are considered difficult to explore. The results show that our approach achieves faster growth and higher external reward for the same training time compared to the baseline models, which implies improved exploration in a very sparse reward environment.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.11563",
        "string": "[Exploration by self-supervised exploitation](https://arxiv.org/pdf/2302.11563)"
    },
    "Exponential Hardness of Reinforcement Learning with Linear Function Approximation": {
        "abstract": "A fundamental question in reinforcement learning theory is: suppose the optimal value functions are linear in given features, can we learn them efficiently? This problem's counterpart in supervised learning, linear regression, can be solved both statistically and computationally efficiently. Therefore, it was quite surprising when a recent work \\cite{kane2022computational} showed a computational-statistical gap for linear reinforcement learning: even though there are polynomial sample-complexity algorithms, unless NP = RP, there are no polynomial time algorithms for this setting.\n  In this work, we build on their result to show a computational lower bound, which is exponential in feature dimension and horizon, for linear reinforcement learning under the Randomized Exponential Time Hypothesis. To prove this we build a round-based game where in each round the learner is searching for an unknown vector in a unit hypercube. The rewards in this game are chosen such that if the learner achieves large reward, then the learner's actions can be used to simulate solving a variant of 3-SAT, where (a) each variable shows up in a bounded number of clauses (b) if an instance has no solutions then it also has no solutions that satisfy more than (1-$\u03b5$)-fraction of clauses. We use standard reductions to show this 3-SAT variant is approximately as hard as 3-SAT. Finally, we also show a lower bound optimized for horizon dependence that almost matches the best known upper bound of $\\exp(\\sqrt{H})$.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.12940",
        "string": "[Exponential Hardness of Reinforcement Learning with Linear Function Approximation](https://arxiv.org/pdf/2302.12940)"
    },
    "Fantastic Rewards and How to Tame Them: A Case Study on Reward Learning for Task-oriented Dialogue Systems": {
        "abstract": "When learning task-oriented dialogue (ToD) agents, reinforcement learning (RL) techniques can naturally be utilized to train dialogue strategies to achieve user-specific goals. Prior works mainly focus on adopting advanced RL techniques to train the ToD agents, while the design of the reward function is not well studied. This paper aims at answering the question of how to efficiently learn and leverage a reward function for training end-to-end (E2E) ToD agents. Specifically, we introduce two generalized objectives for reward-function learning, inspired by the classical learning-to-rank literature. Further, we utilize the learned reward function to guide the training of the E2E ToD agent. With the proposed techniques, we achieve competitive results on the E2E response-generation task on the Multiwoz 2.0 dataset. Source code and checkpoints are publicly released at https://github.com/Shentao-YANG/Fantastic_Reward_ICLR2023.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.10342",
        "string": "[Fantastic Rewards and How to Tame Them: A Case Study on Reward Learning for Task-oriented Dialogue Systems](https://arxiv.org/pdf/2302.10342)"
    },
    "Few-Shot Structured Policy Learning for Multi-Domain and Multi-Task Dialogues": {
        "abstract": "Reinforcement learning has been widely adopted to model dialogue managers in task-oriented dialogues. However, the user simulator provided by state-of-the-art dialogue frameworks are only rough approximations of human behaviour. The ability to learn from a small number of human interactions is hence crucial, especially on multi-domain and multi-task environments where the action space is large. We therefore propose to use structured policies to improve sample efficiency when learning on these kinds of environments. We also evaluate the impact of learning from human vs simulated experts. Among the different levels of structure that we tested, the graph neural networks (GNNs) show a remarkable superiority by reaching a success rate above 80% with only 50 dialogues, when learning from simulated experts. They also show superiority when learning from human experts, although a performance drop was observed, indicating a possible difficulty in capturing the variability of human strategies. We therefore suggest to concentrate future research efforts on bridging the gap between human data, simulators and automatic evaluators in dialogue frameworks.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.11199",
        "string": "[Few-Shot Structured Policy Learning for Multi-Domain and Multi-Task Dialogues](https://arxiv.org/pdf/2302.11199)"
    },
    "GANterfactual-RL: Understanding Reinforcement Learning Agents' Strategies through Visual Counterfactual Explanations": {
        "abstract": "Counterfactual explanations are a common tool to explain artificial intelligence models. For Reinforcement Learning (RL) agents, they answer \"Why not?\" or \"What if?\" questions by illustrating what minimal change to a state is needed such that an agent chooses a different action. Generating counterfactual explanations for RL agents with visual input is especially challenging because of their large state spaces and because their decisions are part of an overarching policy, which includes long-term decision-making. However, research focusing on counterfactual explanations, specifically for RL agents with visual input, is scarce and does not go beyond identifying defective agents. It is unclear whether counterfactual explanations are still helpful for more complex tasks like analyzing the learned strategies of different agents or choosing a fitting agent for a specific task. We propose a novel but simple method to generate counterfactual explanations for RL agents by formulating the problem as a domain transfer problem which allows the use of adversarial learning techniques like StarGAN. Our method is fully model-agnostic and we demonstrate that it outperforms the only previous method in several computational metrics. Furthermore, we show in a user study that our method performs best when analyzing which strategies different agents pursue.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.12689",
        "string": "[GANterfactual-RL: Understanding Reinforcement Learning Agents' Strategies through Visual Counterfactual Explanations](https://arxiv.org/pdf/2302.12689)"
    },
    "GTRL: An Entity Group-Aware Temporal Knowledge Graph Representation Learning Method": {
        "abstract": "Temporal Knowledge Graph (TKG) representation learning embeds entities and event types into a continuous low-dimensional vector space by integrating the temporal information, which is essential for downstream tasks, e.g., event prediction and question answering. Existing methods stack multiple graph convolution layers to model the influence of distant entities, leading to the over-smoothing problem. To alleviate the problem, recent studies infuse reinforcement learning to obtain paths that contribute to modeling the influence of distant entities. However, due to the limited number of hops, these studies fail to capture the correlation between entities that are far apart and even unreachable. To this end, we propose GTRL, an entity Group-aware Temporal knowledge graph Representation Learning method. GTRL is the first work that incorporates the entity group modeling to capture the correlation between entities by stacking only a finite number of layers. Specifically, the entity group mapper is proposed to generate entity groups from entities in a learning way. Based on entity groups, the implicit correlation encoder is introduced to capture implicit correlations between any pairwise entity groups. In addition, the hierarchical GCNs are exploited to accomplish the message aggregation and representation updating on the entity group graph and the entity graph. Finally, GRUs are employed to capture the temporal dependency in TKGs. Extensive experiments on three real-world datasets demonstrate that GTRL achieves the state-of-the-art performances on the event prediction task, outperforming the best baseline by an average of 13.44%, 9.65%, 12.15%, and 15.12% in MRR, Hits@1, Hits@3, and Hits@10, respectively.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.11091",
        "string": "[GTRL: An Entity Group-Aware Temporal Knowledge Graph Representation Learning Method](https://arxiv.org/pdf/2302.11091)"
    },
    "GraphSR: A Data Augmentation Algorithm for Imbalanced Node Classification": {
        "abstract": "Graph neural networks (GNNs) have achieved great success in node classification tasks. However, existing GNNs naturally bias towards the majority classes with more labelled data and ignore those minority classes with relatively few labelled ones. The traditional techniques often resort over-sampling methods, but they may cause overfitting problem. More recently, some works propose to synthesize additional nodes for minority classes from the labelled nodes, however, there is no any guarantee if those generated nodes really stand for the corresponding minority classes. In fact, improperly synthesized nodes may result in insufficient generalization of the algorithm. To resolve the problem, in this paper we seek to automatically augment the minority classes from the massive unlabelled nodes of the graph. Specifically, we propose \\textit{GraphSR}, a novel self-training strategy to augment the minority classes with significant diversity of unlabelled nodes, which is based on a Similarity-based selection module and a Reinforcement Learning(RL) selection module. The first module finds a subset of unlabelled nodes which are most similar to those labelled minority nodes, and the second one further determines the representative and reliable nodes from the subset via RL technique. Furthermore, the RL-based module can adaptively determine the sampling scale according to current training data. This strategy is general and can be easily combined with different GNNs models. Our experiments demonstrate the proposed approach outperforms the state-of-the-art baselines on various class-imbalanced datasets.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.12814",
        "string": "[GraphSR: A Data Augmentation Algorithm for Imbalanced Node Classification](https://arxiv.org/pdf/2302.12814)"
    },
    "Guiding Large Language Models via Directional Stimulus Prompting": {
        "abstract": "We introduce a new framework, Directional Stimulus Prompting, that uses a tuneable language model (LM) to provide guidance for the black-box frozen large language model (LLM) on downstream tasks. Unlike prior work that manually or automatically finds the optimal prompt for each task, we train a policy LM to generate discrete tokens as ``directional stimulus'' of each input, which is a hint/cue such as keywords of an article for summarization. The directional stimulus is then combined with the original input and fed into the LLM to guide its generation toward the desired target. The policy LM can be trained through 1) supervised learning from annotated data and 2) reinforcement learning from offline and online rewards to explore directional stimulus that better aligns LLMs with human preferences. This framework is flexibly applicable to various LMs and tasks. To verify its effectiveness, we apply our framework to summarization and dialogue response generation tasks. Experimental results demonstrate that it can significantly improve LLMs' performance with a small collection of training data: a T5 (780M) trained with 2,000 samples from the CNN/Daily Mail dataset improves Codex (175B)'s performance by 7.2% in ROUGE-Avg scores; 500 dialogues boost the combined score by 52.5%, achieving comparable or even better performance than fully trained models on the MultiWOZ dataset.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.11520",
        "string": "[Guiding Large Language Models via Directional Stimulus Prompting](https://arxiv.org/pdf/2302.11520)"
    },
    "Harnessing the Speed and Accuracy of Machine Learning to Advance Cybersecurity": {
        "abstract": "As cyber attacks continue to increase in frequency and sophistication, detecting malware has become a critical task for maintaining the security of computer systems. Traditional signature-based methods of malware detection have limitations in detecting complex and evolving threats. In recent years, machine learning (ML) has emerged as a promising solution to detect malware effectively. ML algorithms are capable of analyzing large datasets and identifying patterns that are difficult for humans to identify. This paper presents a comprehensive review of the state-of-the-art ML techniques used in malware detection, including supervised and unsupervised learning, deep learning, and reinforcement learning. We also examine the challenges and limitations of ML-based malware detection, such as the potential for adversarial attacks and the need for large amounts of labeled data. Furthermore, we discuss future directions in ML-based malware detection, including the integration of multiple ML algorithms and the use of explainable AI techniques to enhance the interpret ability of ML-based detection systems. Our research highlights the potential of ML-based techniques to improve the speed and accuracy of malware detection, and contribute to enhancing cybersecurity\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.12415",
        "string": "[Harnessing the Speed and Accuracy of Machine Learning to Advance Cybersecurity](https://arxiv.org/pdf/2302.12415)"
    },
    "Hierarchical Needs-driven Agent Learning Systems: From Deep Reinforcement Learning To Diverse Strategies": {
        "abstract": "The needs describe the necessities for a system to survive and evolve, which arouses an agent to action toward a goal, giving purpose and direction to behavior. Based on Maslow hierarchy of needs, an agent needs to satisfy a certain amount of needs at the current level as a condition to arise at the next stage -- upgrade and evolution. Especially, Deep Reinforcement Learning (DAL) can help AI agents (like robots) organize and optimize their behaviors and strategies to develop diverse Strategies based on their current state and needs (expected utilities or rewards). This paper introduces the new hierarchical needs-driven Learning systems based on DAL and investigates the implementation in the single-robot with a novel approach termed Bayesian Soft Actor-Critic (BSAC). Then, we extend this topic to the Multi-Agent systems (MAS), discussing the potential research fields and directions.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.13132",
        "string": "[Hierarchical Needs-driven Agent Learning Systems: From Deep Reinforcement Learning To Diverse Strategies](https://arxiv.org/pdf/2302.13132)"
    },
    "Inequity aversion reduces travel time in the traffic light control problem": {
        "abstract": "The traffic light control problem is to improve the traffic flow by coordinating between the traffic lights. Recently, a successful deep reinforcement learning model, CoLight, was developed to capture the influences of neighboring intersections by a graph attention network. We propose IACoLight that boosts up to 11.4% the performance of CoLight by incorporating the Inequity Aversion (IA) model that reshapes each agent's reward by adding or subtracting advantageous or disadvantageous reward inequities compared to other agents. Unlike in the other applications of IA, where both advantageous and disadvantageous inequities are punished by considering negative coefficients, we allowed them to be also rewarded and explored a range of both positive and negative coefficients. Our experiments demonstrated that making CoLight agents averse to inequities improved the vehicles' average travel time and rewarding rather than punishing advantageous inequities enhanced the results.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.12053",
        "string": "[Inequity aversion reduces travel time in the traffic light control problem](https://arxiv.org/pdf/2302.12053)"
    },
    "Kernel-Based Distributed Q-Learning: A Scalable Reinforcement Learning Approach for Dynamic Treatment Regimes": {
        "abstract": "In recent years, large amounts of electronic health records (EHRs) concerning chronic diseases, such as cancer, diabetes, and mental disease, have been collected to facilitate medical diagnosis. Modeling the dynamic properties of EHRs related to chronic diseases can be efficiently done using dynamic treatment regimes (DTRs), which are a set of sequential decision rules. While Reinforcement learning (RL) is a widely used method for creating DTRs, there is ongoing research in developing RL algorithms that can effectively handle large amounts of data. In this paper, we present a novel approach, a distributed Q-learning algorithm, for generating DTRs. The novelties of our research are as follows: 1) From a methodological perspective, we present a novel and scalable approach for generating DTRs by combining distributed learning with Q-learning. The proposed approach is specifically designed to handle large amounts of data and effectively generate DTRs. 2) From a theoretical standpoint, we provide generalization error bounds for the proposed distributed Q-learning algorithm, which are derived within the framework of statistical learning theory. These bounds quantify the relationships between sample size, prediction accuracy, and computational burden, providing insights into the performance of the algorithm. 3) From an applied perspective, we demonstrate the effectiveness of our proposed distributed Q-learning algorithm for DTRs by applying it to clinical cancer treatments. The results show that our algorithm outperforms both traditional linear Q-learning and commonly used deep Q-learning in terms of both prediction accuracy and computation cost.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.10434",
        "string": "[Kernel-Based Distributed Q-Learning: A Scalable Reinforcement Learning Approach for Dynamic Treatment Regimes](https://arxiv.org/pdf/2302.10434)"
    },
    "Learning Agile Flights through Narrow Gaps with Varying Angles using Onboard Sensing": {
        "abstract": "This paper addresses the problem of traversing through unknown, tilted, and narrow gaps for quadrotors using Deep Reinforcement Learning (DRL). Previous learning-based methods relied on accurate knowledge of the environment, including the gap's pose and size. In contrast, we integrate onboard sensing and detect the gap from a single onboard camera. The training problem is challenging for two reasons: a precise and robust whole-body planning and control policy is required for variable-tilted and narrow gaps, and an effective Sim2Real method is needed to successfully conduct real-world experiments. To this end, we propose a learning framework for agile gap traversal flight, which successfully trains the vehicle to traverse through the center of the gap at an approximate attitude to the gap with aggressive tilted angles. The policy trained only in a simulation environment can be transferred into different domains with fine-tuning while maintaining the success rate. Our proposed framework, which integrates onboard sensing and a neural network controller, achieves a success rate of 84.51% in real-world experiments, with gap orientations up to 60deg. To the best of our knowledge, this is the first paper that performs the learning-based variable-tilted narrow gap traversal flight in the real world, without prior knowledge of the environment.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.11233",
        "string": "[Learning Agile Flights through Narrow Gaps with Varying Angles using Onboard Sensing](https://arxiv.org/pdf/2302.11233)"
    },
    "Leveraging Jumpy Models for Planning and Fast Learning in Robotic Domains": {
        "abstract": "In this paper we study the problem of learning multi-step dynamics prediction models (jumpy models) from unlabeled experience and their utility for fast inference of (high-level) plans in downstream tasks. In particular we propose to learn a jumpy model alongside a skill embedding space offline, from previously collected experience for which no labels or reward annotations are required. We then investigate several options of harnessing those learned components in combination with model-based planning or model-free reinforcement learning (RL) to speed up learning on downstream tasks. We conduct a set of experiments in the RGB-stacking environment, showing that planning with the learned skills and the associated model can enable zero-shot generalization to new tasks, and can further speed up training of policies via reinforcement learning. These experiments demonstrate that jumpy models which incorporate temporal abstraction can facilitate planning in long-horizon tasks in which standard dynamics models fail.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.12617",
        "string": "[Leveraging Jumpy Models for Planning and Fast Learning in Robotic Domains](https://arxiv.org/pdf/2302.12617)"
    },
    "Logarithmic Switching Cost in Reinforcement Learning beyond Linear MDPs": {
        "abstract": "In many real-life reinforcement learning (RL) problems, deploying new policies is costly. In those scenarios, algorithms must solve exploration (which requires adaptivity) while switching the deployed policy sparsely (which limits adaptivity). In this paper, we go beyond the existing state-of-the-art on this problem that focused on linear Markov Decision Processes (MDPs) by considering linear Bellman-complete MDPs with low inherent Bellman error. We propose the ELEANOR-LowSwitching algorithm that achieves the near-optimal regret with a switching cost logarithmic in the number of episodes and linear in the time-horizon $H$ and feature dimension $d$. We also prove a lower bound proportional to $dH$ among all algorithms with sublinear regret. In addition, we show the ``doubling trick'' used in ELEANOR-LowSwitching can be further leveraged for the generalized linear function approximation, under which we design a sample-efficient algorithm with near-optimal switching cost.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.12456",
        "string": "[Logarithmic Switching Cost in Reinforcement Learning beyond Linear MDPs](https://arxiv.org/pdf/2302.12456)"
    },
    "MAC-PO: Multi-Agent Experience Replay via Collective Priority Optimization": {
        "abstract": "Experience replay is crucial for off-policy reinforcement learning (RL) methods. By remembering and reusing the experiences from past different policies, experience replay significantly improves the training efficiency and stability of RL algorithms. Many decision-making problems in practice naturally involve multiple agents and require multi-agent reinforcement learning (MARL) under centralized training decentralized execution paradigm. Nevertheless, existing MARL algorithms often adopt standard experience replay where the transitions are uniformly sampled regardless of their importance. Finding prioritized sampling weights that are optimized for MARL experience replay has yet to be explored. To this end, we propose \\name, which formulates optimal prioritized experience replay for multi-agent problems as a regret minimization over the sampling weights of transitions. Such optimization is relaxed and solved using the Lagrangian multiplier approach to obtain the close-form optimal sampling weights. By minimizing the resulting policy regret, we can narrow the gap between the current policy and a nominal optimal policy, thus acquiring an improved prioritization scheme for multi-agent tasks. Our experimental results on Predator-Prey and StarCraft Multi-Agent Challenge environments demonstrate the effectiveness of our method, having a better ability to replay important transitions and outperforming other state-of-the-art baselines.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.10418",
        "string": "[MAC-PO: Multi-Agent Experience Replay via Collective Priority Optimization](https://arxiv.org/pdf/2302.10418)"
    },
    "Minimax-Bayes Reinforcement Learning": {
        "abstract": "While the Bayesian decision-theoretic framework offers an elegant solution to the problem of decision making under uncertainty, one question is how to appropriately select the prior distribution. One idea is to employ a worst-case prior. However, this is not as easy to specify in sequential decision making as in simple statistical estimation problems. This paper studies (sometimes approximate) minimax-Bayes solutions for various reinforcement learning problems to gain insights into the properties of the corresponding priors and policies. We find that while the worst-case prior depends on the setting, the corresponding minimax policies are more robust than those that assume a standard (i.e. uniform) prior.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.10831",
        "string": "[Minimax-Bayes Reinforcement Learning](https://arxiv.org/pdf/2302.10831)"
    },
    "Model-Based Uncertainty in Value Functions": {
        "abstract": "We consider the problem of quantifying uncertainty over expected cumulative rewards in model-based reinforcement learning. In particular, we focus on characterizing the variance over values induced by a distribution over MDPs. Previous work upper bounds the posterior variance over values by solving a so-called uncertainty Bellman equation, but the over-approximation may result in inefficient exploration. We propose a new uncertainty Bellman equation whose solution converges to the true posterior variance over values and explicitly characterizes the gap in previous work. Moreover, our uncertainty quantification technique is easily integrated into common exploration strategies and scales naturally beyond the tabular setting by using standard deep reinforcement learning architectures. Experiments in difficult exploration tasks, both in tabular and continuous control settings, show that our sharper uncertainty estimates improve sample-efficiency.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.12526",
        "string": "[Model-Based Uncertainty in Value Functions](https://arxiv.org/pdf/2302.12526)"
    },
    "Modular Deep Learning": {
        "abstract": "Transfer learning has recently become the dominant paradigm of machine learning. Pre-trained models fine-tuned for downstream tasks achieve better performance with fewer labelled examples. Nonetheless, it remains unclear how to develop models that specialise towards multiple tasks without incurring negative interference and that generalise systematically to non-identically distributed tasks. Modular deep learning has emerged as a promising solution to these challenges. In this framework, units of computation are often implemented as autonomous parameter-efficient modules. Information is conditionally routed to a subset of modules and subsequently aggregated. These properties enable positive transfer and systematic generalisation by separating computation from routing and updating modules locally. We offer a survey of modular architectures, providing a unified view over several threads of research that evolved independently in the scientific literature. Moreover, we explore various additional purposes of modularity, including scaling language models, causal inference, programme induction, and planning in reinforcement learning. Finally, we report various concrete applications where modularity has been successfully deployed such as cross-lingual and cross-modal knowledge transfer. Related talks and projects to this survey, are available at https://www.modulardeeplearning.com/.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.11529",
        "string": "[Modular Deep Learning](https://arxiv.org/pdf/2302.11529)"
    },
    "Multi-Agent Reinforcement Learning with Common Policy for Antenna Tilt Optimization": {
        "abstract": "This paper proposes a method for wireless network optimization applicable to tuning cell parameters that impact the performance of the adjusted cell and the surrounding neighboring cells. The method relies on multiple reinforcement learning agents that share a common policy and include information from neighboring cells in the state and reward. In order not to impair network performance during the first steps of learning, agents are pre-trained during an earlier phase of offline learning, in which an initial policy is obtained using feedback from a static network simulator and considering a wide variety of scenarios. Finally, agents can wisely tune the cell parameters of a test network by suggesting small incremental changes to slowly steer the network toward an optimal configuration. Agents propose optimal changes using the experience gained with the simulator in the pre-training phase, but also continue to learn from current network readings after each change. The results show how the proposed approach significantly improves the performance gains already provided by expert system-based methods when applied to remote antenna tilt optimization. Additional gains are also seen when comparing the proposed approach with a similar method in which the state and reward do not include information from neighboring cells.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.12899",
        "string": "[Multi-Agent Reinforcement Learning with Common Policy for Antenna Tilt Optimization](https://arxiv.org/pdf/2302.12899)"
    },
    "Multiagent Inverse Reinforcement Learning via Theory of Mind Reasoning": {
        "abstract": "To understand how people interact with each other in collaborative settings, especially in situations where individuals know little about their teammates, Multiagent Inverse Reinforcement Learning (MIRL) aims to infer the reward functions guiding the behavior of each individual given trajectories of a team's behavior during task performance. Unlike current MIRL approaches, team members \\emph{are not} assumed to know each other's goals a priori, rather they collaborate by adapting to the goals of others perceived by observing their behavior, all while jointly performing a task. To address this problem, we propose a novel approach to MIRL via Theory of Mind (MIRL-ToM). For each agent, we first use ToM reasoning to estimate a posterior distribution over baseline reward profiles given their demonstrated behavior. We then perform MIRL via decentralized equilibrium by employing single-agent Maximum Entropy IRL to infer a reward function for each agent, where we simulate the behavior of other teammates according to the time-varying distribution over profiles. We evaluate our approach in a simulated 2-player search-and-rescue operation where the goal of the agents, playing different roles, is to search for and evacuate victims in the environment. Results show that the choice of baseline profiles is paramount to the recovery of ground-truth rewards, and MIRL-ToM is able to recover the rewards used by agents interacting with either known and unknown teammates.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.10238",
        "string": "[Multiagent Inverse Reinforcement Learning via Theory of Mind Reasoning](https://arxiv.org/pdf/2302.10238)"
    },
    "Neural Laplace Control for Continuous-time Delayed Systems": {
        "abstract": "Many real-world offline reinforcement learning (RL) problems involve continuous-time environments with delays. Such environments are characterized by two distinctive features: firstly, the state x(t) is observed at irregular time intervals, and secondly, the current action a(t) only affects the future state x(t + g) with an unknown delay g > 0. A prime example of such an environment is satellite control where the communication link between earth and a satellite causes irregular observations and delays. Existing offline RL algorithms have achieved success in environments with irregularly observed states in time or known delays. However, environments involving both irregular observations in time and unknown delays remains an open and challenging problem. To this end, we propose Neural Laplace Control, a continuous-time model-based offline RL method that combines a Neural Laplace dynamics model with a model predictive control (MPC) planner--and is able to learn from an offline dataset sampled with irregular time intervals from an environment that has a inherent unknown constant delay. We show experimentally on continuous-time delayed environments it is able to achieve near expert policy performance.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.12604",
        "string": "[Neural Laplace Control for Continuous-time Delayed Systems](https://arxiv.org/pdf/2302.12604)"
    },
    "Neural Optimal Control using Learned System Dynamics": {
        "abstract": "We study the problem of generating control laws for systems with unknown dynamics. Our approach is to represent the controller and the value function with neural networks, and to train them using loss functions adapted from the Hamilton-Jacobi-Bellman (HJB) equations. In the absence of a known dynamics model, our method first learns the state transitions from data collected by interacting with the system in an offline process. The learned transition function is then integrated to the HJB equations and used to forward simulate the control signals produced by our controller in a feedback loop.\n  In contrast to trajectory optimization methods that optimize the controller for a single initial state, our controller can generate near-optimal control signals for initial states from a large portion of the state space. Compared to recent model-based reinforcement learning algorithms, we show that our method is more sample efficient and trains faster by an order of magnitude. We demonstrate our method in a number of tasks, including the control of a quadrotor with 12 state variables.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.09846",
        "string": "[Neural Optimal Control using Learned System Dynamics](https://arxiv.org/pdf/2302.09846)"
    },
    "Offline Reinforcement Learning for Mixture-of-Expert Dialogue Management": {
        "abstract": "Reinforcement learning (RL) has shown great promise for developing dialogue management (DM) agents that are non-myopic, conduct rich conversations, and maximize overall user satisfaction. Despite recent developments in RL and language models (LMs), using RL to power conversational chatbots remains challenging, in part because RL requires online exploration to learn effectively, whereas collecting novel human-bot interactions can be expensive and unsafe. This issue is exacerbated by the combinatorial action spaces facing these algorithms, as most LM agents generate responses at the word level. We develop a variety of RL algorithms, specialized to dialogue planning, that leverage recent Mixture-of-Expert Language Models (MoE-LMs) -- models that capture diverse semantics, generate utterances reflecting different intents, and are amenable for multi-turn DM. By exploiting MoE-LM structure, our methods significantly reduce the size of the action space and improve the efficacy of RL-based DM.\n  We evaluate our methods in open-domain dialogue to demonstrate their effectiveness w.r.t.\\ the diversity of intent in generated utterances and overall DM performance.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.10850",
        "string": "[Offline Reinforcement Learning for Mixture-of-Expert Dialogue Management](https://arxiv.org/pdf/2302.10850)"
    },
    "On Bellman's principle of optimality and Reinforcement learning for safety-constrained Markov decision process": {
        "abstract": "We study optimality for the safety-constrained Markov decision process which is the underlying framework for safe reinforcement learning. Specifically, we consider a constrained Markov decision process (with finite states and finite actions) where the goal of the decision maker is to reach a target set while avoiding an unsafe set(s) with certain probabilistic guarantees. Therefore the underlying Markov chain for any control policy will be multichain since by definition there exists a target set and an unsafe set. The decision maker also has to be optimal (with respect to a cost function) while navigating to the target set. This gives rise to a multi-objective optimization problem. We highlight the fact that Bellman's principle of optimality may not hold for constrained Markov decision problems with an underlying multichain structure (as shown by the counterexample). We resolve the counterexample by formulating the aforementioned multi-objective optimization problem as a zero-sum game and thereafter construct an asynchronous value iteration scheme for the Lagrangian (similar to Shapley's algorithm. Finally, we consider the reinforcement learning problem for the same and construct a modified Q-learning algorithm for learning the Lagrangian from data. We also provide a lower bound on the number of iterations required for learning the Lagrangian and corresponding error bounds.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.13152",
        "string": "[On Bellman's principle of optimality and Reinforcement learning for safety-constrained Markov decision process](https://arxiv.org/pdf/2302.13152)"
    },
    "Optimal Convergence Rate for Exact Policy Mirror Descent in Discounted Markov Decision Processes": {
        "abstract": "The classical algorithms used in tabular reinforcement learning (Value Iteration and Policy Iteration) have been shown to converge linearly with a rate given by the discount factor $\u03b3$ of a discounted Markov Decision Process. Recently, there has been an increased interest in the study of gradient based methods. In this work, we show that the dimension-free linear $\u03b3$-rate of classical reinforcement learning algorithms can be achieved by a general family of unregularised Policy Mirror Descent (PMD) algorithms under an adaptive step-size. We also provide a matching worst-case lower-bound that demonstrates that the $\u03b3$-rate is optimal for PMD methods. Our work offers a novel perspective on the convergence of PMD. We avoid the use of the performance difference lemma beyond establishing the monotonic improvement of the iterates, which leads to a simple analysis that may be of independent interest. We also extend our analysis to the inexact setting and establish the first dimension-free $\\varepsilon$-optimal sample complexity for unregularised PMD under a generative model, improving upon the best-known result.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.11381",
        "string": "[Optimal Convergence Rate for Exact Policy Mirror Descent in Discounted Markov Decision Processes](https://arxiv.org/pdf/2302.11381)"
    },
    "Potential-based reward shaping for learning to play text-based adventure games": {
        "abstract": "Text-based games are a popular testbed for language-based reinforcement learning (RL). In previous work, deep Q-learning is commonly used as the learning agent. Q-learning algorithms are challenging to apply to complex real-world domains due to, for example, their instability in training. Therefore, in this paper, we adapt the soft-actor-critic (SAC) algorithm to the text-based environment. To deal with sparse extrinsic rewards from the environment, we combine it with a potential-based reward shaping technique to provide more informative (dense) reward signals to the RL agent. We apply our method to play difficult text-based games. The SAC method achieves higher scores than the Q-learning methods on many games with only half the number of training steps. This shows that it is well-suited for text-based games. Moreover, we show that the reward shaping technique helps the agent to learn the policy faster and achieve higher scores. In particular, we consider a dynamically learned value function as a potential function for shaping the learner's original sparse reward signals.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.10720",
        "string": "[Potential-based reward shaping for learning to play text-based adventure games](https://arxiv.org/pdf/2302.10720)"
    },
    "Prioritized Trace Selection: Towards High-Performance DRL-based Network Controllers": {
        "abstract": "Deep Reinforcement Learning (DRL) based controllers offer high performance in a variety of network environments. However, simulator-based training of DRL controllers using highly skewed datasets of real-world traces often results in poor performance in the wild. In this paper, we put forward a generalizable solution for training high-performance DRL controllers in simulators -- Prioritized Trace Selection (PTS). PTS employs an automated three-stage process. First, we identify critical features that determine trace behavior. Second, we classify the traces into clusters. Finally, we dynamically identify and prioritize the salient clusters during training.\n  PTS does not require any changes to the DRL workflow. It can work across both on-policy and off-policy DRL algorithms. We use Adaptive Bit Rate selection and Congestion Control as representative applications to show that PTS offers better performance in simulation and real-world, across multiple controllers and DRL algorithms. Our novel ABR controller, Gelato, trained with PTS outperforms state-of-the-art controllers on the real-world live-streaming platform, Puffer, reducing stalls by 59% and significantly improving average video quality.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.12403",
        "string": "[Prioritized Trace Selection: Towards High-Performance DRL-based Network Controllers](https://arxiv.org/pdf/2302.12403)"
    },
    "Provably Efficient Exploration in Quantum Reinforcement Learning with Logarithmic Worst-Case Regret": {
        "abstract": "While quantum reinforcement learning (RL) has attracted a surge of attention recently, its theoretical understanding is limited. In particular, it remains elusive how to design provably efficient quantum RL algorithms that can address the exploration-exploitation trade-off. To this end, we propose a novel UCRL-style algorithm that takes advantage of quantum computing for tabular Markov decision processes (MDPs) with $S$ states, $A$ actions, and horizon $H$, and establish an $\\mathcal{O}(\\mathrm{poly}(S, A, H, \\log T))$ worst-case regret for it, where $T$ is the number of episodes. Furthermore, we extend our results to quantum RL with linear function approximation, which is capable of handling problems with large state spaces. Specifically, we develop a quantum algorithm based on value target regression (VTR) for linear mixture MDPs with $d$-dimensional linear representation and prove that it enjoys $\\mathcal{O}(\\mathrm{poly}(d, H, \\log T))$ regret. Our algorithms are variants of UCRL/UCRL-VTR algorithms in classical RL, which also leverage a novel combination of lazy updating mechanisms and quantum estimation subroutines. This is the key to breaking the $\u03a9(\\sqrt{T})$-regret barrier in classical RL. To the best of our knowledge, this is the first work studying the online exploration in quantum RL with provable logarithmic worst-case regret.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.10796",
        "string": "[Provably Efficient Exploration in Quantum Reinforcement Learning with Logarithmic Worst-Case Regret](https://arxiv.org/pdf/2302.10796)"
    },
    "Provably Efficient Neural Offline Reinforcement Learning via Perturbed Rewards": {
        "abstract": "We propose a novel offline reinforcement learning (RL) algorithm, namely Value Iteration with Perturbed Rewards (VIPeR) which amalgamates the randomized value function idea with the pessimism principle. Most current offline RL algorithms explicitly construct statistical confidence regions to obtain pessimism via lower confidence bounds (LCB), which cannot easily scale to complex problems where a neural network is used to estimate the value functions. Instead, VIPeR implicitly obtains pessimism by simply perturbing the offline data multiple times with carefully-designed i.i.d Gaussian noises to learn an ensemble of estimated state-action values and acting greedily to the minimum of the ensemble. The estimated state-action values are obtained by fitting a parametric model (e.g. neural networks) to the perturbed datasets using gradient descent. As a result, VIPeR only needs $\\mathcal{O}(1)$ time complexity for action selection while LCB-based algorithms require at least $\u03a9(K^2)$, where $K$ is the total number of trajectories in the offline data. We also propose a novel data splitting technique that helps remove the potentially large log covering number in the learning bound. We prove that VIPeR yields a provable uncertainty quantifier with overparameterized neural networks and achieves an $\\tilde{\\mathcal{O}}\\left( \\frac{ \u03baH^{5/2} \\tilde{d} }{\\sqrt{K}} \\right)$ sub-optimality where $\\tilde{d}$ is the effective dimension, $H$ is the horizon length and $\u03ba$ measures the distributional shift. We corroborate the statistical and computational efficiency of VIPeR with an empirical evaluation in a wide set of synthetic and real-world datasets. To the best of our knowledge, VIPeR is the first offline RL algorithm that is both provably and computationally efficient in general Markov decision processes (MDPs) with neural network function approximation.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.12780",
        "string": "[Provably Efficient Neural Offline Reinforcement Learning via Perturbed Rewards](https://arxiv.org/pdf/2302.12780)"
    },
    "Provably Efficient Reinforcement Learning via Surprise Bound": {
        "abstract": "Value function approximation is important in modern reinforcement learning (RL) problems especially when the state space is (infinitely) large. Despite the importance and wide applicability of value function approximation, its theoretical understanding is still not as sophisticated as its empirical success, especially in the context of general function approximation. In this paper, we propose a provably efficient RL algorithm (both computationally and statistically) with general value function approximations. We show that if the value functions can be approximated by a function class that satisfies the Bellman-completeness assumption, our algorithm achieves an $\\widetilde{O}(\\text{poly}(\u03b9H)\\sqrt{T})$ regret bound where $\u03b9$ is the product of the surprise bound and log-covering numbers, $H$ is the planning horizon, $K$ is the number of episodes and $T = HK$ is the total number of steps the agent interacts with the environment. Our algorithm achieves reasonable regret bounds when applied to both the linear setting and the sparse high-dimensional linear setting. Moreover, our algorithm only needs to solve $O(H\\log K)$ empirical risk minimization (ERM) problems, which is far more efficient than previous algorithms that need to solve ERM problems for $\u03a9(HK)$ times.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.11634",
        "string": "[Provably Efficient Reinforcement Learning via Surprise Bound](https://arxiv.org/pdf/2302.11634)"
    },
    "Reinforcement Learning based Autonomous Multi-Rotor Landing on Moving Platforms": {
        "abstract": "Multi-rotor UAVs suffer from a restricted range and flight duration due to limited battery capacity. Autonomous landing on a 2D moving platform offers the possibility to replenish batteries and offload data, thus increasing the utility of the vehicle. Classical approaches rely on accurate, complex and difficult-to-derive models of the vehicle and the environment. Reinforcement learning (RL) provides an attractive alternative due to its ability to learn a suitable control policy exclusively from data during a training procedure. However, current methods require several hours to train, have limited success rates and depend on hyperparameters that need to be tuned by trial-and-error. We address all these issues in this work. First, we decompose the landing procedure into a sequence of simpler, but similar learning tasks. This is enabled by applying two instances of the same RL based controller trained for 1D motion for controlling the multi-rotor's movement in both the longitudinal and the lateral direction. Second, we introduce a powerful state space discretization technique that is based on i) kinematic modeling of the moving platform to derive information about the state space topology and ii) structuring the training as a sequential curriculum using transfer learning. Third, we leverage the kinematics model of the moving platform to also derive interpretable hyperparameters for the training process that ensure sufficient maneuverability of the multi-rotor vehicle. The training is performed using the tabular RL method Double Q-Learning. Through extensive simulations we show that the presented method significantly increases the rate of successful landings, while requiring less training time compared to other deep RL approaches. Finally, we deploy and demonstrate our algorithm on real hardware. For all evaluation scenarios we provide statistics on the agent's performance.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.13192",
        "string": "[Reinforcement Learning based Autonomous Multi-Rotor Landing on Moving Platforms](https://arxiv.org/pdf/2302.13192)"
    },
    "Reinforcement Learning for Block Decomposition of CAD Models": {
        "abstract": "We present a novel AI-assisted method for decomposing (segmenting) planar CAD (computer-aided design) models into well shaped rectangular blocks as a proof-of-principle of a general decomposition method applicable to complex 2D and 3D CAD models. The decomposed blocks are required for generating good quality meshes (tilings of quadrilaterals or hexahedra) suitable for numerical simulations of physical systems governed by conservation laws. The problem of hexahedral mesh generation of general CAD models has vexed researchers for over 3 decades and analysts often spend more than 50% of the design-analysis cycle time decomposing complex models into simpler parts meshable by existing techniques. Our method uses reinforcement learning to train an agent to perform a series of optimal cuts on the CAD model that result in a good quality block decomposition. We show that the agent quickly learns an effective strategy for picking the location and direction of the cuts and maximizing its rewards as opposed to making random cuts. This paper is the first successful demonstration of an agent autonomously learning how to perform this block decomposition task effectively thereby holding the promise of a viable method to automate this challenging process.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.11066",
        "string": "[Reinforcement Learning for Block Decomposition of CAD Models](https://arxiv.org/pdf/2302.11066)"
    },
    "Reinforcement Learning in a Birth and Death Process: Breaking the Dependence on the State Space": {
        "abstract": "In this paper, we revisit the regret of undiscounted reinforcement learning in MDPs with a birth and death structure. Specifically, we consider a controlled queue with impatient jobs and the main objective is to optimize a trade-off between energy consumption and user-perceived performance. Within this setting, the \\emph{diameter} $D$ of the MDP is $\u03a9(S^S)$, where $S$ is the number of states. Therefore, the existing lower and upper bounds on the regret at time$T$, of order $O(\\sqrt{DSAT})$ for MDPs with $S$ states and $A$ actions, may suggest that reinforcement learning is inefficient here. In our main result however, we exploit the structure of our MDPs to show that the regret of a slightly-tweaked version of the classical learning algorithm {\\sc Ucrl2} is in fact upper bounded by $\\tilde{\\mathcal{O}}(\\sqrt{E_2AT})$ where $E_2$ is related to the weighted second moment of the stationary measure of a reference policy. Importantly, $E_2$ is bounded independently of $S$. Thus, our bound is asymptotically independent of the number of states and of the diameter. This result is based on a careful study of the number of visits performed by the learning algorithm to the states of the MDP, which is highly non-uniform.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.10667",
        "string": "[Reinforcement Learning in a Birth and Death Process: Breaking the Dependence on the State Space](https://arxiv.org/pdf/2302.10667)"
    },
    "Reinforcement Learning-based Control of Nonlinear Systems using Carleman Approximation: Structured and Unstructured Designs": {
        "abstract": "We develop data-driven reinforcement learning (RL) control designs for input-affine nonlinear systems. We use Carleman linearization to express the state-space representation of the nonlinear dynamical model in the Carleman space, and develop a real-time algorithm that can learn nonlinear state-feedback controllers using state and input measurements in the infinite-dimensional Carleman space. Thereafter, we study the practicality of having a finite-order truncation of the control signal, followed by its closed-loop stability analysis. Finally, we develop two additional designs that can learn structured as well as sparse representations of the RL-based nonlinear controller, and provide theoretical conditions for ensuring their closed-loop stability. We present numerical examples to show how our proposed method generates closed-loop responses that are close to the optimal performance of the nonlinear plant. We also compare our designs to other data-driven nonlinear RL control methods such as those based on neural networks, and illustrate their relative advantages and drawbacks.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.10864",
        "string": "[Reinforcement Learning-based Control of Nonlinear Systems using Carleman Approximation: Structured and Unstructured Designs](https://arxiv.org/pdf/2302.10864)"
    },
    "Revisiting the Gumbel-Softmax in MADDPG": {
        "abstract": "MADDPG is an algorithm in multi-agent reinforcement learning (MARL) that extends the popular single-agent method, DDPG, to multi-agent scenarios. Importantly, DDPG is an algorithm designed for continuous action spaces, where the gradient of the state-action value function exists. For this algorithm to work in discrete action spaces, discrete gradient estimation must be performed. For MADDPG, the Gumbel-Softmax (GS) estimator is used -- a reparameterisation which relaxes a discrete distribution into a similar continuous one. This method, however, is statistically biased, and a recent MARL benchmarking paper suggests that this bias makes MADDPG perform poorly in grid-world situations, where the action space is discrete. Fortunately, many alternatives to the GS exist, boasting a wide range of properties. This paper explores several of these alternatives and integrates them into MADDPG for discrete grid-world scenarios. The corresponding impact on various performance metrics is then measured and analysed. It is found that one of the proposed estimators performs significantly better than the original GS in several tasks, achieving up to 55% higher returns, along with faster convergence.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.11793",
        "string": "[Revisiting the Gumbel-Softmax in MADDPG](https://arxiv.org/pdf/2302.11793)"
    },
    "Robust Auto-landing Control of an agile Regional Jet Using Fuzzy Q-learning": {
        "abstract": "A robust auto-landing problem of a Truss-braced Wing (TBW) regional jet aircraft with poor stability characteristics is presented in this study employing a Fuzzy Reinforcement Learning scheme. Reinforcement Learning (RL) has seen a recent surge in practical uses in control systems. In contrast to many studies implementing Deep Learning in RL algorithms to generate continuous actions, the methodology of this study is straightforward and avoids complex neural network architectures by applying Fuzzy rules. An innovative, agile civil aircraft is selected not only to meet future aviation community expectations but also to demonstrate the robustness of the suggested method. In order to create a multi-objective RL environment, a Six-degree-of-freedom (6-DoF) simulation is first developed. By transforming the auto-landing problem of the aircraft into a Markov Decision Process (MDP) formulation, the problem is solved by designing a low-level Fuzzy Q-learning (FQL) controller. More specifically, the well-known Q-learning method, which is a discrete RL algorithm, is supplemented by Fuzzy rules to provide continuous actions with no need to complex learning structures. The performance of the proposed system is then evaluated by extensive flight simulations in different flight conditions considering severe wind gusts, measurement noises, actuator faults, and model uncertainties. Besides, the controller effectiveness would be compared with existing competing techniques such as Dynamic Inversion (DI) and Q-learning. The simulation results indicate the superior performance of the proposed control system as a reliable and robust control method to be employed in real applications.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.10997",
        "string": "[Robust Auto-landing Control of an agile Regional Jet Using Fuzzy Q-learning](https://arxiv.org/pdf/2302.10997)"
    },
    "Safe Deep Reinforcement Learning by Verifying Task-Level Properties": {
        "abstract": "Cost functions are commonly employed in Safe Deep Reinforcement Learning (DRL). However, the cost is typically encoded as an indicator function due to the difficulty of quantifying the risk of policy decisions in the state space. Such an encoding requires the agent to visit numerous unsafe states to learn a cost-value function to drive the learning process toward safety. Hence, increasing the number of unsafe interactions and decreasing sample efficiency. In this paper, we investigate an alternative approach that uses domain knowledge to quantify the risk in the proximity of such states by defining a violation metric. This metric is computed by verifying task-level properties, shaped as input-output conditions, and it is used as a penalty to bias the policy away from unsafe states without learning an additional value function. We investigate the benefits of using the violation metric in standard Safe DRL benchmarks and robotic mapless navigation tasks. The navigation experiments bridge the gap between Safe DRL and robotics, introducing a framework that allows rapid testing on real robots. Our experiments show that policies trained with the violation penalty achieve higher performance over Safe DRL baselines and significantly reduce the number of visited unsafe states.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.10030",
        "string": "[Safe Deep Reinforcement Learning by Verifying Task-Level Properties](https://arxiv.org/pdf/2302.10030)"
    },
    "Securing IoT Communication using Physical Sensor Data -- Graph Layer Security with Federated Multi-Agent Deep Reinforcement Learning": {
        "abstract": "Internet-of-Things (IoT) devices are often used to transmit physical sensor data over digital wireless channels. Traditional Physical Layer Security (PLS)-based cryptography approaches rely on accurate channel estimation and information exchange for key generation, which irrevocably ties key quality with digital channel estimation quality. Recently, we proposed a new concept called Graph Layer Security (GLS), where digital keys are derived from physical sensor readings. The sensor readings between legitimate users are correlated through a common background infrastructure environment (e.g., a common water distribution network or electric grid). The challenge for GLS has been how to achieve distributed key generation. This paper presents a Federated multi-agent Deep reinforcement learning-assisted Distributed Key generation scheme (FD2K), which fully exploits the common features of physical dynamics to establish secret key between legitimate users. We present for the first time initial experimental results of GLS with federated learning, achieving considerable security performance in terms of key agreement rate (KAR), and key randomness.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.12592",
        "string": "[Securing IoT Communication using Physical Sensor Data -- Graph Layer Security with Federated Multi-Agent Deep Reinforcement Learning](https://arxiv.org/pdf/2302.12592)"
    },
    "Selective experience replay compression using coresets for lifelong deep reinforcement learning in medical imaging": {
        "abstract": "Selective experience replay is a popular strategy for integrating lifelong learning with deep reinforcement learning. Selective experience replay aims to recount selected experiences from previous tasks to avoid catastrophic forgetting. Furthermore, selective experience replay based techniques are model agnostic and allow experiences to be shared across different models. However, storing experiences from all previous tasks make lifelong learning using selective experience replay computationally very expensive and impractical as the number of tasks increase. To that end, we propose a reward distribution-preserving coreset compression technique for compressing experience replay buffers stored for selective experience replay.\n  We evaluated the coreset compression technique on the brain tumor segmentation (BRATS) dataset for the task of ventricle localization and on the whole-body MRI for localization of left knee cap, left kidney, right trochanter, left lung, and spleen. The coreset lifelong learning models trained on a sequence of 10 different brain MR imaging environments demonstrated excellent performance localizing the ventricle with a mean pixel error distance of 12.93 for the compression ratio of 10x. In comparison, the conventional lifelong learning model localized the ventricle with a mean pixel distance of 10.87. Similarly, the coreset lifelong learning models trained on whole-body MRI demonstrated no significant difference (p=0.28) between the 10x compressed coreset lifelong learning models and conventional lifelong learning models for all the landmarks. The mean pixel distance for the 10x compressed models across all the landmarks was 25.30, compared to 19.24 for the conventional lifelong learning models. Our results demonstrate that the potential of the coreset-based ERB compression method for compressing experiences without a significant drop in performance.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.11510",
        "string": "[Selective experience replay compression using coresets for lifelong deep reinforcement learning in medical imaging](https://arxiv.org/pdf/2302.11510)"
    },
    "Semantic Information Marketing in The Metaverse: A Learning-Based Contract Theory Framework": {
        "abstract": "In this paper, we address the problem of designing incentive mechanisms by a virtual service provider (VSP) to hire sensing IoT devices to sell their sensing data to help creating and rendering the digital copy of the physical world in the Metaverse. Due to the limited bandwidth, we propose to use semantic extraction algorithms to reduce the delivered data by the sensing IoT devices. Nevertheless, mechanisms to hire sensing IoT devices to share their data with the VSP and then deliver the constructed digital twin to the Metaverse users are vulnerable to adverse selection problem. The adverse selection problem, which is caused by information asymmetry between the system entities, becomes harder to solve when the private information of the different entities are multi-dimensional. We propose a novel iterative contract design and use a new variant of multi-agent reinforcement learning (MARL) to solve the modelled multi-dimensional contract problem. To demonstrate the effectiveness of our algorithm, we conduct extensive simulations and measure several key performance metrics of the contract for the Metaverse. Our results show that our designed iterative contract is able to incentivize the participants to interact truthfully, which maximizes the profit of the VSP with minimal individual rationality (IR) and incentive compatibility (IC) violation rates. Furthermore, the proposed learning-based iterative contract framework has limited access to the private information of the participants, which is to the best of our knowledge, the first of its kind in addressing the problem of adverse selection in incentive mechanisms.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.11457",
        "string": "[Semantic Information Marketing in The Metaverse: A Learning-Based Contract Theory Framework](https://arxiv.org/pdf/2302.11457)"
    },
    "Stochastic Approximation Beyond Gradient for Signal Processing and Machine Learning": {
        "abstract": "Stochastic approximation (SA) is a classical algorithm that has had since the early days a huge impact on signal processing, and nowadays on machine learning, due to the necessity to deal with a large amount of data observed with uncertainties. An exemplar special case of SA pertains to the popular stochastic (sub)gradient algorithm which is the working horse behind many important applications. A lesser-known fact is that the SA scheme also extends to non-stochastic-gradient algorithms such as compressed stochastic gradient, stochastic expectation-maximization, and a number of reinforcement learning algorithms. The aim of this article is to overview and introduce the non-stochastic-gradient perspectives of SA to the signal processing and machine learning audiences through presenting a design guideline of SA algorithms backed by theories. Our central theme is to propose a general framework that unifies existing theories of SA, including its non-asymptotic and asymptotic convergence results, and demonstrate their applications on popular non-stochastic-gradient algorithms. We build our analysis framework based on classes of Lyapunov functions that satisfy a variety of mild conditions. We draw connections between non-stochastic-gradient algorithms and scenarios when the Lyapunov function is smooth, convex, or strongly convex. Using the said framework, we illustrate the convergence properties of the non-stochastic-gradient algorithms using concrete examples. Extensions to the emerging variance reduction techniques for improved sample complexity will also be discussed.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.11147",
        "string": "[Stochastic Approximation Beyond Gradient for Signal Processing and Machine Learning](https://arxiv.org/pdf/2302.11147)"
    },
    "Take Me Home: Reversing Distribution Shifts using Reinforcement Learning": {
        "abstract": "Deep neural networks have repeatedly been shown to be non-robust to the uncertainties of the real world. Even subtle adversarial attacks and naturally occurring distribution shifts wreak havoc on systems relying on deep neural networks. In response to this, current state-of-the-art techniques use data-augmentation to enrich the training distribution of the model and consequently improve robustness to natural distribution shifts. We propose an alternative approach that allows the system to recover from distribution shifts online. Specifically, our method applies a sequence of semantic-preserving transformations to bring the shifted data closer in distribution to the training set, as measured by the Wasserstein distance. We formulate the problem of sequence selection as an MDP, which we solve using reinforcement learning. To aid in our estimates of Wasserstein distance, we employ dimensionality reduction through orthonormal projection. We provide both theoretical and empirical evidence that orthonormal projection preserves characteristics of the data at the distributional level. Finally, we apply our distribution shift recovery approach to the ImageNet-C benchmark for distribution shifts, targeting shifts due to additive noise and image histogram modifications. We demonstrate an improvement in average accuracy up to 14.21% across a variety of state-of-the-art ImageNet classifiers.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.10341",
        "string": "[Take Me Home: Reversing Distribution Shifts using Reinforcement Learning](https://arxiv.org/pdf/2302.10341)"
    },
    "Targeted Search Control in AlphaZero for Effective Policy Improvement": {
        "abstract": "AlphaZero is a self-play reinforcement learning algorithm that achieves superhuman play in chess, shogi, and Go via policy iteration. To be an effective policy improvement operator, AlphaZero's search requires accurate value estimates for the states appearing in its search tree. AlphaZero trains upon self-play matches beginning from the initial state of a game and only samples actions over the first few moves, limiting its exploration of states deeper in the game tree. We introduce Go-Exploit, a novel search control strategy for AlphaZero. Go-Exploit samples the start state of its self-play trajectories from an archive of states of interest. Beginning self-play trajectories from varied starting states enables Go-Exploit to more effectively explore the game tree and to learn a value function that generalizes better. Producing shorter self-play trajectories allows Go-Exploit to train upon more independent value targets, improving value training. Finally, the exploration inherent in Go-Exploit reduces its need for exploratory actions, enabling it to train under more exploitative policies. In the games of Connect Four and 9x9 Go, we show that Go-Exploit learns with a greater sample efficiency than standard AlphaZero, resulting in stronger performance against reference opponents and in head-to-head play. We also compare Go-Exploit to KataGo, a more sample efficient reimplementation of AlphaZero, and demonstrate that Go-Exploit has a more effective search control strategy. Furthermore, Go-Exploit's sample efficiency improves when KataGo's other innovations are incorporated.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.12359",
        "string": "[Targeted Search Control in AlphaZero for Effective Policy Improvement](https://arxiv.org/pdf/2302.12359)"
    },
    "The Dormant Neuron Phenomenon in Deep Reinforcement Learning": {
        "abstract": "In this work we identify the dormant neuron phenomenon in deep reinforcement learning, where an agent's network suffers from an increasing number of inactive neurons, thereby affecting network expressivity. We demonstrate the presence of this phenomenon across a variety of algorithms and environments, and highlight its effect on learning. To address this issue, we propose a simple and effective method (ReDo) that Recycles Dormant neurons throughout training. Our experiments demonstrate that ReDo maintains the expressive power of networks by reducing the number of dormant neurons and results in improved performance.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.12902",
        "string": "[The Dormant Neuron Phenomenon in Deep Reinforcement Learning](https://arxiv.org/pdf/2302.12902)"
    },
    "To the Noise and Back: Diffusion for Shared Autonomy": {
        "abstract": "Shared autonomy is an operational concept in which a user and an autonomous agent collaboratively control a robotic system. It provides a number of advantages over the extremes of full-teleoperation and full-autonomy in many settings. Traditional approaches to shared autonomy rely on knowledge of the environment dynamics, a discrete space of user goals that is known a priori, or knowledge of the user's policy -- assumptions that are unrealistic in many domains. Recent works relax some of these assumptions by formulating shared autonomy with model-free deep reinforcement learning (RL). In particular, they no longer need knowledge of the goal space (e.g., that the goals are discrete or constrained) or environment dynamics. However, they need knowledge of a task-specific reward function to train the policy. Unfortunately, such reward specification can be a difficult and brittle process. On top of that, the formulations inherently rely on human-in-the-loop training, and that necessitates them to prepare a policy that mimics users' behavior. In this paper, we present a new approach to shared autonomy that employs a modulation of the forward and reverse diffusion process of diffusion models. Our approach does not assume known environment dynamics or the space of user goals, and in contrast to previous work, it does not require any reward feedback, nor does it require access to the user's policy during training. Instead, our framework learns a distribution over a space of desired behaviors. It then employs a diffusion model to translate the user's actions to a sample from this distribution. Crucially, we show that it is possible to carry out this process in a manner that preserves the user's control authority. We evaluate our framework on a series of challenging continuous control tasks, and analyze its ability to effectively correct user actions while maintaining their autonomy.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.12244",
        "string": "[To the Noise and Back: Diffusion for Shared Autonomy](https://arxiv.org/pdf/2302.12244)"
    },
    "Towards Decentralized Predictive Quality of Service in Next-Generation Vehicular Networks": {
        "abstract": "To ensure safety in teleoperated driving scenarios, communication between vehicles and remote drivers must satisfy strict latency and reliability requirements. In this context, Predictive Quality of Service (PQoS) was investigated as a tool to predict unanticipated degradation of the Quality of Service (QoS), and allow the network to react accordingly. In this work, we design a reinforcement learning (RL) agent to implement PQoS in vehicular networks. To do so, based on data gathered at the Radio Access Network (RAN) and/or the end vehicles, as well as QoS predictions, our framework is able to identify the optimal level of compression to send automotive data under low latency and reliability constraints. We consider different learning schemes, including centralized, fully-distributed, and federated learning. We demonstrate via ns-3 simulations that, while centralized learning generally outperforms any other solution, decentralized learning, and especially federated learning, offers a good trade-off between convergence time and reliability, with positive implications in terms of privacy and complexity.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.11268",
        "string": "[Towards Decentralized Predictive Quality of Service in Next-Generation Vehicular Networks](https://arxiv.org/pdf/2302.11268)"
    },
    "Towards a Sustainable Internet-of-Underwater-Things based on AUVs, SWIPT, and Reinforcement Learning": {
        "abstract": "Life on earth depends on healthy oceans, which supply a large percentage of the planet's oxygen, food, and energy. However, the oceans are under threat from climate change, which is devastating the marine ecosystem and the economic and social systems that depend on it. The Internet-of-underwater-things (IoUTs), a global interconnection of underwater objects, enables round-the-clock monitoring of the oceans. It provides high-resolution data for training machine learning (ML) algorithms for rapidly evaluating potential climate change solutions and speeding up decision-making. The sensors in conventional IoUTs are battery-powered, which limits their lifetime, and constitutes environmental hazards when they die. In this paper, we propose a sustainable scheme to improve the throughput and lifetime of underwater networks, enabling them to potentially operate indefinitely. The scheme is based on simultaneous wireless information and power transfer (SWIPT) from an autonomous underwater vehicle (AUV) used for data collection. We model the problem of jointly maximising throughput and harvested power as a Markov Decision Process (MDP), and develop a model-free reinforcement learning (RL) algorithm as a solution. The model's reward function incentivises the AUV to find optimal trajectories that maximise throughput and power transfer to the underwater nodes while minimising energy consumption. To the best of our knowledge, this is the first attempt at using RL to ensure sustainable underwater networks via SWIPT. The scheme is implemented in an open 3D RL environment specifically developed in MATLAB for this study. The performance results show up 207% improvement in energy efficiency compared to those of a random trajectory scheme used as a baseline model.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.10368",
        "string": "[Towards a Sustainable Internet-of-Underwater-Things based on AUVs, SWIPT, and Reinforcement Learning](https://arxiv.org/pdf/2302.10368)"
    },
    "Trustworthy Reinforcement Learning for Quadrotor UAV Tracking Control Systems": {
        "abstract": "Simultaneously accurate and reliable tracking control for quadrotors in complex dynamic environments is challenging. As aerodynamics derived from drag forces and moment variations are chaotic and difficult to precisely identify, most current quadrotor tracking systems treat them as simple `disturbances' in conventional control approaches. We propose a novel, interpretable trajectory tracker integrating a Distributional Reinforcement Learning disturbance estimator for unknown aerodynamic effects with a Stochastic Model Predictive Controller (SMPC). The proposed estimator `Constrained Distributional Reinforced disturbance estimator' (ConsDRED) accurately identifies uncertainties between true and estimated values of aerodynamic effects. Simplified Affine Disturbance Feedback is used for control parameterization to guarantee convexity, which we then integrate with a SMPC. We theoretically guarantee that ConsDRED achieves at least an optimal global convergence rate and a certain sublinear rate if constraints are violated with an error decreases as the width and the layer of neural network increase. To demonstrate practicality, we show convergent training in simulation and real-world experiments, and empirically verify that ConsDRED is less sensitive to hyperparameter settings compared with canonical constrained RL approaches. We demonstrate our system improves accumulative tracking errors by at least 62% compared with the recent art. Importantly, the proposed framework, ConsDRED-SMPC, balances the tradeoff between pursuing high performance and obeying conservative constraints for practical implementations\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.11694",
        "string": "[Trustworthy Reinforcement Learning for Quadrotor UAV Tracking Control Systems](https://arxiv.org/pdf/2302.11694)"
    },
    "UAV Path Planning Employing MPC- Reinforcement Learning Method for search and rescue mission": {
        "abstract": "In this paper, we tackle the problem of Unmanned Aerial (UA V) path planning in complex and uncertain environments by designing a Model Predictive Control (MPC), based on a Long-Short-Term Memory (LSTM) network integrated into the Deep Deterministic Policy Gradient algorithm. In the proposed solution, LSTM-MPC operates as a deterministic policy within the DDPG network, and it leverages a predicting pool to store predicted future states and actions for improved robustness and efficiency. The use of the predicting pool also enables the initialization of the critic network, leading to improved convergence speed and reduced failure rate compared to traditional reinforcement learning and deep reinforcement learning methods. The effectiveness of the proposed solution is evaluated by numerical simulations.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.10669",
        "string": "[UAV Path Planning Employing MPC- Reinforcement Learning Method for search and rescue mission](https://arxiv.org/pdf/2302.10669)"
    },
    "Understanding the effect of varying amounts of replay per step": {
        "abstract": "Model-based reinforcement learning uses models to plan, where the predictions and policies of an agent can be improved by using more computation without additional data from the environment, thereby improving sample efficiency. However, learning accurate estimates of the model is hard. Subsequently, the natural question is whether we can get similar benefits as planning with model-free methods. Experience replay is an essential component of many model-free algorithms enabling sample-efficient learning and stability by providing a mechanism to store past experiences for further reuse in the gradient computational process. Prior works have established connections between models and experience replay by planning with the latter. This involves increasing the number of times a mini-batch is sampled and used for updates at each step (amount of replay per step). We attempt to exploit this connection by doing a systematic study on the effect of varying amounts of replay per step in a well-known model-free algorithm: Deep Q-Network (DQN) in the Mountain Car environment. We empirically show that increasing replay improves DQN's sample efficiency, reduces the variation in its performance, and makes it more robust to change in hyperparameters. Altogether, this takes a step toward a better algorithm for deployment.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.10311",
        "string": "[Understanding the effect of varying amounts of replay per step](https://arxiv.org/pdf/2302.10311)"
    },
    "Universal Morphology Control via Contextual Modulation": {
        "abstract": "Learning a universal policy across different robot morphologies can significantly improve learning efficiency and generalization in continuous control. However, it poses a challenging multi-task reinforcement learning problem, as the optimal policy may be quite different across robots and critically depend on the morphology. Existing methods utilize graph neural networks or transformers to handle heterogeneous state and action spaces across different morphologies, but pay little attention to the dependency of a robot's control policy on its morphology context. In this paper, we propose a hierarchical architecture to better model this dependency via contextual modulation, which includes two key submodules: (1) Instead of enforcing hard parameter sharing across robots, we use hypernetworks to generate morphology-dependent control parameters; (2) We propose a morphology-dependent attention mechanism to modulate the interactions between different limbs in a robot. Experimental results show that our method not only improves learning performance on a diverse set of training robots, but also generalizes better to unseen morphologies in a zero-shot fashion.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.11070",
        "string": "[Universal Morphology Control via Contextual Modulation](https://arxiv.org/pdf/2302.11070)"
    },
    "Variance-Dependent Regret Bounds for Linear Bandits and Reinforcement Learning: Adaptivity and Computational Efficiency": {
        "abstract": "Recently, several studies (Zhou et al., 2021a; Zhang et al., 2021b; Kim et al., 2021; Zhou and Gu, 2022) have provided variance-dependent regret bounds for linear contextual bandits, which interpolates the regret for the worst-case regime and the deterministic reward regime. However, these algorithms are either computationally intractable or unable to handle unknown variance of the noise. In this paper, we present a novel solution to this open problem by proposing the first computationally efficient algorithm for linear bandits with heteroscedastic noise. Our algorithm is adaptive to the unknown variance of noise and achieves an $\\tilde{O}(d \\sqrt{\\sum_{k = 1}^K \u03c3_k^2} + d)$ regret, where $\u03c3_k^2$ is the variance of the noise at the round $k$, $d$ is the dimension of the contexts and $K$ is the total number of rounds. Our results are based on an adaptive variance-aware confidence set enabled by a new Freedman-type concentration inequality for self-normalized martingales and a multi-layer structure to stratify the context vectors into different layers with different uniform upper bounds on the uncertainty.\n  Furthermore, our approach can be extended to linear mixture Markov decision processes (MDPs) in reinforcement learning. We propose a variance-adaptive algorithm for linear mixture MDPs, which achieves a problem-dependent horizon-free regret bound that can gracefully reduce to a nearly constant regret for deterministic MDPs. Unlike existing nearly minimax optimal algorithms for linear mixture MDPs, our algorithm does not require explicit variance estimation of the transitional probabilities or the use of high-order moment estimators to attain horizon-free regret. We believe the techniques developed in this paper can have independent value for general online decision making problems.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.10371",
        "string": "[Variance-Dependent Regret Bounds for Linear Bandits and Reinforcement Learning: Adaptivity and Computational Efficiency](https://arxiv.org/pdf/2302.10371)"
    },
    "Why Target Networks Stabilise Temporal Difference Methods": {
        "abstract": "Integral to recent successes in deep reinforcement learning has been a class of temporal difference methods that use infrequently updated target values for policy evaluation in a Markov Decision Process. Yet a complete theoretical explanation for the effectiveness of target networks remains elusive. In this work, we provide an analysis of this popular class of algorithms, to finally answer the question: `why do target networks stabilise TD learning'? To do so, we formalise the notion of a partially fitted policy evaluation method, which describes the use of target networks and bridges the gap between fitted methods and semigradient temporal difference algorithms. Using this framework we are able to uniquely characterise the so-called deadly triad - the use of TD updates with (nonlinear) function approximation and off-policy data - which often leads to nonconvergent algorithms. This insight leads us to conclude that the use of target networks can mitigate the effects of poor conditioning in the Jacobian of the TD update. Instead, we show that under mild regularity conditions and a well tuned target network update frequency, convergence can be guaranteed even in the extremely challenging off-policy sampling and nonlinear function approximation setting.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2302.12537",
        "string": "[Why Target Networks Stabilise Temporal Difference Methods](https://arxiv.org/pdf/2302.12537)"
    }
}
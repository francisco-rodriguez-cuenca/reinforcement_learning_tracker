{
    "A Closer Look at Reward Decomposition for High-Level Robotic Explanations": {
        "abstract": "Explaining the behavior of intelligent agents such as robots to humans is challenging due to their incomprehensible proprioceptive states, variational intermediate goals, and resultant unpredictability. Moreover, one-step explanations for reinforcement learning agents can be ambiguous as they fail to account for the agent's future behavior at each transition, adding to the complexity of explaining robot actions. By leveraging abstracted actions that map to task-specific primitives, we avoid explanations on the movement level. Our proposed framework combines reward decomposition (RD) with abstracted action spaces into an explainable learning framework, allowing for non-ambiguous and high-level explanations based on object properties in the task. We demonstrate the effectiveness of our framework through quantitative and qualitative analysis of two robot scenarios, showcasing visual and textual explanations, from output artifacts of RD explanation, that are easy for humans to comprehend. Additionally, we demonstrate the versatility of integrating these artifacts with large language models for reasoning and interactive querying.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.12958",
        "string": "[A Closer Look at Reward Decomposition for High-Level Robotic Explanations](https://arxiv.org/pdf/2304.12958)"
    },
    "A Coupled Flow Approach to Imitation Learning": {
        "abstract": "In reinforcement learning and imitation learning, an object of central importance is the state distribution induced by the policy. It plays a crucial role in the policy gradient theorem, and references to it--along with the related state-action distribution--can be found all across the literature. Despite its importance, the state distribution is mostly discussed indirectly and theoretically, rather than being modeled explicitly. The reason being an absence of appropriate density estimation tools. In this work, we investigate applications of a normalizing flow-based model for the aforementioned distributions. In particular, we use a pair of flows coupled through the optimality point of the Donsker-Varadhan representation of the Kullback-Leibler (KL) divergence, for distribution matching based imitation learning. Our algorithm, Coupled Flow Imitation Learning (CFIL), achieves state-of-the-art performance on benchmark tasks with a single expert trajectory and extends naturally to a variety of other settings, including the subsampled and state-only regimes.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2305.00303",
        "string": "[A Coupled Flow Approach to Imitation Learning](https://arxiv.org/pdf/2305.00303)"
    },
    "A Federated Reinforcement Learning Framework for Link Activation in Multi-link Wi-Fi Networks": {
        "abstract": "Next-generation Wi-Fi networks are looking forward to introducing new features like multi-link operation (MLO) to both achieve higher throughput and lower latency. However, given the limited number of available channels, the use of multiple links by a group of contending Basic Service Sets (BSSs) can result in higher interference and channel contention, thus potentially leading to lower performance and reliability. In such a situation, it could be better for all contending BSSs to use less links if that contributes to reduce channel access contention. Recently, reinforcement learning (RL) has proven its potential for optimizing resource allocation in wireless networks. However, the independent operation of each wireless network makes difficult -- if not almost impossible -- for each individual network to learn a good configuration. To solve this issue, in this paper, we propose the use of a Federated Reinforcement Learning (FRL) framework, i.e., a collaborative machine learning approach to train models across multiple distributed agents without exchanging data, to collaboratively learn the the best MLO-Link Allocation (LA) strategy by a group of neighboring BSSs. The simulation results show that the FRL-based decentralized MLO-LA strategy achieves a better throughput fairness, and so a higher reliability -- because it allows the different BSSs to find a link allocation strategy which maximizes the minimum achieved data rate -- compared to fixed, random and RL-based MLO-LA schemes.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.14720",
        "string": "[A Federated Reinforcement Learning Framework for Link Activation in Multi-link Wi-Fi Networks](https://arxiv.org/pdf/2304.14720)"
    },
    "A Multi-Task Approach to Robust Deep Reinforcement Learning for Resource Allocation": {
        "abstract": "With increasing complexity of modern communication systems, machine learning algorithms have become a focal point of research. However, performance demands have tightened in parallel to complexity. For some of the key applications targeted by future wireless, such as the medical field, strict and reliable performance guarantees are essential, but vanilla machine learning methods have been shown to struggle with these types of requirements. Therefore, the question is raised whether these methods can be extended to better deal with the demands imposed by such applications. In this paper, we look at a combinatorial resource allocation challenge with rare, significant events which must be handled properly. We propose to treat this as a multi-task learning problem, select two methods from this domain, Elastic Weight Consolidation and Gradient Episodic Memory, and integrate them into a vanilla actor-critic scheduler. We compare their performance in dealing with Black Swan Events with the state-of-the-art of augmenting the training data distribution and report that the multi-task approach proves highly effective.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.12660",
        "string": "[A Multi-Task Approach to Robust Deep Reinforcement Learning for Resource Allocation](https://arxiv.org/pdf/2304.12660)"
    },
    "A optimization framework for herbal prescription planning based on deep reinforcement learning": {
        "abstract": "Treatment planning for chronic diseases is a critical task in medical artificial intelligence, particularly in traditional Chinese medicine (TCM). However, generating optimized sequential treatment strategies for patients with chronic diseases in different clinical encounters remains a challenging issue that requires further exploration. In this study, we proposed a TCM herbal prescription planning framework based on deep reinforcement learning for chronic disease treatment (PrescDRL). PrescDRL is a sequential herbal prescription optimization model that focuses on long-term effectiveness rather than achieving maximum reward at every step, thereby ensuring better patient outcomes. We constructed a high-quality benchmark dataset for sequential diagnosis and treatment of diabetes and evaluated PrescDRL against this benchmark. Our results showed that PrescDRL achieved a higher curative effect, with the single-step reward improving by 117% and 153% compared to doctors. Furthermore, PrescDRL outperformed the benchmark in prescription prediction, with precision improving by 40.5% and recall improving by 63%. Overall, our study demonstrates the potential of using artificial intelligence to improve clinical intelligent diagnosis and treatment in TCM.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.12828",
        "string": "[A optimization framework for herbal prescription planning based on deep reinforcement learning](https://arxiv.org/pdf/2304.12828)"
    },
    "ALL-E: Aesthetics-guided Low-light Image Enhancement": {
        "abstract": "Evaluating the performance of low-light image enhancement (LLE) is highly subjective, thus making integrating human preferences into image enhancement a necessity. Existing methods fail to consider this and present a series of potentially valid heuristic criteria for training enhancement models. In this paper, we propose a new paradigm, i.e., aesthetics-guided low-light image enhancement (ALL-E), which introduces aesthetic preferences to LLE and motivates training in a reinforcement learning framework with an aesthetic reward. Each pixel, functioning as an agent, refines itself by recursive actions, i.e., its corresponding adjustment curve is estimated sequentially. Extensive experiments show that integrating aesthetic assessment improves both subjective experience and objective evaluation. Our results on various benchmarks demonstrate the superiority of ALL-E over state-of-the-art methods.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.14610",
        "string": "[ALL-E: Aesthetics-guided Low-light Image Enhancement](https://arxiv.org/pdf/2304.14610)"
    },
    "Active Reinforcement Learning for Personalized Stress Monitoring in Everyday Settings": {
        "abstract": "Most existing sensor-based monitoring frameworks presume that a large available labeled dataset is processed to train accurate detection models. However, in settings where personalization is necessary at deployment time to fine-tune the model, a person-specific dataset needs to be collected online by interacting with the users. Optimizing the collection of labels in such phase is instrumental to impose a tolerable burden on the users while maximizing personal improvement. In this paper, we consider a fine-grain stress detection problem based on wearable sensors targeting everyday settings, and propose a novel context-aware active learning strategy capable of jointly maximizing the meaningfulness of the signal samples we request the user to label and the response rate. We develop a multilayered sensor-edge-cloud platform to periodically capture physiological signals and process them in real-time, as well as to collect labels and retrain the detection model. We collect a large dataset and show that the context-aware active learning technique we propose achieves a desirable detection performance using 88\\% and 32\\% fewer queries from users compared to a randomized strategy and a traditional active learning strategy, respectively.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2305.00111",
        "string": "[Active Reinforcement Learning for Personalized Stress Monitoring in Everyday Settings](https://arxiv.org/pdf/2305.00111)"
    },
    "Adversarial Policy Optimization in Deep Reinforcement Learning": {
        "abstract": "The policy represented by the deep neural network can overfit the spurious features in observations, which hamper a reinforcement learning agent from learning effective policy. This issue becomes severe in high-dimensional state, where the agent struggles to learn a useful policy. Data augmentation can provide a performance boost to RL agents by mitigating the effect of overfitting. However, such data augmentation is a form of prior knowledge, and naively applying them in environments might worsen an agent's performance. In this paper, we propose a novel RL algorithm to mitigate the above issue and improve the efficiency of the learned policy. Our approach consists of a max-min game theoretic objective where a perturber network modifies the state to maximize the agent's probability of taking a different action while minimizing the distortion in the state. In contrast, the policy network updates its parameters to minimize the effect of perturbation while maximizing the expected future reward. Based on this objective, we propose a practical deep reinforcement learning algorithm, Adversarial Policy Optimization (APO). Our method is agnostic to the type of policy optimization, and thus data augmentation can be incorporated to harness the benefit. We evaluated our approaches on several DeepMind Control robotic environments with high-dimensional and noisy state settings. Empirical results demonstrate that our method APO consistently outperforms the state-of-the-art on-policy PPO agent. We further compare our method with state-of-the-art data augmentation, RAD, and regularization-based approach DRAC. Our agent APO shows better performance compared to these baselines.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.14533",
        "string": "[Adversarial Policy Optimization in Deep Reinforcement Learning](https://arxiv.org/pdf/2304.14533)"
    },
    "An efficient neural optimizer for resonant nanostructures: demonstration of highly-saturated red silicon structural color": {
        "abstract": "Freeform nanostructures have the potential to support complex resonances and their interactions, which are crucial for achieving desired spectral responses. However, the design optimization of such structures is nontrivial and computationally intensive. Furthermore, the current \"black box\" design approaches for freeform nanostructures often neglect the underlying physics. Here, we present a hybrid data-efficient neural optimizer for resonant nanostructures by combining a reinforcement learning algorithm and Powell's local optimization technique. As a case study, we design and experimentally demonstrate silicon nanostructures with a highly-saturated red color. Specifically, we achieved CIE color coordinates of (0.677, 0.304)-close to the ideal Schrodinger's red, with polarization independence, high reflectance (>85%), and a large viewing angle (i.e., up to ~ 25deg). The remarkable performance is attributed to underlying generalized multipolar interferences within each nanostructure rather than the collective array effects. Based on that, we were able to demonstrate pixel size down to ~400 nm, corresponding to a printing resolution of 65,000 pixels per inch. Moreover, the proposed design model requires only ~300 iterations to effectively search a 13-dimensional design space - an order of magnitude more efficient than the previously reported approaches. Our work significantly extends the free-form optical design toolbox for high-performance flat-optical components and metadevices.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.13516",
        "string": "[An efficient neural optimizer for resonant nanostructures: demonstration of highly-saturated red silicon structural color](https://arxiv.org/pdf/2304.13516)"
    },
    "Batch Quantum Reinforcement Learning": {
        "abstract": "Training DRL agents is often a time-consuming process as a large number of samples and environment interactions is required. This effect is even amplified in the case of Batch RL, where the agent is trained without environment interactions solely based on a set of previously collected data. Novel approaches based on quantum computing suggest an advantage compared to classical approaches in terms of sample efficiency. To investigate this advantage, we propose a batch RL algorithm leveraging VQC as function approximators in the discrete BCQ algorithm. Additionally, we present a novel data re-uploading scheme based on cyclically shifting the input variables' order in the data encoding layers. We show the efficiency of our algorithm on the OpenAI CartPole environment and compare its performance to classical neural network-based discrete BCQ.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2305.00905",
        "string": "[Batch Quantum Reinforcement Learning](https://arxiv.org/pdf/2305.00905)"
    },
    "CROP: Towards Distributional-Shift Robust Reinforcement Learning using Compact Reshaped Observation Processing": {
        "abstract": "The safe application of reinforcement learning (RL) requires generalization from limited training data to unseen scenarios. Yet, fulfilling tasks under changing circumstances is a key challenge in RL. Current state-of-the-art approaches for generalization apply data augmentation techniques to increase the diversity of training data. Even though this prevents overfitting to the training environment(s), it hinders policy optimization. Crafting a suitable observation, only containing crucial information, has been shown to be a challenging task itself. To improve data efficiency and generalization capabilities, we propose Compact Reshaped Observation Processing (CROP) to reduce the state information used for policy optimization. By providing only relevant information, overfitting to a specific training layout is precluded and generalization to unseen environments is improved. We formulate three CROPs that can be applied to fully observable observation- and action-spaces and provide methodical foundation. We empirically show the improvements of CROP in a distributionally shifted safety gridworld. We furthermore provide benchmark comparisons to full observability and data-augmentation in two different-sized procedurally generated mazes.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.13616",
        "string": "[CROP: Towards Distributional-Shift Robust Reinforcement Learning using Compact Reshaped Observation Processing](https://arxiv.org/pdf/2304.13616)"
    },
    "Can Agents Run Relay Race with Strangers? Generalization of RL to Out-of-Distribution Trajectories": {
        "abstract": "In this paper, we define, evaluate, and improve the ``relay-generalization'' performance of reinforcement learning (RL) agents on the out-of-distribution ``controllable'' states. Ideally, an RL agent that generally masters a task should reach its goal starting from any controllable state of the environment instead of memorizing a small set of trajectories. For example, a self-driving system should be able to take over the control from humans in the middle of driving and must continue to drive the car safely. To practically evaluate this type of generalization, we start the test agent from the middle of other independently well-trained \\emph{stranger} agents' trajectories. With extensive experimental evaluation, we show the prevalence of \\emph{generalization failure} on controllable states from stranger agents. For example, in the Humanoid environment, we observed that a well-trained Proximal Policy Optimization (PPO) agent, with only 3.9\\% failure rate during regular testing, failed on 81.6\\% of the states generated by well-trained stranger PPO agents. To improve \"relay generalization,\" we propose a novel method called Self-Trajectory Augmentation (STA), which will reset the environment to the agent's old states according to the Q function during training. After applying STA to the Soft Actor Critic's (SAC) training procedure, we reduced the failure rate of SAC under relay-evaluation by more than three times in most settings without impacting agent performance and increasing the needed number of environment interactions. Our code is available at https://github.com/lan-lc/STA.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.13424",
        "string": "[Can Agents Run Relay Race with Strangers? Generalization of RL to Out-of-Distribution Trajectories](https://arxiv.org/pdf/2304.13424)"
    },
    "Centralized control for multi-agent RL in a complex Real-Time-Strategy game": {
        "abstract": "Multi-agent Reinforcement learning (MARL) studies the behaviour of multiple learning agents that coexist in a shared environment. MARL is more challenging than single-agent RL because it involves more complex learning dynamics: the observations and rewards of each agent are functions of all other agents. In the context of MARL, Real-Time Strategy (RTS) games represent very challenging environments where multiple players interact simultaneously and control many units of different natures all at once. In fact, RTS games are so challenging for the current RL methods, that just being able to tackle them with RL is interesting. This project provides the end-to-end experience of applying RL in the Lux AI v2 Kaggle competition, where competitors design agents to control variable-sized fleets of units and tackle a multi-variable optimization, resource gathering, and allocation problem in a 1v1 scenario against other competitors. We use a centralized approach for training the RL agents, and report multiple design decisions along the process. We provide the source code of the project: https://github.com/roger-creus/centralized-control-lux.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.13004",
        "string": "[Centralized control for multi-agent RL in a complex Real-Time-Strategy game](https://arxiv.org/pdf/2304.13004)"
    },
    "Contrastive Energy Prediction for Exact Energy-Guided Diffusion Sampling in Offline Reinforcement Learning": {
        "abstract": "Guided sampling is a vital approach for applying diffusion models in real-world tasks that embeds human-defined guidance during the sampling procedure. This paper considers a general setting where the guidance is defined by an (unnormalized) energy function. The main challenge for this setting is that the intermediate guidance during the diffusion sampling procedure, which is jointly defined by the sampling distribution and the energy function, is unknown and is hard to estimate. To address this challenge, we propose an exact formulation of the intermediate guidance as well as a novel training objective named contrastive energy prediction (CEP) to learn the exact guidance. Our method is guaranteed to converge to the exact guidance under unlimited model capacity and data samples, while previous methods can not. We demonstrate the effectiveness of our method by applying it to offline reinforcement learning (RL). Extensive experiments on D4RL benchmarks demonstrate that our method outperforms existing state-of-the-art algorithms. We also provide some examples of applying CEP for image synthesis to demonstrate the scalability of CEP on high-dimensional data.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.12824",
        "string": "[Contrastive Energy Prediction for Exact Energy-Guided Diffusion Sampling in Offline Reinforcement Learning](https://arxiv.org/pdf/2304.12824)"
    },
    "Cooperative Hierarchical Deep Reinforcement Learning based Joint Sleep, Power, and RIS Control for Energy-Efficient HetNet": {
        "abstract": "Energy efficiency (EE) is one of the most important metrics for 5G and future 6G networks to reduce energy costs and control carbon footprint. Sleep control, as a cost-efficient approach, can significantly lower power consumption by switching off network devices selectively. Meanwhile, reconfigurable intelligent surface (RIS) has emerged as a promising technique to enhance the EE of 5G beyond and 6G networks. In this work, we jointly consider sleep and transmission power control for reconfigurable intelligent surface (RIS)-aided energy-efficient heterogeneous networks (Hetnets). In particular, we first propose a fractional programming (FP) method for RIS phase-shift control, which aims to maximize the sum-rate under given transmission power levels. Then, considering the timescale difference between sleep control and power control, we introduce a cooperative hierarchical deep reinforcement learning (Co-HDRL) algorithm, including a cross-entropy enabled meta-controller for sleep control, and correlated equilibrium-based sub-controllers for power control. Moreover, we proposed a surrogate optimization method as one baseline for RIS control, and conventional HDRL as another baseline for sleep and power control. Finally, simulations show that the RIS-assisted sleep control can achieve more than 16% lower energy consumption and 30% higher energy efficiency than baseline algorithms.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.13226",
        "string": "[Cooperative Hierarchical Deep Reinforcement Learning based Joint Sleep, Power, and RIS Control for Energy-Efficient HetNet](https://arxiv.org/pdf/2304.13226)"
    },
    "Deep Reinforcement Learning in Finite-Horizon to Explore the Most Probable Transition Pathway": {
        "abstract": "This investigation focuses on discovering the most probable transition pathway for stochastic dynamical systems employing reinforcement learning. We first utilize Onsager-Machlup theory to quantify rare events in stochastic dynamical systems, and then convert the most likely transition path issue into a finite-horizon optimal control problem, because, in many instances, the transition path cannot be determined explicitly by variation. We propose the terminal prediction method and integrate it with reinforcement learning, develop our algorithm Finite Horizon Deep Deterministic Policy Gradient(FH-DDPG) to deal with the finite-horizon optimal control issue. Next, we present the convergence analysis of the algorithm for the value function estimation in terms of the neural network approximation error and the sampling error when estimating the network. Finally, experiments are conducted for the transition problem under Gaussian noise to verify the effectiveness of the algorithm.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.12994",
        "string": "[Deep Reinforcement Learning in Finite-Horizon to Explore the Most Probable Transition Pathway](https://arxiv.org/pdf/2304.12994)"
    },
    "Discovering Object-Centric Generalized Value Functions From Pixels": {
        "abstract": "Deep Reinforcement Learning has shown significant progress in extracting useful representations from high-dimensional inputs albeit using hand-crafted auxiliary tasks and pseudo rewards. Automatically learning such representations in an object-centric manner geared towards control and fast adaptation remains an open research problem. In this paper, we introduce a method that tries to discover meaningful features from objects, translating them to temporally coherent \"question\" functions and leveraging the subsequent learned general value functions for control. We compare our approach with state-of-the-art techniques alongside other ablations and show competitive performance in both stationary and non-stationary settings. Finally, we also investigate the discovered general value functions and through qualitative analysis show that the learned representations are not only interpretable but also, centered around objects that are invariant to changes across tasks facilitating fast adaptation.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.13892",
        "string": "[Discovering Object-Centric Generalized Value Functions From Pixels](https://arxiv.org/pdf/2304.13892)"
    },
    "Dynamic Datasets and Market Environments for Financial Reinforcement Learning": {
        "abstract": "The financial market is a particularly challenging playground for deep reinforcement learning due to its unique feature of dynamic datasets. Building high-quality market environments for training financial reinforcement learning (FinRL) agents is difficult due to major factors such as the low signal-to-noise ratio of financial data, survivorship bias of historical data, and model overfitting. In this paper, we present FinRL-Meta, a data-centric and openly accessible library that processes dynamic datasets from real-world markets into gym-style market environments and has been actively maintained by the AI4Finance community. First, following a DataOps paradigm, we provide hundreds of market environments through an automatic data curation pipeline. Second, we provide homegrown examples and reproduce popular research papers as stepping stones for users to design new trading strategies. We also deploy the library on cloud platforms so that users can visualize their own results and assess the relative performance via community-wise competitions. Third, we provide dozens of Jupyter/Python demos organized into a curriculum and a documentation website to serve the rapidly growing community. The open-source codes for the data curation pipeline are available at https://github.com/AI4Finance-Foundation/FinRL-Meta\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.13174",
        "string": "[Dynamic Datasets and Market Environments for Financial Reinforcement Learning](https://arxiv.org/pdf/2304.13174)"
    },
    "Efficient Halftoning via Deep Reinforcement Learning": {
        "abstract": "Halftoning aims to reproduce a continuous-tone image with pixels whose intensities are constrained to two discrete levels. This technique has been deployed on every printer, and the majority of them adopt fast methods (e.g., ordered dithering, error diffusion) that fail to render structural details, which determine halftone's quality. Other prior methods of pursuing visual pleasure by searching for the optimal halftone solution, on the contrary, suffer from their high computational cost. In this paper, we propose a fast and structure-aware halftoning method via a data-driven approach. Specifically, we formulate halftoning as a reinforcement learning problem, in which each binary pixel's value is regarded as an action chosen by a virtual agent with a shared fully convolutional neural network (CNN) policy. In the offline phase, an effective gradient estimator is utilized to train the agents in producing high-quality halftones in one action step. Then, halftones can be generated online by one fast CNN inference. Besides, we propose a novel anisotropy suppressing loss function, which brings the desirable blue-noise property. Finally, we find that optimizing SSIM could result in holes in flat areas, which can be avoided by weighting the metric with the contone's contrast map. Experiments show that our framework can effectively train a light-weight CNN, which is 15x faster than previous structure-aware methods, to generate blue-noise halftones with satisfactory visual quality. We also present a prototype of deep multitoning to demonstrate the extensibility of our method.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.12152",
        "string": "[Efficient Halftoning via Deep Reinforcement Learning](https://arxiv.org/pdf/2304.12152)"
    },
    "Exploring the flavor structure of quarks and leptons with reinforcement learning": {
        "abstract": "We propose a method to explore the flavor structure of quarks and leptons with reinforcement learning. As a concrete model, we utilize a basic policy-based algorithm for models with $U(1)$ flavor symmetry. By training neural networks on the $U(1)$ charges of quarks and leptons, the agent finds 21 models to be consistent with experimentally measured masses and mixing angles of quarks and leptons. In particular, an intrinsic value of normal ordering tends to be larger than that of inverted ordering, and the normal ordering is well fitted with the current experimental data in contrast to the inverted ordering. A specific value of effective mass for the neutrinoless double beta decay and a sizable leptonic CP violation induced by an angular component of flavon field are predicted by autonomous behavior of the agent.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.14176",
        "string": "[Exploring the flavor structure of quarks and leptons with reinforcement learning](https://arxiv.org/pdf/2304.14176)"
    },
    "FLEX: an Adaptive Exploration Algorithm for Nonlinear Systems": {
        "abstract": "Model-based reinforcement learning is a powerful tool, but collecting data to fit an accurate model of the system can be costly. Exploring an unknown environment in a sample-efficient manner is hence of great importance. However, the complexity of dynamics and the computational limitations of real systems make this task challenging. In this work, we introduce FLEX, an exploration algorithm for nonlinear dynamics based on optimal experimental design. Our policy maximizes the information of the next step and results in an adaptive exploration algorithm, compatible with generic parametric learning models and requiring minimal resources. We test our method on a number of nonlinear environments covering different settings, including time-varying dynamics. Keeping in mind that exploration is intended to serve an exploitation objective, we also test our algorithm on downstream model-based classical control tasks and compare it to other state-of-the-art model-based and model-free approaches. The performance achieved by FLEX is competitive and its computational cost is low.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.13426",
        "string": "[FLEX: an Adaptive Exploration Algorithm for Nonlinear Systems](https://arxiv.org/pdf/2304.13426)"
    },
    "Federated Deep Reinforcement Learning for THz-Beam Search with Limited CSI": {
        "abstract": "Terahertz (THz) communication with ultra-wide available spectrum is a promising technique that can achieve the stringent requirement of high data rate in the next-generation wireless networks, yet its severe propagation attenuation significantly hinders its implementation in practice. Finding beam directions for a large-scale antenna array to effectively overcome severe propagation attenuation of THz signals is a pressing need. This paper proposes a novel approach of federated deep reinforcement learning (FDRL) to swiftly perform THz-beam search for multiple base stations (BSs) coordinated by an edge server in a cellular network. All the BSs conduct deep deterministic policy gradient (DDPG)-based DRL to obtain THz beamforming policy with limited channel state information (CSI). They update their DDPG models with hidden information in order to mitigate inter-cell interference. We demonstrate that the cell network can achieve higher throughput as more THz CSI and hidden neurons of DDPG are adopted. We also show that FDRL with partial model update is able to nearly achieve the same performance of FDRL with full model update, which indicates an effective means to reduce communication load between the edge server and the BSs by partial model uploading. Moreover, the proposed FDRL outperforms conventional non-learning-based and existing non-FDRL benchmark optimization methods.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.13109",
        "string": "[Federated Deep Reinforcement Learning for THz-Beam Search with Limited CSI](https://arxiv.org/pdf/2304.13109)"
    },
    "Fulfilling Formal Specifications ASAP by Model-free Reinforcement Learning": {
        "abstract": "We propose a model-free reinforcement learning solution, namely the ASAP-Phi framework, to encourage an agent to fulfill a formal specification ASAP. The framework leverages a piece-wise reward function that assigns quantitative semantic reward to traces not satisfying the specification, and a high constant reward to the remaining. Then, it trains an agent with an actor-critic-based algorithm, such as soft actor-critic (SAC), or deep deterministic policy gradient (DDPG). Moreover, we prove that ASAP-Phi produces policies that prioritize fulfilling a specification ASAP. Extensive experiments are run, including ablation studies, on state-of-the-art benchmarks. Results show that our framework succeeds in finding sufficiently fast trajectories for up to 97\\% test cases and defeats baselines.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.12508",
        "string": "[Fulfilling Formal Specifications ASAP by Model-free Reinforcement Learning](https://arxiv.org/pdf/2304.12508)"
    },
    "Fundamental Tradeoffs in Learning with Prior Information": {
        "abstract": "We seek to understand fundamental tradeoffs between the accuracy of prior information that a learner has on a given problem and its learning performance. We introduce the notion of prioritized risk, which differs from traditional notions of minimax and Bayes risk by allowing us to study such fundamental tradeoffs in settings where reality does not necessarily conform to the learner's prior. We present a general reduction-based approach for extending classical minimax lower-bound techniques in order to lower bound the prioritized risk for statistical estimation problems. We also introduce a novel generalization of Fano's inequality (which may be of independent interest) for lower bounding the prioritized risk in more general settings involving unbounded losses. We illustrate the ability of our framework to provide insights into tradeoffs between prior information and learning performance for problems in estimation, regression, and reinforcement learning.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.13479",
        "string": "[Fundamental Tradeoffs in Learning with Prior Information](https://arxiv.org/pdf/2304.13479)"
    },
    "Greybox Penetration Testing on Cloud Access Control with IAM Modeling and Deep Reinforcement Learning": {
        "abstract": "Identity and Access Management (IAM) is an access control service in cloud platforms. To securely manage cloud resources, customers are required to configure IAM to specify the access control rules for their cloud organizations. However, IAM misconfiguration may be exploited to perform privilege escalation attacks, which can cause severe economic loss. To detect privilege escalations due to IAM misconfigurations, existing third-party cloud security services apply whitebox penetration testing techniques, which require the access of complete IAM configurations. This requirement might cause problems such as information disclosure and anonymization.\n  To mitigate the limitation, we propose a greybox penetration testing approach called TAC for third-party services to detect IAM privilege escalations, without requiring the access of complete IAM configurations. The idea is to intelligently query a limited amount of information that is only related to IAM privilege escalation detection. Cloud customers are allowed to specify which entities such as users and services (automatically anonymized by TAC) in their IAM configurations can be queried, and also limit the maximum number of queries. To realize the idea, we 1) propose abstract IAM modeling to detect IAM privilege escalations based on the collected partial information; 2) apply Reinforcement Learning (RL) with Graph Neural Networks (GNNs) to learn to make as few queries as possible. To pretrain and evaluate TAC with enough diverse tasks, we propose an IAM privilege escalation task generator called IAMVulGen. Experimental results show that TAC detects IAM privilege escalations with significantly lower false negative rates than baselines with high query efficiency, on both our task set and the only publicly available privilege escalation task set called IAM Vulnerable.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.14540",
        "string": "[Greybox Penetration Testing on Cloud Access Control with IAM Modeling and Deep Reinforcement Learning](https://arxiv.org/pdf/2304.14540)"
    },
    "Hierarchical State Abstraction Based on Structural Information Principles": {
        "abstract": "State abstraction optimizes decision-making by ignoring irrelevant environmental information in reinforcement learning with rich observations. Nevertheless, recent approaches focus on adequate representational capacities resulting in essential information loss, affecting their performances on challenging tasks. In this article, we propose a novel mathematical Structural Information principles-based State Abstraction framework, namely SISA, from the information-theoretic perspective. Specifically, an unsupervised, adaptive hierarchical state clustering method without requiring manual assistance is presented, and meanwhile, an optimal encoding tree is generated. On each non-root tree node, a new aggregation function and condition structural entropy are designed to achieve hierarchical state abstraction and compensate for sampling-induced essential information loss in state abstraction. Empirical evaluations on a visual gridworld domain and six continuous control benchmarks demonstrate that, compared with five SOTA state abstraction approaches, SISA significantly improves mean episode reward and sample efficiency up to 18.98 and 44.44%, respectively. Besides, we experimentally show that SISA is a general framework that can be flexibly integrated with different representation-learning objectives to improve their performances further.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.12000",
        "string": "[Hierarchical State Abstraction Based on Structural Information Principles](https://arxiv.org/pdf/2304.12000)"
    },
    "Inferring Preferences from Demonstrations in Multi-objective Reinforcement Learning: A Dynamic Weight-based Approach": {
        "abstract": "Many decision-making problems feature multiple objectives. In such problems, it is not always possible to know the preferences of a decision-maker for different objectives. However, it is often possible to observe the behavior of decision-makers. In multi-objective decision-making, preference inference is the process of inferring the preferences of a decision-maker for different objectives. This research proposes a Dynamic Weight-based Preference Inference (DWPI) algorithm that can infer the preferences of agents acting in multi-objective decision-making problems, based on observed behavior trajectories in the environment. The proposed method is evaluated on three multi-objective Markov decision processes: Deep Sea Treasure, Traffic, and Item Gathering. The performance of the proposed DWPI approach is compared to two existing preference inference methods from the literature, and empirical results demonstrate significant improvements compared to the baseline algorithms, in terms of both time requirements and accuracy of the inferred preferences. The Dynamic Weight-based Preference Inference algorithm also maintains its performance when inferring preferences for sub-optimal behavior demonstrations. In addition to its impressive performance, the Dynamic Weight-based Preference Inference algorithm does not require any interactions during training with the agent whose preferences are inferred, all that is required is a trajectory of observed behavior.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.14115",
        "string": "[Inferring Preferences from Demonstrations in Multi-objective Reinforcement Learning: A Dynamic Weight-based Approach](https://arxiv.org/pdf/2304.14115)"
    },
    "Instance-Optimality in Interactive Decision Making: Toward a Non-Asymptotic Theory": {
        "abstract": "We consider the development of adaptive, instance-dependent algorithms for interactive decision making (bandits, reinforcement learning, and beyond) that, rather than only performing well in the worst case, adapt to favorable properties of real-world instances for improved performance. We aim for instance-optimality, a strong notion of adaptivity which asserts that, on any particular problem instance, the algorithm under consideration outperforms all consistent algorithms. Instance-optimality enjoys a rich asymptotic theory originating from the work of \\citet{lai1985asymptotically,graves1997asymptotically}, but non-asymptotic guarantees have remained elusive outside of certain special cases. Even for problems as simple as tabular reinforcement learning, existing algorithms do not attain instance-optimal performance until the number of rounds of interaction is doubly exponential in the number of states.\n  In this paper, we take the first step toward developing a non-asymptotic theory of instance-optimal decision making with general function approximation. We introduce a new complexity measure, the Allocation-Estimation Coefficient (AEC), and provide a new algorithm, $\\mathsf{AE}^2$, which attains non-asymptotic instance-optimal performance at a rate controlled by the AEC. Our results recover the best known guarantees for well-studied problems such as finite-armed and linear bandits and, when specialized to tabular reinforcement learning, attain the first instance-optimal regret bounds with polynomial dependence on all problem parameters, improving over prior work exponentially. We complement these results with lower bounds that show that i) existing notions of statistical complexity are insufficient to derive non-asymptotic guarantees, and ii) under certain technical conditions, boundedness of the AEC is necessary to learn an instance-optimal allocation of decisions in finite time.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.12466",
        "string": "[Instance-Optimality in Interactive Decision Making: Toward a Non-Asymptotic Theory](https://arxiv.org/pdf/2304.12466)"
    },
    "Learning Agile Soccer Skills for a Bipedal Robot with Deep Reinforcement Learning": {
        "abstract": "We investigate whether Deep Reinforcement Learning (Deep RL) is able to synthesize sophisticated and safe movement skills for a low-cost, miniature humanoid robot that can be composed into complex behavioral strategies in dynamic environments. We used Deep RL to train a humanoid robot with 20 actuated joints to play a simplified one-versus-one (1v1) soccer game. We first trained individual skills in isolation and then composed those skills end-to-end in a self-play setting. The resulting policy exhibits robust and dynamic movement skills such as rapid fall recovery, walking, turning, kicking and more; and transitions between them in a smooth, stable, and efficient manner - well beyond what is intuitively expected from the robot. The agents also developed a basic strategic understanding of the game, and learned, for instance, to anticipate ball movements and to block opponent shots. The full range of behaviors emerged from a small set of simple rewards. Our agents were trained in simulation and transferred to real robots zero-shot. We found that a combination of sufficiently high-frequency control, targeted dynamics randomization, and perturbations during training in simulation enabled good-quality transfer, despite significant unmodeled effects and variations across robot instances. Although the robots are inherently fragile, minor hardware modifications together with basic regularization of the behavior during training led the robots to learn safe and effective movements while still performing in a dynamic and agile way. Indeed, even though the agents were optimized for scoring, in experiments they walked 156% faster, took 63% less time to get up, and kicked 24% faster than a scripted baseline, while efficiently combining the skills to achieve the longer term objectives. Examples of the emergent behaviors and full 1v1 matches are available on the supplementary website.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.13653",
        "string": "[Learning Agile Soccer Skills for a Bipedal Robot with Deep Reinforcement Learning](https://arxiv.org/pdf/2304.13653)"
    },
    "Learning adaptive manipulation of objects with revolute joint: A case study on varied cabinet doors opening": {
        "abstract": "This paper introduces a learning-based framework for robot adaptive manipulating the object with a revolute joint in unstructured environments. We concentrate our discussion on various cabinet door opening tasks. To improve the performance of Deep Reinforcement Learning in this scene, we analytically provide an efficient sampling manner utilizing the constraints of the objects. To open various kinds of doors, we add encoded environment parameters that define the various environments to the input of out policy. To transfer the policy into the real world, we train an adaptation module in simulation and fine-tune the adaptation module to cut down the impact of the policy-unaware environment parameters. We design a series of experiments to validate the efficacy of our framework. Additionally, we testify to the model's performance in the real world compared to the traditional door opening method.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.14602",
        "string": "[Learning adaptive manipulation of objects with revolute joint: A case study on varied cabinet doors opening](https://arxiv.org/pdf/2304.14602)"
    },
    "Loss and Reward Weighing for increased learning in Distributed Reinforcement Learning": {
        "abstract": "This paper introduces two learning schemes for distributed agents in Reinforcement Learning (RL) environments, namely Reward-Weighted (R-Weighted) and Loss-Weighted (L-Weighted) gradient merger. The R/L weighted methods replace standard practices for training multiple agents, such as summing or averaging the gradients. The core of our methods is to scale the gradient of each actor based on how high the reward (for R-Weighted) or the loss (for L-Weighted) is compared to the other actors. During training, each agent operates in differently initialized versions of the same environment, which gives different gradients from different actors. In essence, the R-Weights and L-Weights of each agent inform the other agents of its potential, which again reports which environment should be prioritized for learning. This approach of distributed learning is possible because environments that yield higher rewards, or low losses, have more critical information than environments that yield lower rewards or higher losses. We empirically demonstrate that the R-Weighted methods work superior to the state-of-the-art in multiple RL environments.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.12778",
        "string": "[Loss and Reward Weighing for increased learning in Distributed Reinforcement Learning](https://arxiv.org/pdf/2304.12778)"
    },
    "Meta-Reinforcement Learning Based on Self-Supervised Task Representation Learning": {
        "abstract": "Meta-reinforcement learning enables artificial agents to learn from related training tasks and adapt to new tasks efficiently with minimal interaction data. However, most existing research is still limited to narrow task distributions that are parametric and stationary, and does not consider out-of-distribution tasks during the evaluation, thus, restricting its application. In this paper, we propose MoSS, a context-based Meta-reinforcement learning algorithm based on Self-Supervised task representation learning to address this challenge. We extend meta-RL to broad non-parametric task distributions which have never been explored before, and also achieve state-of-the-art results in non-stationary and out-of-distribution tasks. Specifically, MoSS consists of a task inference module and a policy module. We utilize the Gaussian mixture model for task representation to imitate the parametric and non-parametric task variations. Additionally, our online adaptation strategy enables the agent to react at the first sight of a task change, thus being applicable in non-stationary tasks. MoSS also exhibits strong generalization robustness in out-of-distributions tasks which benefits from the reliable and robust task representation. The policy is built on top of an off-policy RL algorithm and the entire network is trained completely off-policy to ensure high sample efficiency. On MuJoCo and Meta-World benchmarks, MoSS outperforms prior works in terms of asymptotic performance, sample efficiency (3-50x faster), adaptation efficiency, and generalization robustness on broad and diverse task distributions.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2305.00286",
        "string": "[Meta-Reinforcement Learning Based on Self-Supervised Task Representation Learning](https://arxiv.org/pdf/2305.00286)"
    },
    "Model Extraction Attacks Against Reinforcement Learning Based Controllers": {
        "abstract": "We introduce the problem of model-extraction attacks in cyber-physical systems in which an attacker attempts to estimate (or extract) the feedback controller of the system. Extracting (or estimating) the controller provides an unmatched edge to attackers since it allows them to predict the future control actions of the system and plan their attack accordingly. Hence, it is important to understand the ability of the attackers to perform such an attack. In this paper, we focus on the setting when a Deep Neural Network (DNN) controller is trained using Reinforcement Learning (RL) algorithms and is used to control a stochastic system. We play the role of the attacker that aims to estimate such an unknown DNN controller, and we propose a two-phase algorithm. In the first phase, also called the offline phase, the attacker uses side-channel information about the RL-reward function and the system dynamics to identify a set of candidate estimates of the unknown DNN. In the second phase, also called the online phase, the attacker observes the behavior of the unknown DNN and uses these observations to shortlist the set of final policy estimates. We provide theoretical analysis of the error between the unknown DNN and the estimated one. We also provide numerical results showing the effectiveness of the proposed algorithm.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.13090",
        "string": "[Model Extraction Attacks Against Reinforcement Learning Based Controllers](https://arxiv.org/pdf/2304.13090)"
    },
    "Multi-criteria Hardware Trojan Detection: A Reinforcement Learning Approach": {
        "abstract": "Hardware Trojans (HTs) are undesired design or manufacturing modifications that can severely alter the security and functionality of digital integrated circuits. HTs can be inserted according to various design criteria, e.g., nets switching activity, observability, controllability, etc. However, to our knowledge, most HT detection methods are only based on a single criterion, i.e., nets switching activity. This paper proposes a multi-criteria reinforcement learning (RL) HT detection tool that features a tunable reward function for different HT detection scenarios. The tool allows for exploring existing detection strategies and can adapt new detection scenarios with minimal effort. We also propose a generic methodology for comparing HT detection methods fairly. Our preliminary results show an average of 84.2% successful HT detection in ISCAS-85 benchmark\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.13232",
        "string": "[Multi-criteria Hardware Trojan Detection: A Reinforcement Learning Approach](https://arxiv.org/pdf/2304.13232)"
    },
    "N$\\text{A}^\\text{2}$Q: Neural Attention Additive Model for Interpretable Multi-Agent Q-Learning": {
        "abstract": "Value decomposition is widely used in cooperative multi-agent reinforcement learning, however, its implicit credit assignment mechanism is not yet fully understood due to black-box networks. In this work, we study an interpretable value decomposition framework via the family of generalized additive models. We present a novel method, named Neural Attention Additive Q-learning~(N$\\text{A}^\\text{2}$Q), providing inherent intelligibility of collaboration behavior. N$\\text{A}^\\text{2}$Q can explicitly factorize the optimal joint policy induced by enriching shape functions to model all possible coalitions of agents into individual policies. Moreover, we construct identity semantics to promote estimating credits together with the global state and individual value functions, where local semantic masks help us diagnose whether each agent captures relevant-task information. Extensive experiments show that N$\\text{A}^\\text{2}$Q consistently achieves superior performance compared to different state-of-the-art methods on all challenging tasks, while yielding human-like interpretability.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.13383",
        "string": "[N$\\text{A}^\\text{2}$Q: Neural Attention Additive Model for Interpretable Multi-Agent Q-Learning](https://arxiv.org/pdf/2304.13383)"
    },
    "NNSplitter: An Active Defense Solution to DNN Model via Automated Weight Obfuscation": {
        "abstract": "As a type of valuable intellectual property (IP), deep neural network (DNN) models have been protected by techniques like watermarking. However, such passive model protection cannot fully prevent model abuse. In this work, we propose an active model IP protection scheme, namely NNSplitter, which actively protects the model by splitting it into two parts: the obfuscated model that performs poorly due to weight obfuscation, and the model secrets consisting of the indexes and original values of the obfuscated weights, which can only be accessed by authorized users. NNSplitter uses the trusted execution environment to secure the secrets and a reinforcement learning-based controller to reduce the number of obfuscated weights while maximizing accuracy drop. Our experiments show that by only modifying 313 out of over 28 million (i.e., 0.001%) weights, the accuracy of the obfuscated VGG-11 model on Fashion-MNIST can drop to 10%. We also demonstrate that NNSplitter is stealthy and resilient against potential attack surfaces, including norm clipping and fine-tuning attacks.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2305.00097",
        "string": "[NNSplitter: An Active Defense Solution to DNN Model via Automated Weight Obfuscation](https://arxiv.org/pdf/2305.00097)"
    },
    "One-Step Distributional Reinforcement Learning": {
        "abstract": "Reinforcement learning (RL) allows an agent interacting sequentially with an environment to maximize its long-term expected return. In the distributional RL (DistrRL) paradigm, the agent goes beyond the limit of the expected value, to capture the underlying probability distribution of the return across all time steps. The set of DistrRL algorithms has led to improved empirical performance. Nevertheless, the theory of DistrRL is still not fully understood, especially in the control case. In this paper, we present the simpler one-step distributional reinforcement learning (OS-DistrRL) framework encompassing only the randomness induced by the one-step dynamics of the environment. Contrary to DistrRL, we show that our approach comes with a unified theory for both policy evaluation and control. Indeed, we propose two OS-DistrRL algorithms for which we provide an almost sure convergence analysis. The proposed approach compares favorably with categorical DistrRL on various environments.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.14421",
        "string": "[One-Step Distributional Reinforcement Learning](https://arxiv.org/pdf/2304.14421)"
    },
    "Optimal Scheduling in IoT-Driven Smart Isolated Microgrids Based on Deep Reinforcement Learning": {
        "abstract": "In this paper, we investigate the scheduling issue of diesel generators (DGs) in an Internet of Things (IoT)-Driven isolated microgrid (MG) by deep reinforcement learning (DRL). The renewable energy is fully exploited under the uncertainty of renewable generation and load demand. The DRL agent learns an optimal policy from history renewable and load data of previous days, where the policy can generate real-time decisions based on observations of past renewable and load data of previous hours collected by connected sensors. The goal is to reduce operating cost on the premise of ensuring supply-demand balance. In specific, a novel finite-horizon partial observable Markov decision process (POMDP) model is conceived considering the spinning reserve. In order to overcome the challenge of discrete-continuous hybrid action space due to the binary DG switching decision and continuous energy dispatch (ED) decision, a DRL algorithm, namely the hybrid action finite-horizon RDPG (HAFH-RDPG), is proposed. HAFH-RDPG seamlessly integrates two classical DRL algorithms, i.e., deep Q-network (DQN) and recurrent deterministic policy gradient (RDPG), based on a finite-horizon dynamic programming (DP) framework. Extensive experiments are performed with real-world data in an IoT-driven MG to evaluate the capability of the proposed algorithm in handling the uncertainty due to inter-hour and inter-day power fluctuation and to compare its performance with those of the benchmark algorithms.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2305.00127",
        "string": "[Optimal Scheduling in IoT-Driven Smart Isolated Microgrids Based on Deep Reinforcement Learning](https://arxiv.org/pdf/2305.00127)"
    },
    "Optimizing Energy Efficiency in Metro Systems Under Uncertainty Disturbances Using Reinforcement Learning": {
        "abstract": "In the realm of urban transportation, metro systems serve as crucial and sustainable means of public transit. However, their substantial energy consumption poses a challenge to the goal of sustainability. Disturbances such as delays and passenger flow changes can further exacerbate this issue by negatively affecting energy efficiency in metro systems. To tackle this problem, we propose a policy-based reinforcement learning approach that reschedules the metro timetable and optimizes energy efficiency in metro systems under disturbances by adjusting the dwell time and cruise speed of trains. Our experiments conducted in a simulation environment demonstrate the superiority of our method over baseline methods, achieving a traction energy consumption reduction of up to 10.9% and an increase in regenerative braking energy utilization of up to 47.9%. This study provides an effective solution to the energy-saving problem of urban rail transit.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.13443",
        "string": "[Optimizing Energy Efficiency in Metro Systems Under Uncertainty Disturbances Using Reinforcement Learning](https://arxiv.org/pdf/2304.13443)"
    },
    "Parallel bootstrap-based on-policy deep reinforcement learning for continuous flow control applications": {
        "abstract": "The coupling of deep reinforcement learning to numerical flow control problems has recently received a considerable attention, leading to groundbreaking results and opening new perspectives for the domain. Due to the usually high computational cost of fluid dynamics solvers, the use of parallel environments during the learning process represents an essential ingredient to attain efficient control in a reasonable time. Yet, most of the deep reinforcement learning literature for flow control relies on on-policy algorithms, for which the massively parallel transition collection may break theoretical assumptions and lead to suboptimal control models. To overcome this issue, we propose a parallelism pattern relying on partial-trajectory buffers terminated by a return bootstrapping step, allowing a flexible use of parallel environments while preserving the on-policiness of the updates. This approach is illustrated on a CPU-intensive continuous flow control problem from the literature.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.12330",
        "string": "[Parallel bootstrap-based on-policy deep reinforcement learning for continuous flow control applications](https://arxiv.org/pdf/2304.12330)"
    },
    "Partially Observable Mean Field Multi-Agent Reinforcement Learning Based on Graph-Attention": {
        "abstract": "Traditional multi-agent reinforcement learning algorithms are difficultly applied in a large-scale multi-agent environment. The introduction of mean field theory has enhanced the scalability of multi-agent reinforcement learning in recent years. This paper considers partially observable multi-agent reinforcement learning (MARL), where each agent can only observe other agents within a fixed range. This partial observability affects the agent's ability to assess the quality of the actions of surrounding agents. This paper focuses on developing a method to capture more effective information from local observations in order to select more effective actions. Previous work in this field employs probability distributions or weighted mean field to update the average actions of neighborhood agents, but it does not fully consider the feature information of surrounding neighbors and leads to a local optimum. In this paper, we propose a novel multi-agent reinforcement learning algorithm, Partially Observable Mean Field Multi-Agent Reinforcement Learning based on Graph--Attention (GAMFQ) to remedy this flaw. GAMFQ uses a graph attention module and a mean field module to describe how an agent is influenced by the actions of other agents at each time step. This graph attention module consists of a graph attention encoder and a differentiable attention mechanism, and this mechanism outputs a dynamic graph to represent the effectiveness of neighborhood agents against central agents. The mean--field module approximates the effect of a neighborhood agent on a central agent as the average effect of effective neighborhood agents. We evaluate GAMFQ on three challenging tasks in the MAgents framework. Experiments show that GAMFQ outperforms baselines including the state-of-the-art partially observable mean-field reinforcement learning algorithms.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.12653",
        "string": "[Partially Observable Mean Field Multi-Agent Reinforcement Learning Based on Graph-Attention](https://arxiv.org/pdf/2304.12653)"
    },
    "Policy Resilience to Environment Poisoning Attacks on Reinforcement Learning": {
        "abstract": "This paper investigates policy resilience to training-environment poisoning attacks on reinforcement learning (RL) policies, with the goal of recovering the deployment performance of a poisoned RL policy. Due to the fact that the policy resilience is an add-on concern to RL algorithms, it should be resource-efficient, time-conserving, and widely applicable without compromising the performance of RL algorithms. This paper proposes such a policy-resilience mechanism based on an idea of knowledge sharing. We summarize the policy resilience as three stages: preparation, diagnosis, recovery. Specifically, we design the mechanism as a federated architecture coupled with a meta-learning manner, pursuing an efficient extraction and sharing of the environment knowledge. With the shared knowledge, a poisoned agent can quickly identify the deployment condition and accordingly recover its policy performance. We empirically evaluate the resilience mechanism for both model-based and model-free RL algorithms, showing its effectiveness and efficiency in restoring the deployment performance of a poisoned policy.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.12151",
        "string": "[Policy Resilience to Environment Poisoning Attacks on Reinforcement Learning](https://arxiv.org/pdf/2304.12151)"
    },
    "Proto-Value Networks: Scaling Representation Learning with Auxiliary Tasks": {
        "abstract": "Auxiliary tasks improve the representations learned by deep reinforcement learning agents. Analytically, their effect is reasonably well understood; in practice, however, their primary use remains in support of a main learning objective, rather than as a method for learning representations. This is perhaps surprising given that many auxiliary tasks are defined procedurally, and hence can be treated as an essentially infinite source of information about the environment. Based on this observation, we study the effectiveness of auxiliary tasks for learning rich representations, focusing on the setting where the number of tasks and the size of the agent's network are simultaneously increased. For this purpose, we derive a new family of auxiliary tasks based on the successor measure. These tasks are easy to implement and have appealing theoretical properties. Combined with a suitable off-policy learning rule, the result is a representation learning algorithm that can be understood as extending Mahadevan & Maggioni (2007)'s proto-value functions to deep reinforcement learning -- accordingly, we call the resulting object proto-value networks. Through a series of experiments on the Arcade Learning Environment, we demonstrate that proto-value networks produce rich features that may be used to obtain performance comparable to established algorithms, using only linear approximation and a small number (~4M) of interactions with the environment's reward function.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.12567",
        "string": "[Proto-Value Networks: Scaling Representation Learning with Auxiliary Tasks](https://arxiv.org/pdf/2304.12567)"
    },
    "Provable benefits of general coverage conditions in efficient online RL with function approximation": {
        "abstract": "In online reinforcement learning (RL), instead of employing standard structural assumptions on Markov decision processes (MDPs), using a certain coverage condition (original from offline RL) is enough to ensure sample-efficient guarantees (Xie et al. 2023). In this work, we focus on this new direction by digging more possible and general coverage conditions, and study the potential and the utility of them in efficient online RL. We identify more concepts, including the $L^p$ variant of concentrability, the density ratio realizability, and trade-off on the partial/rest coverage condition, that can be also beneficial to sample-efficient online RL, achieving improved regret bound. Furthermore, if exploratory offline data are used, under our coverage conditions, both statistically and computationally efficient guarantees can be achieved for online RL. Besides, even though the MDP structure is given, e.g., linear MDP, we elucidate that, good coverage conditions are still beneficial to obtain faster regret bound beyond $\\widetilde{O}(\\sqrt{T})$ and even a logarithmic order regret. These results provide a good justification for the usage of general coverage conditions in efficient online RL.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.12886",
        "string": "[Provable benefits of general coverage conditions in efficient online RL with function approximation](https://arxiv.org/pdf/2304.12886)"
    },
    "Proximal Curriculum for Reinforcement Learning Agents": {
        "abstract": "We consider the problem of curriculum design for reinforcement learning (RL) agents in contextual multi-task settings. Existing techniques on automatic curriculum design typically require domain-specific hyperparameter tuning or have limited theoretical underpinnings. To tackle these limitations, we design our curriculum strategy, ProCuRL, inspired by the pedagogical concept of Zone of Proximal Development (ZPD). ProCuRL captures the intuition that learning progress is maximized when picking tasks that are neither too hard nor too easy for the learner. We mathematically derive ProCuRL by analyzing two simple learning settings. We also present a practical variant of ProCuRL that can be directly integrated with deep RL frameworks with minimal hyperparameter tuning. Experimental results on a variety of domains demonstrate the effectiveness of our curriculum strategy over state-of-the-art baselines in accelerating the training process of deep RL agents.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.12877",
        "string": "[Proximal Curriculum for Reinforcement Learning Agents](https://arxiv.org/pdf/2304.12877)"
    },
    "Quantum Natural Policy Gradients: Towards Sample-Efficient Reinforcement Learning": {
        "abstract": "Reinforcement learning is a growing field in AI with a lot of potential. Intelligent behavior is learned automatically through trial and error in interaction with the environment. However, this learning process is often costly. Using variational quantum circuits as function approximators can reduce this cost. In order to implement this, we propose the quantum natural policy gradient (QNPG) algorithm -- a second-order gradient-based routine that takes advantage of an efficient approximation of the quantum Fisher information matrix. We experimentally demonstrate that QNPG outperforms first-order based training on Contextual Bandits environments regarding convergence speed and stability and thereby reduces the sample complexity. Furthermore, we provide evidence for the practical feasibility of our approach by training on a 12-qubit hardware device.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.13571",
        "string": "[Quantum Natural Policy Gradients: Towards Sample-Efficient Reinforcement Learning](https://arxiv.org/pdf/2304.13571)"
    },
    "ReLBOT: A Transfer Learning Approach to Minimize Reinforcement Learning Risks in Smart Buildings": {
        "abstract": "Smart buildings aim to optimize energy consumption by applying artificial intelligent algorithms. When a smart building is commissioned there is no historical data that could be used to train these algorithms. On-line Reinforcement Learning (RL) algorithms have shown significant promise, but their deployment carries a significant risk, because as the RL agent initially explores its action space it could cause significant discomfort to the building residents. In this paper we present ReLBOT, a new technique that uses transfer learning in conjunction with deep RL to transfer knowledge from an existing, optimized smart building, to the newly commissioning building, to reduce the adverse impact of the reinforcement learning agent's warm-up period. We demonstrate improvements of up to 6.2 times in the duration, and up to 132 times in prediction variance for the reinforcement learning agent's warm-up period.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2305.00365",
        "string": "[ReLBOT: A Transfer Learning Approach to Minimize Reinforcement Learning Risks in Smart Buildings](https://arxiv.org/pdf/2305.00365)"
    },
    "Reinforcement Learning with Knowledge Representation and Reasoning: A Brief Survey": {
        "abstract": "Reinforcement Learning(RL) has achieved tremendous development in recent years, but still faces significant obstacles in addressing complex real-life problems due to the issues of poor system generalization, low sample efficiency as well as safety and interpretability concerns. The core reason underlying such dilemmas can be attributed to the fact that most of the work has focused on the computational aspect of value functions or policies using a representational model to describe atomic components of rewards, states and actions etc, thus neglecting the rich high-level declarative domain knowledge of facts, relations and rules that can be either provided a priori or acquired through reasoning over time. Recently, there has been a rapidly growing interest in the use of Knowledge Representation and Reasoning(KRR) methods, usually using logical languages, to enable more abstract representation and efficient learning in RL. In this survey, we provide a preliminary overview on these endeavors that leverage the strengths of KRR to help solving various problems in RL, and discuss the challenging open problems and possible directions for future work in this area.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.12090",
        "string": "[Reinforcement Learning with Knowledge Representation and Reasoning: A Brief Survey](https://arxiv.org/pdf/2304.12090)"
    },
    "Reinforcement Learning with Partial Parametric Model Knowledge": {
        "abstract": "We adapt reinforcement learning (RL) methods for continuous control to bridge the gap between complete ignorance and perfect knowledge of the environment. Our method, Partial Knowledge Least Squares Policy Iteration (PLSPI), takes inspiration from both model-free RL and model-based control. It uses incomplete information from a partial model and retains RL's data-driven adaption towards optimal performance. The linear quadratic regulator provides a case study; numerical experiments demonstrate the effectiveness and resulting benefits of the proposed method.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.13223",
        "string": "[Reinforcement Learning with Partial Parametric Model Knowledge](https://arxiv.org/pdf/2304.13223)"
    },
    "Roll-Drop: accounting for observation noise with a single parameter": {
        "abstract": "This paper proposes a simple strategy for sim-to-real in Deep-Reinforcement Learning (DRL) -- called Roll-Drop -- that uses dropout during simulation to account for observation noise during deployment without explicitly modelling its distribution for each state. DRL is a promising approach to control robots for highly dynamic and feedback-based manoeuvres, and accurate simulators are crucial to providing cheap and abundant data to learn the desired behaviour. Nevertheless, the simulated data are noiseless and generally show a distributional shift that challenges the deployment on real machines where sensor readings are affected by noise. The standard solution is modelling the latter and injecting it during training; while this requires a thorough system identification, Roll-Drop enhances the robustness to sensor noise by tuning only a single parameter. We demonstrate an 80% success rate when up to 25% noise is injected in the observations, with twice higher robustness than the baselines. We deploy the controller trained in simulation on a Unitree A1 platform and assess this improved robustness on the physical system.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.13150",
        "string": "[Roll-Drop: accounting for observation noise with a single parameter](https://arxiv.org/pdf/2304.13150)"
    },
    "SEA: A Spatially Explicit Architecture for Multi-Agent Reinforcement Learning": {
        "abstract": "Spatial information is essential in various fields. How to explicitly model according to the spatial location of agents is also very important for the multi-agent problem, especially when the number of agents is changing and the scale is enormous. Inspired by the point cloud task in computer vision, we propose a spatial information extraction structure for multi-agent reinforcement learning in this paper. Agents can effectively share the neighborhood and global information through a spatially encoder-decoder structure. Our method follows the centralized training with decentralized execution (CTDE) paradigm. In addition, our structure can be applied to various existing mainstream reinforcement learning algorithms with minor modifications and can deal with the problem with a variable number of agents. The experiments in several multi-agent scenarios show that the existing methods can get convincing results by adding our spatially explicit architecture.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.12532",
        "string": "[SEA: A Spatially Explicit Architecture for Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/2304.12532)"
    },
    "Semi-Infinitely Constrained Markov Decision Processes and Efficient Reinforcement Learning": {
        "abstract": "We propose a novel generalization of constrained Markov decision processes (CMDPs) that we call the \\emph{semi-infinitely constrained Markov decision process} (SICMDP). Particularly, we consider a continuum of constraints instead of a finite number of constraints as in the case of ordinary CMDPs. We also devise two reinforcement learning algorithms for SICMDPs that we call SI-CRL and SI-CPO. SI-CRL is a model-based reinforcement learning algorithm. Given an estimate of the transition model, we first transform the reinforcement learning problem into a linear semi-infinitely programming (LSIP) problem and then use the dual exchange method in the LSIP literature to solve it. SI-CPO is a policy optimization algorithm. Borrowing the ideas from the cooperative stochastic approximation approach, we make alternative updates to the policy parameters to maximize the reward or minimize the cost. To the best of our knowledge, we are the first to apply tools from semi-infinitely programming (SIP) to solve constrained reinforcement learning problems. We present theoretical analysis for SI-CRL and SI-CPO, identifying their iteration complexity and sample complexity. We also conduct extensive numerical examples to illustrate the SICMDP model and demonstrate that our proposed algorithms are able to solve complex sequential decision-making tasks leveraging modern deep reinforcement learning techniques.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2305.00254",
        "string": "[Semi-Infinitely Constrained Markov Decision Processes and Efficient Reinforcement Learning](https://arxiv.org/pdf/2305.00254)"
    },
    "SocNavGym: A Reinforcement Learning Gym for Social Navigation": {
        "abstract": "It is essential for autonomous robots to be socially compliant while navigating in human-populated environments. Machine Learning and, especially, Deep Reinforcement Learning have recently gained considerable traction in the field of Social Navigation. This can be partially attributed to the resulting policies not being bound by human limitations in terms of code complexity or the number of variables that are handled. Unfortunately, the lack of safety guarantees and the large data requirements by DRL algorithms make learning in the real world unfeasible. To bridge this gap, simulation environments are frequently used. We propose SocNavGym, an advanced simulation environment for social navigation that can generate a wide variety of social navigation scenarios and facilitates the development of intelligent social agents. SocNavGym is light-weight, fast, easy-to-use, and can be effortlessly configured to generate different types of social navigation scenarios. It can also be configured to work with different hand-crafted and data-driven social reward signals and to yield a variety of evaluation metrics to benchmark agents' performance. Further, we also provide a case study where a Dueling-DQN agent is trained to learn social-navigation policies using SocNavGym. The results provides evidence that SocNavGym can be used to train an agent from scratch to navigate in simple as well as complex social scenarios. Our experiments also show that the agents trained using the data-driven reward function displays more advanced social compliance in comparison to the heuristic-based reward function.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.14102",
        "string": "[SocNavGym: A Reinforcement Learning Gym for Social Navigation](https://arxiv.org/pdf/2304.14102)"
    },
    "Stubborn: An Environment for Evaluating Stubbornness between Agents with Aligned Incentives": {
        "abstract": "Recent research in multi-agent reinforcement learning (MARL) has shown success in learning social behavior and cooperation. Social dilemmas between agents in mixed-sum settings have been studied extensively, but there is little research into social dilemmas in fullycooperative settings, where agents have no prospect of gaining reward at another agent's expense.\n  While fully-aligned interests are conducive to cooperation between agents, they do not guarantee it. We propose a measure of \"stubbornness\" between agents that aims to capture the human social behavior from which it takes its name: a disagreement that is gradually escalating and potentially disastrous. We would like to promote research into the tendency of agents to be stubborn, the reactions of counterpart agents, and the resulting social dynamics.\n  In this paper we present Stubborn, an environment for evaluating stubbornness between agents with fully-aligned incentives. In our preliminary results, the agents learn to use their partner's stubbornness as a signal for improving the choices that they make in the environment.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.12280",
        "string": "[Stubborn: An Environment for Evaluating Stubbornness between Agents with Aligned Incentives](https://arxiv.org/pdf/2304.12280)"
    },
    "Topic-oriented Adversarial Attacks against Black-box Neural Ranking Models": {
        "abstract": "Neural ranking models (NRMs) have attracted considerable attention in information retrieval. Unfortunately, NRMs may inherit the adversarial vulnerabilities of general neural networks, which might be leveraged by black-hat search engine optimization practitioners. Recently, adversarial attacks against NRMs have been explored in the paired attack setting, generating an adversarial perturbation to a target document for a specific query. In this paper, we focus on a more general type of perturbation and introduce the topic-oriented adversarial ranking attack task against NRMs, which aims to find an imperceptible perturbation that can promote a target document in ranking for a group of queries with the same topic. We define both static and dynamic settings for the task and focus on decision-based black-box attacks. We propose a novel framework to improve topic-oriented attack performance based on a surrogate ranking model. The attack problem is formalized as a Markov decision process (MDP) and addressed using reinforcement learning. Specifically, a topic-oriented reward function guides the policy to find a successful adversarial example that can be promoted in rankings to as many queries as possible in a group. Experimental results demonstrate that the proposed framework can significantly outperform existing attack strategies, and we conclude by re-iterating that there exist potential risks for applying NRMs in the real world.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.14867",
        "string": "[Topic-oriented Adversarial Attacks against Black-box Neural Ranking Models](https://arxiv.org/pdf/2304.14867)"
    },
    "Towards Theoretical Understanding of Inverse Reinforcement Learning": {
        "abstract": "Inverse reinforcement learning (IRL) denotes a powerful family of algorithms for recovering a reward function justifying the behavior demonstrated by an expert agent. A well-known limitation of IRL is the ambiguity in the choice of the reward function, due to the existence of multiple rewards that explain the observed behavior. This limitation has been recently circumvented by formulating IRL as the problem of estimating the feasible reward set, i.e., the region of the rewards compatible with the expert's behavior. In this paper, we make a step towards closing the theory gap of IRL in the case of finite-horizon problems with a generative model. We start by formally introducing the problem of estimating the feasible reward set, the corresponding PAC requirement, and discussing the properties of particular classes of rewards. Then, we provide the first minimax lower bound on the sample complexity for the problem of estimating the feasible reward set of order $\u03a9\\Bigl( \\frac{H^3SA}{\u03b5^2} \\bigl( \\log \\bigl(\\frac{1}\u03b4\\bigl) + S \\bigl)\\Bigl)$, being $S$ and $A$ the number of states and actions respectively, $H$ the horizon, $\u03b5$ the desired accuracy, and $\u03b4$ the confidence. We analyze the sample complexity of a uniform sampling strategy (US-IRL), proving a matching upper bound up to logarithmic factors. Finally, we outline several open questions in IRL and propose future research directions.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.12966",
        "string": "[Towards Theoretical Understanding of Inverse Reinforcement Learning](https://arxiv.org/pdf/2304.12966)"
    },
    "When to Replan? An Adaptive Replanning Strategy for Autonomous Navigation using Deep Reinforcement Learning": {
        "abstract": "The hierarchy of global and local planners is one of the most commonly utilized system designs in robot autonomous navigation. While the global planner generates a reference path from the current to goal locations based on the pre-built static map, the local planner produces a collision-free, kinodynamic trajectory to follow the reference path while avoiding perceived obstacles. The reference path should be replanned regularly to accommodate new obstacles that were absent in the pre-built map, but when to execute replanning remains an open question. In this work, we conduct an extensive simulation experiment to compare various replanning strategies and confirm that effective strategies highly depend on the environment as well as on the global and local planners. We then propose a new adaptive replanning strategy based on deep reinforcement learning, where an agent learns from experiences to decide appropriate replanning timings in the given environment and planning setups. Our experimental results demonstrate that the proposed replanning agent can achieve performance on par or even better than current best-performing strategies across multiple situations in terms of navigation robustness and efficiency.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.12046",
        "string": "[When to Replan? An Adaptive Replanning Strategy for Autonomous Navigation using Deep Reinforcement Learning](https://arxiv.org/pdf/2304.12046)"
    },
    "X-RLflow: Graph Reinforcement Learning for Neural Network Subgraphs Transformation": {
        "abstract": "Tensor graph superoptimisation systems perform a sequence of subgraph substitution to neural networks, to find the optimal computation graph structure. Such a graph transformation process naturally falls into the framework of sequential decision-making, and existing systems typically employ a greedy search approach, which cannot explore the whole search space as it cannot tolerate a temporary loss of performance. In this paper, we address the tensor graph superoptimisation problem by exploring an alternative search approach, reinforcement learning (RL). Our proposed approach, X-RLflow, can learn to perform neural network dataflow graph rewriting, which substitutes a subgraph one at a time. X-RLflow is based on a model-free RL agent that uses a graph neural network (GNN) to encode the target computation graph and outputs a transformed computation graph iteratively. We show that our approach can outperform state-of-the-art superoptimisation systems over a range of deep learning models and achieve by up to 40% on those that are based on transformer-style architectures.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.14698",
        "string": "[X-RLflow: Graph Reinforcement Learning for Neural Network Subgraphs Transformation](https://arxiv.org/pdf/2304.14698)"
    }
}
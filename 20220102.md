# Papers interesantes publicados esta semana

## Game Theory

Ueda, M. (2022). Memory-two strategies forming symmetric mutual reinforcement learning equilibrium in repeated prisoner's dilemma game. arXiv preprint arXiv:2108.03258v2.

    Analysis of simmetric equilibria of mutual reinforcement learning on the _repeated_ prisioner's dilemma game.

    Uses memory-two deterministic strategies

    The repeated prisioner's dilemma game -> can lead to stronger results by mutual cooperation - folk theorem

    VERY interesting

Towards Automating Codenames Spymasters with Deep
Reinforcement Learning

    multi-player co-operative games

    Although traditional multi-agent reinforcement learning (RL) techniques tend to work well when RL agents work with each other, they fail to work well when co-operating with humans (Siu et al., 2021; Bakhtin et al., 2021)

    text-based games

    problems: 

    main challenges with text-based games is the large action space available to agents

    common sense about how the world works

    Codenames is a good benchmark for both human-AI co-operation and text-based reinforcement learning,

    VERY interesting

Warmth and Competence in Human-Agent Cooperation

    Human choose the Reinforcement Learning Agent they want to play with based on their interaction

    Using game of Coins

    We recommend human-agent interaction researchers routinely incorporate the measurement of social perception and subjective preferences into their studies

    VERY interesting

## Reinforcement Learning Theory

Towards Learning Abstractions via Reinforcement
Learning

    Synthesis of efficient communication schemes in multi-agent systems trained by RL

    Combination of symbolic methods and machine Learning: neuro-symbolic system

    Reinforcement Learning is interleaved with steps to extend the current language with novel higher level concepts

    We demonstrate that this approach allows agents to converge quickly on a small collaborative construction task

    VERY interesting


## Transformers

Transformer in Transformer as Backbone for Deep Reinforcement Learning

    Proposes pure Transformer_based network for deep RL

    Transformer in Transforme backbone: cascades two transformers:

    The inner one is used to process a single observation
    The outer one processes the observation history

    Combining both is expected to to extract spatial-temporal representations for good decision making

    satisfactory performance

    moderately interesting

On Transforming Reinforcement Learning by
Transformer: The Development Trajectory

    Review on transformer-based RL

    two categories: architecture enhacement and trajectory optimization

## Mathematical Theory

Convergence of Batch Asynchronous Stochastic Approximation
With Applications to Reinforcement Learning

OFFLINE POLICY OPTIMIZATION IN RL WITH VARIANCE REGULARIZATION

On the Convergence of Discounted Policy Gradient Methods

Strangeness-driven Exploration in Multi-Agent
Reinforcement Learning


{
    "A Data-Driven State Aggregation Approach for Dynamic Discrete Choice Models": {
        "abstract": "We study dynamic discrete choice models, where a commonly studied problem involves estimating parameters of agent reward functions (also known as \"structural\" parameters), using agent behavioral data. Maximum likelihood estimation for such models requires dynamic programming, which is limited by the curse of dimensionality. In this work, we present a novel algorithm that provides a data-driven method for selecting and aggregating states, which lowers the computational and sample complexity of estimation. Our method works in two stages. In the first stage, we use a flexible inverse reinforcement learning approach to estimate agent Q-functions. We use these estimated Q-functions, along with a clustering algorithm, to select a subset of states that are the most pivotal for driving changes in Q-functions. In the second stage, with these selected \"aggregated\" states, we conduct maximum likelihood estimation using a commonly used nested fixed-point algorithm. The proposed two-stage approach mitigates the curse of dimensionality by reducing the problem dimension. Theoretically, we derive finite-sample bounds on the associated estimation error, which also characterize the trade-off of computational complexity, estimation error, and sample complexity. We demonstrate the empirical performance of the algorithm in two classic dynamic discrete choice estimation applications.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.04916",
        "string": "[A Data-Driven State Aggregation Approach for Dynamic Discrete Choice Models](https://arxiv.org/pdf/2304.04916)"
    },
    "A Platform-Agnostic Deep Reinforcement Learning Framework for Effective Sim2Real Transfer in Autonomous Driving": {
        "abstract": "Deep Reinforcement Learning (DRL) has shown remarkable success in solving complex tasks across various research fields. However, transferring DRL agents to the real world is still challenging due to the significant discrepancies between simulation and reality. To address this issue, we propose a robust DRL framework that leverages platform-dependent perception modules to extract task-relevant information and train a lane-following and overtaking agent in simulation. This framework facilitates the seamless transfer of the DRL agent to new simulated environments and the real world with minimal effort. We evaluate the performance of the agent in various driving scenarios in both simulation and the real world, and compare it to human players and the PID baseline in simulation. Our proposed framework significantly reduces the gaps between different platforms and the Sim2Real gap, enabling the trained agent to achieve similar performance in both simulation and the real world, driving the vehicle effectively.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.08235",
        "string": "[A Platform-Agnostic Deep Reinforcement Learning Framework for Effective Sim2Real Transfer in Autonomous Driving](https://arxiv.org/pdf/2304.08235)"
    },
    "A Review on Longitudinal Car-Following Model": {
        "abstract": "The car-following (CF) model is the core component for traffic simulations and has been built-in in many production vehicles with Advanced Driving Assistance Systems (ADAS). Research of CF behavior allows us to identify the sources of different macro phenomena induced by the basic process of pairwise vehicle interaction. The CF behavior and control model encompasses various fields, such as traffic engineering, physics, cognitive science, machine learning, and reinforcement learning. This paper provides a comprehensive survey highlighting differences, complementarities, and overlaps among various CF models according to their underlying logic and principles. We reviewed representative algorithms, ranging from the theory-based kinematic models, stimulus-response models, and cruise control models to data-driven Behavior Cloning (BC) and Imitation Learning (IL) and outlined their strengths and limitations. This review categorizes CF models that are conceptualized in varying principles and summarize the vast literature with a holistic framework.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.07143",
        "string": "[A Review on Longitudinal Car-Following Model](https://arxiv.org/pdf/2304.07143)"
    },
    "A Tale of Sampling and Estimation in Discounted Reinforcement Learning": {
        "abstract": "The most relevant problems in discounted reinforcement learning involve estimating the mean of a function under the stationary distribution of a Markov reward process, such as the expected return in policy evaluation, or the policy gradient in policy optimization. In practice, these estimates are produced through a finite-horizon episodic sampling, which neglects the mixing properties of the Markov process. It is mostly unclear how this mismatch between the practical and the ideal setting affects the estimation, and the literature lacks a formal study on the pitfalls of episodic sampling, and how to do it optimally. In this paper, we present a minimax lower bound on the discounted mean estimation problem that explicitly connects the estimation error with the mixing properties of the Markov process and the discount factor. Then, we provide a statistical analysis on a set of notable estimators and the corresponding sampling procedures, which includes the finite-horizon estimators often used in practice. Crucially, we show that estimating the mean by directly sampling from the discounted kernel of the Markov process brings compelling statistical properties w.r.t. the alternative estimators, as it matches the lower bound without requiring a careful tuning of the episode horizon.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.05073",
        "string": "[A Tale of Sampling and Estimation in Discounted Reinforcement Learning](https://arxiv.org/pdf/2304.05073)"
    },
    "A novel approach of a deep reinforcement learning based motion cueing algorithm for vehicle driving simulation": {
        "abstract": "In the field of motion simulation, the level of immersion strongly depends on the motion cueing algorithm (MCA), as it transfers the reference motion of the simulated vehicle to a motion of the motion simulation platform (MSP). The challenge for the MCA is to reproduce the motion perception of a real vehicle driver as accurately as possible without exceeding the limits of the workspace of the MSP in order to provide a realistic virtual driving experience. In case of a large discrepancy between the perceived motion signals and the optical cues, motion sickness may occur with the typical symptoms of nausea, dizziness, headache and fatigue. Existing approaches either produce non-optimal results, e.g., due to filtering, linearization, or simplifications, or the required computational time exceeds the real-time requirements of a closed-loop application.\n  In this work a new solution is presented, where not a human designer specifies the principles of the MCA but an artificial intelligence (AI) learns the optimal motion by trial and error in an interaction with the MSP. To achieve this, deep reinforcement learning (RL) is applied, where an agent interacts with an environment formulated as a Markov decision process~(MDP). This allows the agent to directly control a simulated MSP to obtain feedback on its performance in terms of platform workspace usage and the motion acting on the simulator user. The RL algorithm used is proximal policy optimization (PPO), where the value function and the policy corresponding to the control strategy are learned and both are mapped in artificial neural networks (ANN). This approach is implemented in Python and the functionality is demonstrated by the practical example of pre-recorded lateral maneuvers. The subsequent validation on a standardized double lane change shows that the RL algorithm is able to learn the control strategy and improve the quality of...\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.07600",
        "string": "[A novel approach of a deep reinforcement learning based motion cueing algorithm for vehicle driving simulation](https://arxiv.org/pdf/2304.07600)"
    },
    "Advances in Cybercrime Prediction: A Survey of Machine, Deep, Transfer, and Adaptive Learning Techniques": {
        "abstract": "Cybercrime is a growing threat to organizations and individuals worldwide, with criminals using increasingly sophisticated techniques to breach security systems and steal sensitive data. In recent years, machine learning, deep learning, and transfer learning techniques have emerged as promising tools for predicting cybercrime and preventing it before it occurs. This paper aims to provide a comprehensive survey of the latest advancements in cybercrime prediction using above mentioned techniques, highlighting the latest research related to each approach. For this purpose, we reviewed more than 150 research articles and discussed around 50 most recent and relevant research articles. We start the review by discussing some common methods used by cyber criminals and then focus on the latest machine learning techniques and deep learning techniques, such as recurrent and convolutional neural networks, which were effective in detecting anomalous behavior and identifying potential threats. We also discuss transfer learning, which allows models trained on one dataset to be adapted for use on another dataset, and then focus on active and reinforcement Learning as part of early-stage algorithmic research in cybercrime prediction. Finally, we discuss critical innovations, research gaps, and future research opportunities in Cybercrime prediction. Overall, this paper presents a holistic view of cutting-edge developments in cybercrime prediction, shedding light on the strengths and limitations of each method and equipping researchers and practitioners with essential insights, publicly available datasets, and resources necessary to develop efficient cybercrime prediction systems.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.04819",
        "string": "[Advances in Cybercrime Prediction: A Survey of Machine, Deep, Transfer, and Adaptive Learning Techniques](https://arxiv.org/pdf/2304.04819)"
    },
    "Automaton-Guided Curriculum Generation for Reinforcement Learning Agents": {
        "abstract": "Despite advances in Reinforcement Learning, many sequential decision making tasks remain prohibitively expensive and impractical to learn. Recently, approaches that automatically generate reward functions from logical task specifications have been proposed to mitigate this issue; however, they scale poorly on long-horizon tasks (i.e., tasks where the agent needs to perform a series of correct actions to reach the goal state, considering future transitions while choosing an action). Employing a curriculum (a sequence of increasingly complex tasks) further improves the learning speed of the agent by sequencing intermediate tasks suited to the learning capacity of the agent. However, generating curricula from the logical specification still remains an unsolved problem. To this end, we propose AGCL, Automaton-guided Curriculum Learning, a novel method for automatically generating curricula for the target task in the form of Directed Acyclic Graphs (DAGs). AGCL encodes the specification in the form of a deterministic finite automaton (DFA), and then uses the DFA along with the Object-Oriented MDP (OOMDP) representation to generate a curriculum as a DAG, where the vertices correspond to tasks, and edges correspond to the direction of knowledge transfer. Experiments in gridworld and physics-based simulated robotics domains show that the curricula produced by AGCL achieve improved time-to-threshold performance on a complex sequential decision-making problem relative to state-of-the-art curriculum learning (e.g, teacher-student, self-play) and automaton-guided reinforcement learning baselines (e.g, Q-Learning for Reward Machines). Further, we demonstrate that AGCL performs well even in the presence of noise in the task's OOMDP description, and also when distractor objects are present that are not modeled in the logical specification of the tasks' objectives.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.05271",
        "string": "[Automaton-Guided Curriculum Generation for Reinforcement Learning Agents](https://arxiv.org/pdf/2304.05271)"
    },
    "Bandit-Based Policy Invariant Explicit Shaping for Incorporating External Advice in Reinforcement Learning": {
        "abstract": "A key challenge for a reinforcement learning (RL) agent is to incorporate external/expert1 advice in its learning. The desired goals of an algorithm that can shape the learning of an RL agent with external advice include (a) maintaining policy invariance; (b) accelerating the learning of the agent; and (c) learning from arbitrary advice [3]. To address this challenge this paper formulates the problem of incorporating external advice in RL as a multi-armed bandit called shaping-bandits. The reward of each arm of shaping bandits corresponds to the return obtained by following the expert or by following a default RL algorithm learning on the true environment reward.We show that directly applying existing bandit and shaping algorithms that do not reason about the non-stationary nature of the underlying returns can lead to poor results. Thus we propose UCB-PIES (UPIES), Racing-PIES (RPIES), and Lazy PIES (LPIES) three different shaping algorithms built on different assumptions that reason about the long-term consequences of following the expert policy or the default RL algorithm. Our experiments in four different settings show that these proposed algorithms achieve the above-mentioned goals whereas the other algorithms fail to do so.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.07163",
        "string": "[Bandit-Based Policy Invariant Explicit Shaping for Incorporating External Advice in Reinforcement Learning](https://arxiv.org/pdf/2304.07163)"
    },
    "Bi-level Latent Variable Model for Sample-Efficient Multi-Agent Reinforcement Learning": {
        "abstract": "Despite their potential in real-world applications, multi-agent reinforcement learning (MARL) algorithms often suffer from high sample complexity. To address this issue, we present a novel model-based MARL algorithm, BiLL (Bi-Level Latent Variable Model-based Learning), that learns a bi-level latent variable model from high-dimensional inputs. At the top level, the model learns latent representations of the global state, which encode global information relevant to behavior learning. At the bottom level, it learns latent representations for each agent, given the global latent representations from the top level. The model generates latent trajectories to use for policy learning. We evaluate our algorithm on complex multi-agent tasks in the challenging SMAC and Flatland environments. Our algorithm outperforms state-of-the-art model-free and model-based baselines in sample efficiency, including on two extremely challenging Super Hard SMAC maps.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.06011",
        "string": "[Bi-level Latent Variable Model for Sample-Efficient Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/2304.06011)"
    },
    "ChatGPT: Applications, Opportunities, and Threats": {
        "abstract": "Developed by OpenAI, ChatGPT (Conditional Generative Pre-trained Transformer) is an artificial intelligence technology that is fine-tuned using supervised machine learning and reinforcement learning techniques, allowing a computer to generate natural language conversation fully autonomously. ChatGPT is built on the transformer architecture and trained on millions of conversations from various sources. The system combines the power of pre-trained deep learning models with a programmability layer to provide a strong base for generating natural language conversations. In this study, after reviewing the existing literature, we examine the applications, opportunities, and threats of ChatGPT in 10 main domains, providing detailed examples for the business and industry as well as education. We also conducted an experimental study, checking the effectiveness and comparing the performances of GPT-3.5 and GPT-4, and found that the latter performs significantly better. Despite its exceptional ability to generate natural-sounding responses, the authors believe that ChatGPT does not possess the same level of understanding, empathy, and creativity as a human and cannot fully replace them in most situations.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.09103",
        "string": "[ChatGPT: Applications, Opportunities, and Threats](https://arxiv.org/pdf/2304.09103)"
    },
    "Contact Models in Robotics: a Comparative Analysis": {
        "abstract": "Physics simulation is ubiquitous in robotics. Whether in model-based approaches (e.g., trajectory optimization), or model-free algorithms (e.g., reinforcement learning), physics simulators are a central component of modern control pipelines in robotics. Over the past decades, several robotic simulators have been developed, each with dedicated contact modeling assumptions and algorithmic solutions. In this article, we survey the main contact models and the associated numerical methods commonly used in robotics for simulating advanced robot motions involving contact interactions. In particular, we recall the physical laws underlying contacts and friction (i.e., Signorini condition, Coulomb's law, and the maximum dissipation principle), and how they are transcribed in current simulators. For each physics engine, we expose their inherent physical relaxations along with their limitations due to the numerical techniques employed. Based on our study, we propose theoretically grounded quantitative criteria on which we build benchmarks assessing both the physical and computational aspects of simulation. We support our work with an open-source and efficient C++ implementation of the existing algorithmic variations. Our results demonstrate that some approximations or algorithms commonly used in robotics can severely widen the reality gap and impact target applications. We hope this work will help motivate the development of new contact models, contact solvers, and robotic simulators in general, at the root of recent progress in motion generation in robotics.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.06372",
        "string": "[Contact Models in Robotics: a Comparative Analysis](https://arxiv.org/pdf/2304.06372)"
    },
    "Context-aware Domain Adaptation for Time Series Anomaly Detection": {
        "abstract": "Time series anomaly detection is a challenging task with a wide range of real-world applications. Due to label sparsity, training a deep anomaly detector often relies on unsupervised approaches. Recent efforts have been devoted to time series domain adaptation to leverage knowledge from similar domains. However, existing solutions may suffer from negative knowledge transfer on anomalies due to their diversity and sparsity. Motivated by the empirical study of context alignment between two domains, we aim to transfer knowledge between two domains via adaptively sampling context information for two domains. This is challenging because it requires simultaneously modeling the complex in-domain temporal dependencies and cross-domain correlations while exploiting label information from the source domain. To this end, we propose a framework that combines context sampling and anomaly detection into a joint learning procedure. We formulate context sampling into the Markov decision process and exploit deep reinforcement learning to optimize the time series domain adaptation process via context sampling and design a tailored reward function to generate domain-invariant features that better align two domains for anomaly detection. Experiments on three public datasets show promise for knowledge transfer between two similar domains and two entirely different domains.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.07453",
        "string": "[Context-aware Domain Adaptation for Time Series Anomaly Detection](https://arxiv.org/pdf/2304.07453)"
    },
    "Control invariant set enhanced reinforcement learning for process control: improved sampling efficiency and guaranteed stability": {
        "abstract": "Reinforcement learning (RL) is an area of significant research interest, and safe RL in particular is attracting attention due to its ability to handle safety-driven constraints that are crucial for real-world applications of RL algorithms. This work proposes a novel approach to RL training, called control invariant set (CIS) enhanced RL, which leverages the benefits of CIS to improve stability guarantees and sampling efficiency. The approach consists of two learning stages: offline and online. In the offline stage, CIS is incorporated into the reward design, initial state sampling, and state reset procedures. In the online stage, RL is retrained whenever the state is outside of CIS, which serves as a stability criterion. A backup table that utilizes the explicit form of CIS is obtained to ensure the online stability. To evaluate the proposed approach, we apply it to a simulated chemical reactor. The results show a significant improvement in sampling efficiency during offline training and closed-loop stability in the online implementation.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.05509",
        "string": "[Control invariant set enhanced reinforcement learning for process control: improved sampling efficiency and guaranteed stability](https://arxiv.org/pdf/2304.05509)"
    },
    "DOSM: Demand-Prediction based Online Service Management for Vehicular Edge Computing Networks": {
        "abstract": "In this work, we investigate an online service management problem in vehicular edge computing networks. To satisfy the varying service demands of mobile vehicles, a service management framework is required to make decisions on the service lifecycle to maintain good network performance. We describe the service lifecycle consists of creating an instance of a given service (\\textit{scale-out}), moving an instance to a different edge node (\\textit{migration}), and/or termination of an underutilized instance (\\textit{scale-in}). In this paper, we propose an efficient online algorithm to perform service management in each time slot, where performance quality in the current time slot, the service demand in future time slots, and the minimal observed delay by vehicles and the minimal migration delay are considered while making the decisions on service lifecycle. Here, the future service demand is computed from a gated recurrent unit (GRU)-based prediction model, and the network performance quality is estimated using a deep reinforcement learning (DRL) model which has the ability to interact with the vehicular environment in real-time. The choice of optimal edge location to deploy a service instance at different times is based on our proposed optimization formulations. Simulation experiments using real-world vehicle trajectories are carried out to evaluate the performance of our proposed demand-prediction based online service management (DOSM) framework against different state-of-the-art solutions using several performance metrics.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.05637",
        "string": "[DOSM: Demand-Prediction based Online Service Management for Vehicular Edge Computing Networks](https://arxiv.org/pdf/2304.05637)"
    },
    "Deep Reinforcement Learning with Importance Weighted A3C for QoE enhancement in Video Delivery Services": {
        "abstract": "Adaptive bitrate (ABR) algorithms are used to adapt the video bitrate based on the network conditions to improve the overall video quality of experience (QoE). Recently, reinforcement learning (RL) and asynchronous advantage actor-critic (A3C) methods have been used to generate adaptive bit rate algorithms and they have been shown to improve the overall QoE as compared to fixed rule ABR algorithms. However, a common issue in the A3C methods is the lag between behaviour policy and target policy. As a result, the behaviour and the target policies are no longer synchronized which results in suboptimal updates. In this work, we present ALISA: An Actor-Learner Architecture with Importance Sampling for efficient learning in ABR algorithms. ALISA incorporates importance sampling weights to give more weightage to relevant experience to address the lag issues with the existing A3C methods. We present the design and implementation of ALISA, and compare its performance to state-of-the-art video rate adaptation algorithms including vanilla A3C implemented in the Pensieve framework and other fixed-rule schedulers like BB, BOLA, and RB. Our results show that ALISA improves average QoE by up to 25%-48% higher average QoE than Pensieve, and even more when compared to fixed-rule schedulers.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.04527",
        "string": "[Deep Reinforcement Learning with Importance Weighted A3C for QoE enhancement in Video Delivery Services](https://arxiv.org/pdf/2304.04527)"
    },
    "Deep reinforcement learning applied to an assembly sequence planning problem with user preferences": {
        "abstract": "Deep reinforcement learning (DRL) has demonstrated its potential in solving complex manufacturing decision-making problems, especially in a context where the system learns over time with actual operation in the absence of training data. One interesting and challenging application for such methods is the assembly sequence planning (ASP) problem. In this paper, we propose an approach to the implementation of DRL methods in ASP. The proposed approach introduces in the RL environment parametric actions to improve training time and sample efficiency and uses two different reward signals: (1) user's preferences and (2) total assembly time duration. The user's preferences signal addresses the difficulties and non-ergonomic properties of the assembly faced by the human and the total assembly time signal enforces the optimization of the assembly. Three of the most powerful deep RL methods were studied, Advantage Actor-Critic (A2C), Deep Q-Learning (DQN), and Rainbow, in two different scenarios: a stochastic and a deterministic one. Finally, the performance of the DRL algorithms was compared to tabular Q-Learnings performance. After 10,000 episodes, the system achieved near optimal behaviour for the algorithms tabular Q-Learning, A2C, and Rainbow. Though, for more complex scenarios, the algorithm tabular Q-Learning is expected to underperform in comparison to the other 2 algorithms. The results support the potential for the application of deep reinforcement learning in assembly sequence planning problems with human interaction.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.06567",
        "string": "[Deep reinforcement learning applied to an assembly sequence planning problem with user preferences](https://arxiv.org/pdf/2304.06567)"
    },
    "Dexterous In-Hand Manipulation of Slender Cylindrical Objects through Deep Reinforcement Learning with Tactile Sensing": {
        "abstract": "Continuous in-hand manipulation is an important physical interaction skill, where tactile sensing provides indispensable contact information to enable dexterous manipulation of small objects. This work proposed a framework for end-to-end policy learning with tactile feedback and sim-to-real transfer, which achieved fine in-hand manipulation that controls the pose of a thin cylindrical object, such as a long stick, to track various continuous trajectories through multiple contacts of three fingertips of a dexterous robot hand with tactile sensor arrays. We estimated the central contact position between the stick and each fingertip from the high-dimensional tactile information and showed that the learned policies achieved effective manipulation performance with the processed tactile feedback. The policies were trained with deep reinforcement learning in simulation and successfully transferred to real-world experiments, using coordinated model calibration and domain randomization. We evaluated the effectiveness of tactile information via comparative studies and validated the sim-to-real performance through real-world experiments.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.05141",
        "string": "[Dexterous In-Hand Manipulation of Slender Cylindrical Objects through Deep Reinforcement Learning with Tactile Sensing](https://arxiv.org/pdf/2304.05141)"
    },
    "Diagnosing and Augmenting Feature Representations in Correctional Inverse Reinforcement Learning": {
        "abstract": "Robots have been increasingly better at doing tasks for humans by learning from their feedback, but still often suffer from model misalignment due to missing or incorrectly learned features. When the features the robot needs to learn to perform its task are missing or do not generalize well to new settings, the robot will not be able to learn the task the human wants and, even worse, may learn a completely different and undesired behavior. Prior work shows how the robot can detect when its representation is missing some feature and can, thus, ask the human to be taught about the new feature; however, these works do not differentiate between features that are completely missing and those that exist but do not generalize to new environments. In the latter case, the robot would detect misalignment and simply learn a new feature, leading to an arbitrarily growing feature representation that can, in turn, lead to spurious correlations and incorrect learning down the line. In this work, we propose separating the two sources of misalignment: we propose a framework for determining whether a feature the robot needs is incorrectly learned and does not generalize to new environment setups vs. is entirely missing from the robot's representation. Once we detect the source of error, we show how the human can initiate the realignment process for the model: if the feature is missing, we follow prior work for learning new features; however, if the feature exists but does not generalize, we use data augmentation to expand its training and, thus, complete the correction. We demonstrate the proposed approach in experiments with a simulated 7DoF robot manipulator and physical human corrections.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.05238",
        "string": "[Diagnosing and Augmenting Feature Representations in Correctional Inverse Reinforcement Learning](https://arxiv.org/pdf/2304.05238)"
    },
    "Did we personalize? Assessing personalization by an online reinforcement learning algorithm using resampling": {
        "abstract": "There is a growing interest in using reinforcement learning (RL) to personalize sequences of treatments in digital health to support users in adopting healthier behaviors. Such sequential decision-making problems involve decisions about when to treat and how to treat based on the user's context (e.g., prior activity level, location, etc.). Online RL is a promising data-driven approach for this problem as it learns based on each user's historical responses and uses that knowledge to personalize these decisions. However, to decide whether the RL algorithm should be included in an ``optimized'' intervention for real-world deployment, we must assess the data evidence indicating that the RL algorithm is actually personalizing the treatments to its users. Due to the stochasticity in the RL algorithm, one may get a false impression that it is learning in certain states and using this learning to provide specific treatments. We use a working definition of personalization and introduce a resampling-based methodology for investigating whether the personalization exhibited by the RL algorithm is an artifact of the RL algorithm stochasticity. We illustrate our methodology with a case study by analyzing the data from a physical activity clinical trial called HeartSteps, which included the use of an online RL algorithm. We demonstrate how our approach enhances data-driven truth-in-advertising of algorithm personalization both across all users as well as within specific users in the study.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.05365",
        "string": "[Did we personalize? Assessing personalization by an online reinforcement learning algorithm using resampling](https://arxiv.org/pdf/2304.05365)"
    },
    "Efficient Automation of Neural Network Design: A Survey on Differentiable Neural Architecture Search": {
        "abstract": "In the past few years, Differentiable Neural Architecture Search (DNAS) rapidly imposed itself as the trending approach to automate the discovery of deep neural network architectures. This rise is mainly due to the popularity of DARTS, one of the first major DNAS methods. In contrast with previous works based on Reinforcement Learning or Evolutionary Algorithms, DNAS is faster by several orders of magnitude and uses fewer computational resources. In this comprehensive survey, we focus specifically on DNAS and review recent approaches in this field. Furthermore, we propose a novel challenge-based taxonomy to classify DNAS methods. We also discuss the contributions brought to DNAS in the past few years and its impact on the global NAS field. Finally, we conclude by giving some insights into future research directions for the DNAS field.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.05405",
        "string": "[Efficient Automation of Neural Network Design: A Survey on Differentiable Neural Architecture Search](https://arxiv.org/pdf/2304.05405)"
    },
    "Emergent autonomous scientific research capabilities of large language models": {
        "abstract": "Transformer-based large language models are rapidly advancing in the field of machine learning research, with applications spanning natural language, biology, chemistry, and computer programming. Extreme scaling and reinforcement learning from human feedback have significantly improved the quality of generated text, enabling these models to perform various tasks and reason about their choices. In this paper, we present an Intelligent Agent system that combines multiple large language models for autonomous design, planning, and execution of scientific experiments. We showcase the Agent's scientific research capabilities with three distinct examples, with the most complex being the successful performance of catalyzed cross-coupling reactions. Finally, we discuss the safety implications of such systems and propose measures to prevent their misuse.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.05332",
        "string": "[Emergent autonomous scientific research capabilities of large language models](https://arxiv.org/pdf/2304.05332)"
    },
    "Exploiting Intrinsic Stochasticity of Real-Time Simulation to Facilitate Robust Reinforcement Learning for Robot Manipulation": {
        "abstract": "Simulation is essential to reinforcement learning (RL) before implementation in the real world, especially for safety-critical applications like robot manipulation. Conventionally, RL agents are sensitive to the discrepancies between the simulation and the real world, known as the sim-to-real gap. The application of domain randomization, a technique used to fill this gap, is limited to the imposition of heuristic-randomized models. We investigate the properties of intrinsic stochasticity of real-time simulation (RT-IS) of off-the-shelf simulation software and its potential to improve the robustness of RL methods and the performance of domain randomization. Firstly, we conduct analytical studies to measure the correlation of RT-IS with the occupation of the computer hardware and validate its comparability with the natural stochasticity of a physical robot. Then, we apply the RT-IS feature in the training of an RL agent. The simulation and physical experiment results verify the feasibility and applicability of RT-IS to robust RL agent design for robot manipulation tasks. The RT-IS-powered robust RL agent outperforms conventional RL agents on robots with modeling uncertainties. It requires fewer heuristic randomization and achieves better generalizability than the conventional domain-randomization-powered agents. Our findings provide a new perspective on the sim-to-real problem in practical applications like robot manipulation tasks.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.06056",
        "string": "[Exploiting Intrinsic Stochasticity of Real-Time Simulation to Facilitate Robust Reinforcement Learning for Robot Manipulation](https://arxiv.org/pdf/2304.06056)"
    },
    "Exploiting Symmetry and Heuristic Demonstrations in Off-policy Reinforcement Learning for Robotic Manipulation": {
        "abstract": "Reinforcement learning demonstrates significant potential in automatically building control policies in numerous domains, but shows low efficiency when applied to robot manipulation tasks due to the curse of dimensionality. To facilitate the learning of such tasks, prior knowledge or heuristics that incorporate inherent simplification can effectively improve the learning performance. This paper aims to define and incorporate the natural symmetry present in physical robotic environments. Then, sample-efficient policies are trained by exploiting the expert demonstrations in symmetrical environments through an amalgamation of reinforcement and behavior cloning, which gives the off-policy learning process a diverse yet compact initiation. Furthermore, it presents a rigorous framework for a recent concept and explores its scope for robot manipulation tasks. The proposed method is validated via two point-to-point reaching tasks of an industrial arm, with and without an obstacle, in a simulation experiment study. A PID controller, which tracks the linear joint-space trajectories with hard-coded temporal logic to produce interim midpoints, is used to generate demonstrations in the study. The results of the study present the effect of the number of demonstrations and quantify the magnitude of behavior cloning to exemplify the possible improvement of model-free reinforcement learning in common manipulation tasks. A comparison study between the proposed method and a traditional off-policy reinforcement learning algorithm indicates its advantage in learning performance and potential value for applications.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.06055",
        "string": "[Exploiting Symmetry and Heuristic Demonstrations in Off-policy Reinforcement Learning for Robotic Manipulation](https://arxiv.org/pdf/2304.06055)"
    },
    "Feudal Graph Reinforcement Learning": {
        "abstract": "We focus on learning composable policies to control a variety of physical agents with possibly different structures. Among state-of-the-art methods, prominent approaches exploit graph-based representations and weight-sharing modular policies based on the message-passing framework. However, as shown by recent literature, message passing can create bottlenecks in information propagation and hinder global coordination. This drawback can become even more problematic in tasks where high-level planning is crucial. In fact, in similar scenarios, each modular policy - e.g., controlling a joint of a robot - would request to coordinate not only for basic locomotion but also achieve high-level goals, such as navigating a maze. A classical solution to avoid similar pitfalls is to resort to hierarchical decision-making. In this work, we adopt the Feudal Reinforcement Learning paradigm to develop agents where control actions are the outcome of a hierarchical (pyramidal) message-passing process. In the proposed Feudal Graph Reinforcement Learning (FGRL) framework, high-level decisions at the top level of the hierarchy are propagated through a layered graph representing a hierarchy of policies. Lower layers mimic the morphology of the physical system and upper layers can capture more abstract sub-modules. The purpose of this preliminary work is to formalize the framework and provide proof-of-concept experiments on benchmark environments (MuJoCo locomotion tasks). Empirical evaluation shows promising results on both standard benchmarks and zero-shot transfer learning settings.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.05099",
        "string": "[Feudal Graph Reinforcement Learning](https://arxiv.org/pdf/2304.05099)"
    },
    "For Pre-Trained Vision Models in Motor Control, Not All Policy Learning Methods are Created Equal": {
        "abstract": "In recent years, increasing attention has been directed to leveraging pre-trained vision models for motor control. While existing works mainly emphasize the importance of this pre-training phase, the arguably equally important role played by downstream policy learning during control-specific fine-tuning is often neglected. It thus remains unclear if pre-trained vision models are consistent in their effectiveness under different control policies. To bridge this gap in understanding, we conduct a comprehensive study on 14 pre-trained vision models using 3 distinct classes of policy learning methods, including reinforcement learning (RL), imitation learning through behavior cloning (BC), and imitation learning with a visual reward function (VRF). Our study yields a series of intriguing results, including the discovery that the effectiveness of pre-training is highly dependent on the choice of the downstream policy learning algorithm. We show that conventionally accepted evaluation based on RL methods is highly variable and therefore unreliable, and further advocate for using more robust methods like VRF and BC. To facilitate more universal evaluations of pre-trained models and their policy learning methods in the future, we also release a benchmark of 21 tasks across 3 different environments alongside our work.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.04591",
        "string": "[For Pre-Trained Vision Models in Motor Control, Not All Policy Learning Methods are Created Equal](https://arxiv.org/pdf/2304.04591)"
    },
    "Frontier Semantic Exploration for Visual Target Navigation": {
        "abstract": "This work focuses on the problem of visual target navigation, which is very important for autonomous robots as it is closely related to high-level tasks. To find a special object in unknown environments, classical and learning-based approaches are fundamental components of navigation that have been investigated thoroughly in the past. However, due to the difficulty in the representation of complicated scenes and the learning of the navigation policy, previous methods are still not adequate, especially for large unknown scenes. Hence, we propose a novel framework for visual target navigation using the frontier semantic policy. In this proposed framework, the semantic map and the frontier map are built from the current observation of the environment. Using the features of the maps and object category, deep reinforcement learning enables to learn a frontier semantic policy which can be used to select a frontier cell as a long-term goal to explore the environment efficiently. Experiments on Gibson and Habitat-Matterport 3D (HM3D) demonstrate that the proposed framework significantly outperforms existing map-based methods in terms of success rate and efficiency. Ablation analysis also indicates that the proposed approach learns a more efficient exploration policy based on the frontiers. A demonstration is provided to verify the applicability of applying our model to real-world transfer. The supplementary video and code can be accessed via the following link: https://sites.google.com/view/fsevn.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.05506",
        "string": "[Frontier Semantic Exploration for Visual Target Navigation](https://arxiv.org/pdf/2304.05506)"
    },
    "Hamiltonian Switching Control of Noisy Bipartite Qubit Systems": {
        "abstract": "We develop a Hamiltonian switching ansatz for bipartite control that is inspired by the Quantum Approximate Optimization Algorithm (QAOA), to mitigate environmental noise on qubits. We illustrate the approach with application to the protection of quantum gates performed on i) a central spin qubit coupling to bath spins through isotropic Heisenberg interactions, ii) superconducting transmon qubits coupling to environmental two-level-systems (TLS) through dipole-dipole interactions, and iii) qubits coupled to both TLS and a Lindblad bath. The control field is classical and acts only on the system qubits. We use reinforcement learning with policy gradient (PG) to optimize the Hamiltonian switching control protocols, using a fidelity objective defined with respect to specific target quantum gates. We use this approach to demonstrate effective suppression of both coherent and dissipative noise, with numerical studies achieving target gate implementations with fidelities over 0.9999 (four nines) in the majority of our test cases and showing improvement beyond this to values of 0.999999999 (nine nines) upon a subsequent optimization by Gradient Ascent Pulse Engineering (GRAPE). We analyze how the control depth, total evolution time, number of environmental TLS, and choice of optimization method affect the fidelity achieved by the optimal protocols and reveal some critical behaviors of bipartite control of quantum gates.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.05478",
        "string": "[Hamiltonian Switching Control of Noisy Bipartite Qubit Systems](https://arxiv.org/pdf/2304.05478)"
    },
    "Hierarchical Agent-based Reinforcement Learning Framework for Automated Quality Assessment of Fetal Ultrasound Video": {
        "abstract": "Ultrasound is the primary modality to examine fetal growth during pregnancy, while the image quality could be affected by various factors. Quality assessment is essential for controlling the quality of ultrasound images to guarantee both the perceptual and diagnostic values. Existing automated approaches often require heavy structural annotations and the predictions may not necessarily be consistent with the assessment results by human experts. Furthermore, the overall quality of a scan and the correlation between the quality of frames should not be overlooked. In this work, we propose a reinforcement learning framework powered by two hierarchical agents that collaboratively learn to perform both frame-level and video-level quality assessments. It is equipped with a specially-designed reward mechanism that considers temporal dependency among frame quality and only requires sparse binary annotations to train. Experimental results on a challenging fetal brain dataset verify that the proposed framework could perform dual-level quality assessment and its predictions correlate well with the subjective assessment results.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.07036",
        "string": "[Hierarchical Agent-based Reinforcement Learning Framework for Automated Quality Assessment of Fetal Ultrasound Video](https://arxiv.org/pdf/2304.07036)"
    },
    "High-Fidelity, Frequency-Flexible Two-Qubit Fluxonium Gates with a Transmon Coupler": {
        "abstract": "We propose and demonstrate an architecture for fluxonium-fluxonium two-qubit gates mediated by transmon couplers (FTF, for fluxonium-transmon-fluxonium). Relative to architectures that exclusively rely on a direct coupling between fluxonium qubits, FTF enables stronger couplings for gates using non-computational states while simultaneously suppressing the static controlled-phase entangling rate ($ZZ$) down to kHz levels, all without requiring strict parameter matching. Here we implement FTF with a flux-tunable transmon coupler and demonstrate a microwave-activated controlled-Z (CZ) gate whose operation frequency can be tuned over a 2 GHz range, adding frequency allocation freedom for FTF's in larger systems. Across this range, state-of-the-art CZ gate fidelities were observed over many bias points and reproduced across the two devices characterized in this work. After optimizing both the operation frequency and the gate duration, we achieved peak CZ fidelities in the 99.85-99.9\\% range. Finally, we implemented model-free reinforcement learning of the pulse parameters to boost the mean gate fidelity up to $99.922\\pm0.009\\%$, averaged over roughly an hour between scheduled training runs. Beyond the microwave-activated CZ gate we present here, FTF can be applied to a variety of other fluxonium gate schemes to improve gate fidelities and passively reduce unwanted $ZZ$ interactions.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.06087",
        "string": "[High-Fidelity, Frequency-Flexible Two-Qubit Fluxonium Gates with a Transmon Coupler](https://arxiv.org/pdf/2304.06087)"
    },
    "Human-Robot Skill Transfer with Enhanced Compliance via Dynamic Movement Primitives": {
        "abstract": "Finding an efficient way to adapt robot trajectory is a priority to improve overall performance of robots. One approach for trajectory planning is through transferring human-like skills to robots by Learning from Demonstrations (LfD). The human demonstration is considered the target motion to mimic. However, human motion is typically optimal for human embodiment but not for robots because of the differences between human biomechanics and robot dynamics. The Dynamic Movement Primitives (DMP) framework is a viable solution for this limitation of LfD, but it requires tuning the second-order dynamics in the formulation. Our contribution is introducing a systematic method to extract the dynamic features from human demonstration to auto-tune the parameters in the DMP framework. In addition to its use with LfD, another utility of the proposed method is that it can readily be used in conjunction with Reinforcement Learning (RL) for robot training. In this way, the extracted features facilitate the transfer of human skills by allowing the robot to explore the possible trajectories more efficiently and increasing robot compliance significantly. We introduced a methodology to extract the dynamic features from multiple trajectories based on the optimization of human-likeness and similarity in the parametric space. Our method was implemented into an actual human-robot setup to extract human dynamic features and used to regenerate the robot trajectories following both LfD and RL with DMP. It resulted in a stable performance of the robot, maintaining a high degree of human-likeness based on accumulated distance error as good as the best heuristic tuning.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.05703",
        "string": "[Human-Robot Skill Transfer with Enhanced Compliance via Dynamic Movement Primitives](https://arxiv.org/pdf/2304.05703)"
    },
    "Improving ABR Performance for Short Video Streaming Using Multi-Agent Reinforcement Learning with Expert Guidance": {
        "abstract": "In the realm of short video streaming, popular adaptive bitrate (ABR) algorithms developed for classical long video applications suffer from catastrophic failures because they are tuned to solely adapt bitrates. Instead, short video adaptive bitrate (SABR) algorithms have to properly determine which video at which bitrate level together for content prefetching, without sacrificing the users' quality of experience (QoE) and yielding noticeable bandwidth wastage jointly. Unfortunately, existing SABR methods are inevitably entangled with slow convergence and poor generalization. Thus, in this paper, we propose Incendio, a novel SABR framework that applies Multi-Agent Reinforcement Learning (MARL) with Expert Guidance to separate the decision of video ID and video bitrate in respective buffer management and bitrate adaptation agents to maximize the system-level utilized score modeled as a compound function of QoE and bandwidth wastage metrics. To train Incendio, it is first initialized by imitating the hand-crafted expert rules and then fine-tuned through the use of MARL. Results from extensive experiments indicate that Incendio outperforms the current state-of-the-art SABR algorithm with a 53.2% improvement measured by the utility score while maintaining low training complexity and inference time.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.04637",
        "string": "[Improving ABR Performance for Short Video Streaming Using Multi-Agent Reinforcement Learning with Expert Guidance](https://arxiv.org/pdf/2304.04637)"
    },
    "Language Instructed Reinforcement Learning for Human-AI Coordination": {
        "abstract": "One of the fundamental quests of AI is to produce agents that coordinate well with humans. This problem is challenging, especially in domains that lack high quality human behavioral data, because multi-agent reinforcement learning (RL) often converges to different equilibria from the ones that humans prefer. We propose a novel framework, instructRL, that enables humans to specify what kind of strategies they expect from their AI partners through natural language instructions. We use pretrained large language models to generate a prior policy conditioned on the human instruction and use the prior to regularize the RL objective. This leads to the RL agent converging to equilibria that are aligned with human preferences. We show that instructRL converges to human-like policies that satisfy the given instructions in a proof-of-concept environment as well as the challenging Hanabi benchmark. Finally, we show that knowing the language instruction significantly boosts human-AI coordination performance in human evaluations in Hanabi.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.07297",
        "string": "[Language Instructed Reinforcement Learning for Human-AI Coordination](https://arxiv.org/pdf/2304.07297)"
    },
    "Learning Over All Contracting and Lipschitz Closed-Loops for Partially-Observed Nonlinear Systems": {
        "abstract": "This paper presents a policy parameterization for learning-based control on nonlinear, partially-observed dynamical systems. The parameterization is based on a nonlinear version of the Youla parameterization and the recently proposed Recurrent Equilibrium Network (REN) class of models. We prove that the resulting Youla-REN parameterization automatically satisfies stability (contraction) and user-tunable robustness (Lipschitz) conditions on the closed-loop system. This means it can be used for safe learning-based control with no additional constraints or projections required to enforce stability or robustness. We test the new policy class in simulation on two reinforcement learning tasks: 1) magnetic suspension, and 2) inverting a rotary-arm pendulum. We find that the Youla-REN performs similarly to existing learning-based and optimal control methods while also ensuring stability and exhibiting improved robustness to adversarial disturbances.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.06193",
        "string": "[Learning Over All Contracting and Lipschitz Closed-Loops for Partially-Observed Nonlinear Systems](https://arxiv.org/pdf/2304.06193)"
    },
    "Learning a Universal Human Prior for Dexterous Manipulation from Human Preference": {
        "abstract": "Generating human-like behavior on robots is a great challenge especially in dexterous manipulation tasks with robotic hands. Even in simulation with no sample constraints, scripting controllers is intractable due to high degrees of freedom, and manual reward engineering can also be hard and lead to non-realistic motions. Leveraging the recent progress on Reinforcement Learning from Human Feedback (RLHF), we propose a framework to learn a universal human prior using direct human preference feedback over videos, for efficiently tuning the RL policy on 20 dual-hand robot manipulation tasks in simulation, without a single human demonstration. One task-agnostic reward model is trained through iteratively generating diverse polices and collecting human preference over the trajectories; it is then applied for regularizing the behavior of polices in the fine-tuning stage. Our method empirically demonstrates more human-like behaviors on robot hands in diverse tasks including even unseen tasks, indicating its generalization capability.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.04602",
        "string": "[Learning a Universal Human Prior for Dexterous Manipulation from Human Preference](https://arxiv.org/pdf/2304.04602)"
    },
    "Learning to Communicate and Collaborate in a Competitive Multi-Agent Setup to Clean the Ocean from Macroplastics": {
        "abstract": "Finding a balance between collaboration and competition is crucial for artificial agents in many real-world applications. We investigate this using a Multi-Agent Reinforcement Learning (MARL) setup on the back of a high-impact problem. The accumulation and yearly growth of plastic in the ocean cause irreparable damage to many aspects of oceanic health and the marina system. To prevent further damage, we need to find ways to reduce macroplastics from known plastic patches in the ocean. Here we propose a Graph Neural Network (GNN) based communication mechanism that increases the agents' observation space. In our custom environment, agents control a plastic collecting vessel. The communication mechanism enables agents to develop a communication protocol using a binary signal. While the goal of the agent collective is to clean up as much as possible, agents are rewarded for the individual amount of macroplastics collected. Hence agents have to learn to communicate effectively while maintaining high individual performance. We compare our proposed communication mechanism with a multi-agent baseline without the ability to communicate. Results show communication enables collaboration and increases collective performance significantly. This means agents have learned the importance of communication and found a balance between collaboration and competition.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.05872",
        "string": "[Learning to Communicate and Collaborate in a Competitive Multi-Agent Setup to Clean the Ocean from Macroplastics](https://arxiv.org/pdf/2304.05872)"
    },
    "Learning to Learn Group Alignment: A Self-Tuning Credo Framework with Multiagent Teams": {
        "abstract": "Mixed incentives among a population with multiagent teams has been shown to have advantages over a fully cooperative system; however, discovering the best mixture of incentives or team structure is a difficult and dynamic problem. We propose a framework where individual learning agents self-regulate their configuration of incentives through various parts of their reward function. This work extends previous work by giving agents the ability to dynamically update their group alignment during learning and by allowing teammates to have different group alignment. Our model builds on ideas from hierarchical reinforcement learning and meta-learning to learn the configuration of a reward function that supports the development of a behavioral policy. We provide preliminary results in a commonly studied multiagent environment and find that agents can achieve better global outcomes by self-tuning their respective group alignment parameters.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.07337",
        "string": "[Learning to Learn Group Alignment: A Self-Tuning Credo Framework with Multiagent Teams](https://arxiv.org/pdf/2304.07337)"
    },
    "Medical Question Summarization with Entity-driven Contrastive Learning": {
        "abstract": "By summarizing longer consumer health questions into shorter and essential ones, medical question answering (MQA) systems can more accurately understand consumer intentions and retrieve suitable answers. However, medical question summarization is very challenging due to obvious distinctions in health trouble descriptions from patients and doctors. Although existing works have attempted to utilize Seq2Seq, reinforcement learning, or contrastive learning to solve the problem, two challenges remain: how to correctly capture question focus to model its semantic intention, and how to obtain reliable datasets to fairly evaluate performance. To address these challenges, this paper proposes a novel medical question summarization framework using entity-driven contrastive learning (ECL). ECL employs medical entities in frequently asked questions (FAQs) as focuses and devises an effective mechanism to generate hard negative samples. This approach forces models to pay attention to the crucial focus information and generate more ideal question summarization. Additionally, we find that some MQA datasets suffer from serious data leakage problems, such as the iCliniq dataset's 33% duplicate rate. To evaluate the related methods fairly, this paper carefully checks leaked samples to reorganize more reasonable datasets. Extensive experiments demonstrate that our ECL method outperforms state-of-the-art methods by accurately capturing question focus and generating medical question summaries. The code and datasets are available at https://github.com/yrbobo/MQS-ECL.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.07437",
        "string": "[Medical Question Summarization with Entity-driven Contrastive Learning](https://arxiv.org/pdf/2304.07437)"
    },
    "Minimax-Optimal Reward-Agnostic Exploration in Reinforcement Learning": {
        "abstract": "This paper studies reward-agnostic exploration in reinforcement learning (RL) -- a scenario where the learner is unware of the reward functions during the exploration stage -- and designs an algorithm that improves over the state of the art. More precisely, consider a finite-horizon non-stationary Markov decision process with $S$ states, $A$ actions, and horizon length $H$, and suppose that there are no more than a polynomial number of given reward functions of interest. By collecting an order of \\begin{align*}\n  \\frac{SAH^3}{\\varepsilon^2} \\text{ sample episodes (up to log factor)} \\end{align*} without guidance of the reward information, our algorithm is able to find $\\varepsilon$-optimal policies for all these reward functions, provided that $\\varepsilon$ is sufficiently small. This forms the first reward-agnostic exploration scheme in this context that achieves provable minimax optimality. Furthermore, once the sample size exceeds $\\frac{S^2AH^3}{\\varepsilon^2}$ episodes (up to log factor), our algorithm is able to yield $\\varepsilon$ accuracy for arbitrarily many reward functions (even when they are adversarially designed), a task commonly dubbed as ``reward-free exploration.'' The novelty of our algorithm design draws on insights from offline RL: the exploration scheme attempts to maximize a critical reward-agnostic quantity that dictates the performance of offline RL, while the policy learning paradigm leverages ideas from sample-optimal offline RL paradigms.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.07278",
        "string": "[Minimax-Optimal Reward-Agnostic Exploration in Reinforcement Learning](https://arxiv.org/pdf/2304.07278)"
    },
    "Model-based Dynamic Shielding for Safe and Efficient Multi-Agent Reinforcement Learning": {
        "abstract": "Multi-Agent Reinforcement Learning (MARL) discovers policies that maximize reward but do not have safety guarantees during the learning and deployment phases. Although shielding with Linear Temporal Logic (LTL) is a promising formal method to ensure safety in single-agent Reinforcement Learning (RL), it results in conservative behaviors when scaling to multi-agent scenarios. Additionally, it poses computational challenges for synthesizing shields in complex multi-agent environments. This work introduces Model-based Dynamic Shielding (MBDS) to support MARL algorithm design. Our algorithm synthesizes distributive shields, which are reactive systems running in parallel with each MARL agent, to monitor and rectify unsafe behaviors. The shields can dynamically split, merge, and recompute based on agents' states. This design enables efficient synthesis of shields to monitor agents in complex environments without coordination overheads. We also propose an algorithm to synthesize shields without prior knowledge of the dynamics model. The proposed algorithm obtains an approximate world model by interacting with the environment during the early stage of exploration, making our MBDS enjoy formal safety guarantees with high probability. We demonstrate in simulations that our framework can surpass existing baselines in terms of safety guarantees and learning performance.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.06281",
        "string": "[Model-based Dynamic Shielding for Safe and Efficient Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/2304.06281)"
    },
    "Multi-agent Policy Reciprocity with Theoretical Guarantee": {
        "abstract": "Modern multi-agent reinforcement learning (RL) algorithms hold great potential for solving a variety of real-world problems. However, they do not fully exploit cross-agent knowledge to reduce sample complexity and improve performance. Although transfer RL supports knowledge sharing, it is hyperparameter sensitive and complex. To solve this problem, we propose a novel multi-agent policy reciprocity (PR) framework, where each agent can fully exploit cross-agent policies even in mismatched states. We then define an adjacency space for mismatched states and design a plug-and-play module for value iteration, which enables agents to infer more precise returns. To improve the scalability of PR, deep PR is proposed for continuous control tasks. Moreover, theoretical analysis shows that agents can asymptotically reach consensus through individual perceived rewards and converge to an optimal value function, which implies the stability and effectiveness of PR, respectively. Experimental results on discrete and continuous environments demonstrate that PR outperforms various existing RL and transfer RL methods.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.05632",
        "string": "[Multi-agent Policy Reciprocity with Theoretical Guarantee](https://arxiv.org/pdf/2304.05632)"
    },
    "MvCo-DoT:Multi-View Contrastive Domain Transfer Network for Medical Report Generation": {
        "abstract": "In clinical scenarios, multiple medical images with different views are usually generated at the same time, and they have high semantic consistency. However, the existing medical report generation methods cannot exploit the rich multi-view mutual information of medical images. Therefore, in this work, we propose the first multi-view medical report generation model, called MvCo-DoT. Specifically, MvCo-DoT first propose a multi-view contrastive learning (MvCo) strategy to help the deep reinforcement learning based model utilize the consistency of multi-view inputs for better model learning. Then, to close the performance gaps of using multi-view and single-view inputs, a domain transfer network is further proposed to ensure MvCo-DoT achieve almost the same performance as multi-view inputs using only single-view inputs.Extensive experiments on the IU X-Ray public dataset show that MvCo-DoT outperforms the SOTA medical report generation baselines in all metrics.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.07465",
        "string": "[MvCo-DoT:Multi-View Contrastive Domain Transfer Network for Medical Report Generation](https://arxiv.org/pdf/2304.07465)"
    },
    "NaviSTAR: Socially Aware Robot Navigation with Hybrid Spatio-Temporal Graph Transformer and Preference Learning": {
        "abstract": "Developing robotic technologies for use in human society requires ensuring the safety of robots' navigation behaviors while adhering to pedestrians' expectations and social norms. However, maintaining real-time communication between robots and pedestrians to avoid collisions can be challenging. To address these challenges, we propose a novel socially-aware navigation benchmark called NaviSTAR, which utilizes a hybrid Spatio-Temporal grAph tRansformer (STAR) to understand interactions in human-rich environments fusing potential crowd multi-modal information. We leverage off-policy reinforcement learning algorithm with preference learning to train a policy and a reward function network with supervisor guidance. Additionally, we design a social score function to evaluate the overall performance of social navigation. To compare, we train and test our algorithm and other state-of-the-art methods in both simulator and real-world scenarios independently. Our results show that NaviSTAR outperforms previous methods with outstanding performance\\footnote{The source code and experiment videos of this work are available at: https://sites.google.com/view/san-navistar\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.05979",
        "string": "[NaviSTAR: Socially Aware Robot Navigation with Hybrid Spatio-Temporal Graph Transformer and Preference Learning](https://arxiv.org/pdf/2304.05979)"
    },
    "OpenAssistant Conversations -- Democratizing Large Language Model Alignment": {
        "abstract": "Aligning large language models (LLMs) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by ChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) greatly reduce the required skill and domain knowledge to effectively harness the capabilities of LLMs, increasing their accessibility and utility across various domains. However, state-of-the-art alignment techniques like RLHF rely on high-quality human feedback data, which is expensive to create and often remains proprietary. In an effort to democratize research on large-scale alignment, we release OpenAssistant Conversations, a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages distributed across 66,497 conversation trees, in 35 different languages, annotated with 461,292 quality ratings. The corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers. To demonstrate the OpenAssistant Conversations dataset's effectiveness, we present OpenAssistant, the first fully open-source large-scale instruction-tuned model to be trained on human data. A preference study revealed that OpenAssistant replies are comparably preferred to GPT-3.5-turbo (ChatGPT) with a relative winrate of 48.3% vs. 51.7% respectively. We release our code and data under fully permissive licenses.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.07327",
        "string": "[OpenAssistant Conversations -- Democratizing Large Language Model Alignment](https://arxiv.org/pdf/2304.07327)"
    },
    "Optimal Interpretability-Performance Trade-off of Classification Trees with Black-Box Reinforcement Learning": {
        "abstract": "Interpretability of AI models allows for user safety checks to build trust in these models. In particular, decision trees (DTs) provide a global view on the learned model and clearly outlines the role of the features that are critical to classify a given data. However, interpretability is hindered if the DT is too large. To learn compact trees, a Reinforcement Learning (RL) framework has been recently proposed to explore the space of DTs. A given supervised classification task is modeled as a Markov decision problem (MDP) and then augmented with additional actions that gather information about the features, equivalent to building a DT. By appropriately penalizing these actions, the RL agent learns to optimally trade-off size and performance of a DT. However, to do so, this RL agent has to solve a partially observable MDP. The main contribution of this paper is to prove that it is sufficient to solve a fully observable problem to learn a DT optimizing the interpretability-performance trade-off. As such any planning or RL algorithm can be used. We demonstrate the effectiveness of this approach on a set of classical supervised classification datasets and compare our approach with other interpretability-performance optimizing methods.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.05839",
        "string": "[Optimal Interpretability-Performance Trade-off of Classification Trees with Black-Box Reinforcement Learning](https://arxiv.org/pdf/2304.05839)"
    },
    "QNEAT: Natural Evolution of Variational Quantum Circuit Architecture": {
        "abstract": "Quantum Machine Learning (QML) is a recent and rapidly evolving field where the theoretical framework and logic of quantum mechanics are employed to solve machine learning tasks. Various techniques with different levels of quantum-classical hybridization have been proposed. Here we focus on variational quantum circuits (VQC), which emerged as the most promising candidates for the quantum counterpart of neural networks in the noisy intermediate-scale quantum (NISQ) era. Although showing promising results, VQCs can be hard to train because of different issues, e.g., barren plateau, periodicity of the weights, or choice of architecture. This paper focuses on this last problem for finding optimal architectures of variational quantum circuits for various tasks. To address it, we propose a gradient-free algorithm inspired by natural evolution to optimize both the weights and the architecture of the VQC. In particular, we present a version of the well-known neuroevolution of augmenting topologies (NEAT) algorithm and adapt it to the case of variational quantum circuits. We refer to the proposed architecture search algorithm for VQC as QNEAT. We test the algorithm with different benchmark problems of classical fields of machine learning i.e. reinforcement learning and combinatorial optimization.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.06981",
        "string": "[QNEAT: Natural Evolution of Variational Quantum Circuit Architecture](https://arxiv.org/pdf/2304.06981)"
    },
    "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment": {
        "abstract": "Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially significant repercussions. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) as a means of addressing this problem, wherein generative models are fine-tuned using RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment of generative models, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generative models more effectively. Utilizing a reward model and a sufficient number of samples, our approach selects the high-quality samples, discarding those that exhibit undesired behavior, and subsequently assembles a streaming dataset. This dataset serves as the basis for aligning the generative model and can be employed under both offline and online settings. Notably, the sample generation process within RAFT is gradient-free, rendering it compatible with black-box generators. Through extensive experiments, we demonstrate that our proposed algorithm exhibits strong performance in the context of both large language models and diffusion models.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.06767",
        "string": "[RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment](https://arxiv.org/pdf/2304.06767)"
    },
    "RESPECT: Reinforcement Learning based Edge Scheduling on Pipelined Coral Edge TPUs": {
        "abstract": "Deep neural networks (DNNs) have substantial computational and memory requirements, and the compilation of its computational graphs has a great impact on the performance of resource-constrained (e.g., computation, I/O, and memory-bound) edge computing systems. While efficient execution of their computational graph requires an effective scheduling algorithm, generating the optimal scheduling solution is a challenging NP-hard problem. Furthermore, the complexity of scheduling DNN computational graphs will further increase on pipelined multi-core systems considering memory communication cost, as well as the increasing size of DNNs. Using the synthetic graph for the training dataset, this work presents a reinforcement learning (RL) based scheduling framework RESPECT, which learns the behaviors of optimal optimization algorithms and generates near-optimal scheduling results with short solving runtime overhead. Our framework has demonstrated up to $\\sim2.5\\times$ real-world on-chip inference runtime speedups over the commercial compiler with ten popular ImageNet models deployed on the physical Coral Edge TPUs system. Moreover, compared to the exact optimization methods, the proposed RL scheduling improves the scheduling optimization runtime by up to 683$\\times$ speedups compared to the commercial compiler and matches the exact optimal solutions with up to 930$\\times$ speedups. Finally, we perform a comprehensive generalizability test, which demonstrates RESPECT successfully imitates optimal solving behaviors from small synthetic graphs to large real-world DNNs computational graphs.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.04716",
        "string": "[RESPECT: Reinforcement Learning based Edge Scheduling on Pipelined Coral Edge TPUs](https://arxiv.org/pdf/2304.04716)"
    },
    "RRHF: Rank Responses to Align Language Models with Human Feedback without tears": {
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) facilitates the alignment of large language models with human preferences, significantly enhancing the quality of interactions between humans and these models. InstructGPT implements RLHF through several stages, including Supervised Fine-Tuning (SFT), reward model training, and Proximal Policy Optimization (PPO). PPO, however, is sensitive to hyperparameters and requires a minimum of four models in its standard implementation, which makes it hard to train. In contrast, we propose a novel learning paradigm called RRHF, which scores responses generated by different sampling policies and learns to align them with human preferences through ranking loss. RRHF can efficiently align language model output probabilities with human preferences as robust as fine-tuning and it only needs 1 to 2 models during tuning. In addition, RRHF can be considered an extension of SFT and reward models while being simpler than PPO in terms of coding, model counts, and hyperparameters. The entire alignment process can be accomplished within a single RRHF training session. We evaluate RRHF using LLaMA and Alpaca on Helpful and Harmless data, demonstrating performance comparable to PPO.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.05302",
        "string": "[RRHF: Rank Responses to Align Language Models with Human Feedback without tears](https://arxiv.org/pdf/2304.05302)"
    },
    "Real-Time Model-Free Deep Reinforcement Learning for Force Control of a Series Elastic Actuator": {
        "abstract": "Many state-of-the art robotic applications utilize series elastic actuators (SEAs) with closed-loop force control to achieve complex tasks such as walking, lifting, and manipulation. Model-free PID control methods are more prone to instability due to nonlinearities in the SEA where cascaded model-based robust controllers can remove these effects to achieve stable force control. However, these model-based methods require detailed investigations to characterize the system accurately. Deep reinforcement learning (DRL) has proved to be an effective model-free method for continuous control tasks, where few works deal with hardware learning. This paper describes the training process of a DRL policy on hardware of an SEA pendulum system for tracking force control trajectories from 0.05 - 0.35 Hz at 50 N amplitude using the Proximal Policy Optimization (PPO) algorithm. Safety mechanisms are developed and utilized for training the policy for 12 hours (overnight) without an operator present within the full 21 hours training period. The tracking performance is evaluated showing improvements of $25$ N in mean absolute error when comparing the first 18 min. of training to the full 21 hours for a 50 N amplitude, 0.1 Hz sinusoid desired force trajectory. Finally, the DRL policy exhibits better tracking and stability margins when compared to a model-free PID controller for a 50 N chirp force trajectory.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.04911",
        "string": "[Real-Time Model-Free Deep Reinforcement Learning for Force Control of a Series Elastic Actuator](https://arxiv.org/pdf/2304.04911)"
    },
    "Reinforcement Learning Based Minimum State-flipped Control for the Reachability of Boolean Control Networks": {
        "abstract": "To realize reachability as well as reduce control costs of Boolean Control Networks (BCNs) with state-flipped control, a reinforcement learning based method is proposed to obtain flip kernels and the optimal policy with minimal flipping actions to realize reachability. The method proposed is model-free and of low computational complexity. In particular, Q-learning (QL), fast QL, and small memory QL are proposed to find flip kernels. Fast QL and small memory QL are two novel algorithms. Specifically, fast QL, namely, QL combined with transfer-learning and special initial states, is of higher efficiency, and small memory QL is applicable to large-scale systems. Meanwhile, we present a novel reward setting, under which the optimal policy with minimal flipping actions to realize reachability is the one of the highest returns. Then, to obtain the optimal policy, we propose QL, and fast small memory QL for large-scale systems. Specifically, on the basis of the small memory QL mentioned before, the fast small memory QL uses a changeable reward setting to speed up the learning efficiency while ensuring the optimality of the policy. For parameter settings, we give some system properties for reference. Finally, two examples, which are a small-scale system and a large-scale one, are considered to verify the proposed method.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.04950",
        "string": "[Reinforcement Learning Based Minimum State-flipped Control for the Reachability of Boolean Control Networks](https://arxiv.org/pdf/2304.04950)"
    },
    "Reinforcement Learning Quantum Local Search": {
        "abstract": "Quantum Local Search (QLS) is a promising approach that employs small-scale quantum computers to tackle large combinatorial optimization problems through local search on quantum hardware, starting from an initial point. However, the random selection of the sub-problem to solve in QLS may not be efficient. In this study, we propose a reinforcement learning (RL) based approach to train an agent for improved subproblem selection in QLS, beyond random selection. Our results demonstrate that the RL agent effectively enhances the average approximation ratio of QLS on fully-connected random Ising problems, indicating the potential of combining RL techniques with Noisy Intermediate-scale Quantum (NISQ) algorithms. This research opens a promising direction for integrating RL into quantum computing to enhance the performance of optimization tasks.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.06473",
        "string": "[Reinforcement Learning Quantum Local Search](https://arxiv.org/pdf/2304.06473)"
    },
    "Reinforcement Learning Tutor Better Supported Lower Performers in a Math Task": {
        "abstract": "Resource limitations make it hard to provide all students with one of the most effective educational interventions: personalized instruction. Reinforcement learning could be a key tool to reduce the development cost and improve the effectiveness of intelligent tutoring software that aims to provide the right support, at the right time, to a student. Here we illustrate that deep reinforcement learning can be used to provide adaptive pedagogical support to students learning about the concept of volume in a narrative storyline software. Using explainable artificial intelligence tools, we extracted interpretable insights about the pedagogical policy learned and demonstrated that the resulting policy had similar performance in a different student population. Most importantly, in both studies, the reinforcement-learning narrative system had the largest benefit for those students with the lowest initial pretest scores, suggesting the opportunity for AI to adapt and provide support for those most in need.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.04933",
        "string": "[Reinforcement Learning Tutor Better Supported Lower Performers in a Math Task](https://arxiv.org/pdf/2304.04933)"
    },
    "Reinforcement Learning from Passive Data via Latent Intentions": {
        "abstract": "Passive observational data, such as human videos, is abundant and rich in information, yet remains largely untapped by current RL methods. Perhaps surprisingly, we show that passive data, despite not having reward or action labels, can still be used to learn features that accelerate downstream RL. Our approach learns from passive data by modeling intentions: measuring how the likelihood of future outcomes change when the agent acts to achieve a particular task. We propose a temporal difference learning objective to learn about intentions, resulting in an algorithm similar to conventional RL, but which learns entirely from passive data. When optimizing this objective, our agent simultaneously learns representations of states, of policies, and of possible outcomes in an environment, all from raw observational data. Both theoretically and empirically, this scheme learns features amenable for value prediction for downstream tasks, and our experiments demonstrate the ability to learn from many forms of passive data, including cross-embodiment video data and YouTube videos.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.04782",
        "string": "[Reinforcement Learning from Passive Data via Latent Intentions](https://arxiv.org/pdf/2304.04782)"
    },
    "Reinforcement Learning-Based Black-Box Model Inversion Attacks": {
        "abstract": "Model inversion attacks are a type of privacy attack that reconstructs private data used to train a machine learning model, solely by accessing the model. Recently, white-box model inversion attacks leveraging Generative Adversarial Networks (GANs) to distill knowledge from public datasets have been receiving great attention because of their excellent attack performance. On the other hand, current black-box model inversion attacks that utilize GANs suffer from issues such as being unable to guarantee the completion of the attack process within a predetermined number of query accesses or achieve the same level of performance as white-box attacks. To overcome these limitations, we propose a reinforcement learning-based black-box model inversion attack. We formulate the latent space search as a Markov Decision Process (MDP) problem and solve it with reinforcement learning. Our method utilizes the confidence scores of the generated images to provide rewards to an agent. Finally, the private data can be reconstructed using the latent vectors found by the agent trained in the MDP. The experiment results on various datasets and models demonstrate that our attack successfully recovers the private information of the target model by achieving state-of-the-art attack performance. We emphasize the importance of studies on privacy-preserving machine learning by proposing a more advanced black-box model inversion attack.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.04625",
        "string": "[Reinforcement Learning-Based Black-Box Model Inversion Attacks](https://arxiv.org/pdf/2304.04625)"
    },
    "Representation Learning with Multi-Step Inverse Kinematics: An Efficient and Optimal Approach to Rich-Observation RL": {
        "abstract": "We study the design of sample-efficient algorithms for reinforcement learning in the presence of rich, high-dimensional observations, formalized via the Block MDP problem. Existing algorithms suffer from either 1) computational intractability, 2) strong statistical assumptions that are not necessarily satisfied in practice, or 3) suboptimal sample complexity. We address these issues by providing the first computationally efficient algorithm that attains rate-optimal sample complexity with respect to the desired accuracy level, with minimal statistical assumptions. Our algorithm, MusIK, combines systematic exploration with representation learning based on multi-step inverse kinematics, a learning objective in which the aim is to predict the learner's own action from the current observation and observations in the (potentially distant) future. MusIK is simple and flexible, and can efficiently take advantage of general-purpose function approximation. Our analysis leverages several new techniques tailored to non-optimistic exploration algorithms, which we anticipate will find broader use.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.05889",
        "string": "[Representation Learning with Multi-Step Inverse Kinematics: An Efficient and Optimal Approach to Rich-Observation RL](https://arxiv.org/pdf/2304.05889)"
    },
    "Robust Body Exposure (RoBE): A Graph-based Dynamics Modeling Approach to Manipulating Blankets over People": {
        "abstract": "Robotic caregivers could potentially improve the quality of life of many who require physical assistance. However, in order to assist individuals who are lying in bed, robots must be capable of dealing with a significant obstacle: the blanket or sheet that will almost always cover the person's body. We propose a method for targeted bedding manipulation over people lying supine in bed where we first learn a model of the cloth's dynamics. Then, we optimize over this model to uncover a given target limb using information about human body shape and pose that only needs to be provided at run-time. We show how this approach enables greater robustness to variation relative to geometric and reinforcement learning baselines via a number of generalization evaluations in simulation and in the real world. We further evaluate our approach in a human study with 12 participants where we demonstrate that a mobile manipulator can adapt to real variation in human body shape, size, pose, and blanket configuration to uncover target body parts without exposing the rest of the body. Source code and supplementary materials are available online.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.04822",
        "string": "[Robust Body Exposure (RoBE): A Graph-based Dynamics Modeling Approach to Manipulating Blankets over People](https://arxiv.org/pdf/2304.04822)"
    },
    "Robust Decision-Making in Spatial Learning: A Comparative Study of Successor Features and Predecessor Features Algorithms": {
        "abstract": "Predictive map theory, one of the theories explaining spatial learning in animals, is based on successor representation (SR) learning algorithms. In the real world, agents such as animals and robots are subjected to noisy observations, which can lead to suboptimal actions or even failure during learning. In this study, we compared the performance of Successor Features (SFs) and Predecessor Features (PFs) algorithms in a noisy one-dimensional maze environment. Our results demonstrated that PFs consistently outperformed SFs in terms of cumulative reward and average step length, with higher resilience to noise. This superiority could be due to PFs' ability to transmit temporal difference errors to more preceding states. We also discuss the biological mechanisms involved in PFs learning for spatial navigation. This study contributes to the theoretical research on computational neuroscience using reinforcement learning algorithms, and highlights the practical potential of PFs in robotics, game AI, and autonomous vehicle navigation.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.06894",
        "string": "[Robust Decision-Making in Spatial Learning: A Comparative Study of Successor Features and Predecessor Features Algorithms](https://arxiv.org/pdf/2304.06894)"
    },
    "STAS: Spatial-Temporal Return Decomposition for Multi-agent Reinforcement Learning": {
        "abstract": "Centralized Training with Decentralized Execution (CTDE) has been proven to be an effective paradigm in cooperative multi-agent reinforcement learning (MARL). One of the major challenges is yet credit assignment, which aims to credit agents by their contributions. Prior studies focus on either implicitly decomposing the joint value function or explicitly computing the payoff distribution of all agents. However, in episodic reinforcement learning settings where global rewards can only be revealed at the end of the episode, existing methods usually fail to work. They lack the functionality of modeling complicated relations of the delayed global reward in the temporal dimension and suffer from large variance and bias. We propose a novel method named Spatial-Temporal Attention with Shapley (STAS) for return decomposition; STAS learns credit assignment in both the temporal and the spatial dimension. It first decomposes the global return back to each time step, then utilizes Shapley Value to redistribute the individual payoff from the decomposed global reward. To mitigate the computational complexity of Shapley Value, we introduce an approximation of marginal contribution and utilize Monte Carlo sampling to estimate Shapley Value. We evaluate our method on the classical Alice & Bob example and Multi-agent Particle Environments benchmarks across different scenarios, and we show our methods achieve an effective spatial-temporal credit assignment and outperform all state-of-art baselines.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.07520",
        "string": "[STAS: Spatial-Temporal Return Decomposition for Multi-agent Reinforcement Learning](https://arxiv.org/pdf/2304.07520)"
    },
    "Spectrum-aware Multi-hop Task Routing in Vehicle-assisted Collaborative Edge Computing": {
        "abstract": "Multi-access edge computing (MEC) is a promising technology to enhance the quality of service, particularly for low-latency services, by enabling computing offloading to edge servers (ESs) in close proximity. To avoid network congestion, collaborative edge computing has become an emerging paradigm to enable different ESs to collaboratively share their data and computation resources. However, most papers in collaborative edge computing only allow one-hop offloading, which may limit computing resource sharing due to either poor channel conditions or computing workload at ESs one-hop away. By allowing ESs multi-hop away to also share the computing workload, a multi-hop MEC enables more ESs to share their computing resources. Inspired by this observation, in this paper, we propose to leverage omnipresent vehicles in a city to form a data transportation network for task delivery in a multi-hop fashion. Here, we propose a general multi-hop task offloading framework for vehicle-assisted MEC where tasks from users can be offloaded to powerful ESs via potentially multi-hop transmissions. Under the proposed framework, we develop a reinforcement learning based task offloading approach to address the curse of dimensionality problem due to vehicular mobility and channel variability, with the goal to maximize the aggregated service throughput under constraints on end-to-end latency, spectrum, and computing resources. Numerical results demonstrate that the proposed algorithm achieves excellent performance with low complexity and outperforms existing benchmark schemes.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.07422",
        "string": "[Spectrum-aware Multi-hop Task Routing in Vehicle-assisted Collaborative Edge Computing](https://arxiv.org/pdf/2304.07422)"
    },
    "Towards Controllable Diffusion Models via Reward-Guided Exploration": {
        "abstract": "By formulating data samples' formation as a Markov denoising process, diffusion models achieve state-of-the-art performances in a collection of tasks. Recently, many variants of diffusion models have been proposed to enable controlled sample generation. Most of these existing methods either formulate the controlling information as an input (i.e.,: conditional representation) for the noise approximator, or introduce a pre-trained classifier in the test-phase to guide the Langevin dynamic towards the conditional goal. However, the former line of methods only work when the controlling information can be formulated as conditional representations, while the latter requires the pre-trained guidance classifier to be differentiable. In this paper, we propose a novel framework named RGDM (Reward-Guided Diffusion Model) that guides the training-phase of diffusion models via reinforcement learning (RL). The proposed training framework bridges the objective of weighted log-likelihood and maximum entropy RL, which enables calculating policy gradients via samples from a pay-off distribution proportional to exponential scaled rewards, rather than from policies themselves. Such a framework alleviates the high gradient variances and enables diffusion models to explore for highly rewarded samples in the reverse process. Experiments on 3D shape and molecule generation tasks show significant improvements over existing conditional diffusion models.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.07132",
        "string": "[Towards Controllable Diffusion Models via Reward-Guided Exploration](https://arxiv.org/pdf/2304.07132)"
    },
    "Uncertainty-driven Trajectory Truncation for Model-based Offline Reinforcement Learning": {
        "abstract": "Equipped with the trained environmental dynamics, model-based offline reinforcement learning (RL) algorithms can often successfully learn good policies from fixed-sized datasets, even some datasets with poor quality. Unfortunately, however, it can not be guaranteed that the generated samples from the trained dynamics model are reliable (e.g., some synthetic samples may lie outside of the support region of the static dataset). To address this issue, we propose Trajectory Truncation with Uncertainty (TATU), which adaptively truncates the synthetic trajectory if the accumulated uncertainty along the trajectory is too large. We theoretically show the performance bound of TATU to justify its benefits. To empirically show the advantages of TATU, we first combine it with two classical model-based offline RL algorithms, MOPO and COMBO. Furthermore, we integrate TATU with several off-the-shelf model-free offline RL algorithms, e.g., BCQ. Experimental results on the D4RL benchmark show that TATU significantly improves their performance, often by a large margin.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2304.04660",
        "string": "[Uncertainty-driven Trajectory Truncation for Model-based Offline Reinforcement Learning](https://arxiv.org/pdf/2304.04660)"
    }
}
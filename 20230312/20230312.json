{
    "A Framework for History-Aware Hyperparameter Optimisation in Reinforcement Learning": {
        "abstract": "A Reinforcement Learning (RL) system depends on a set of initial conditions (hyperparameters) that affect the system's performance. However, defining a good choice of hyperparameters is a challenging problem.\n  Hyperparameter tuning often requires manual or automated searches to find optimal values. Nonetheless, a noticeable limitation is the high cost of algorithm evaluation for complex models, making the tuning process computationally expensive and time-consuming.\n  In this paper, we propose a framework based on integrating complex event processing and temporal models, to alleviate these trade-offs. Through this combination, it is possible to gain insights about a running RL system efficiently and unobtrusively based on data stream monitoring and to create abstract representations that allow reasoning about the historical behaviour of the RL system. The obtained knowledge is exploited to provide feedback to the RL system for optimising its hyperparameters while making effective use of parallel resources.\n  We introduce a novel history-aware epsilon-greedy logic for hyperparameter optimisation that instead of using static hyperparameters that are kept fixed for the whole training, adjusts the hyperparameters at runtime based on the analysis of the agent's performance over time windows in a single agent's lifetime. We tested the proposed approach in a 5G mobile communications case study that uses DQN, a variant of RL, for its decision-making. Our experiments demonstrated the effects of hyperparameter tuning using history on training stability and reward values. The encouraging results show that the proposed history-aware framework significantly improved performance compared to traditional hyperparameter tuning approaches.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.05186",
        "string": "[A Framework for History-Aware Hyperparameter Optimisation in Reinforcement Learning](https://arxiv.org/pdf/2303.05186)"
    },
    "A Multiplicative Value Function for Safe and Efficient Reinforcement Learning": {
        "abstract": "An emerging field of sequential decision problems is safe Reinforcement Learning (RL), where the objective is to maximize the reward while obeying safety constraints. Being able to handle constraints is essential for deploying RL agents in real-world environments, where constraint violations can harm the agent and the environment. To this end, we propose a safe model-free RL algorithm with a novel multiplicative value function consisting of a safety critic and a reward critic. The safety critic predicts the probability of constraint violation and discounts the reward critic that only estimates constraint-free returns. By splitting responsibilities, we facilitate the learning task leading to increased sample efficiency. We integrate our approach into two popular RL algorithms, Proximal Policy Optimization and Soft Actor-Critic, and evaluate our method in four safety-focused environments, including classical RL benchmarks augmented with safety constraints and robot navigation tasks with images and raw Lidar scans as observations. Finally, we make the zero-shot sim-to-real transfer where a differential drive robot has to navigate through a cluttered room. Our code can be found at https://github.com/nikeke19/Safe-Mult-RL.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.04118",
        "string": "[A Multiplicative Value Function for Safe and Efficient Reinforcement Learning](https://arxiv.org/pdf/2303.04118)"
    },
    "A Unified and Efficient Coordinating Framework for Autonomous DBMS Tuning": {
        "abstract": "Recently using machine learning (ML) based techniques to optimize modern database management systems has attracted intensive interest from both industry and academia. With an objective to tune a specific component of a DBMS (e.g., index selection, knobs tuning), the ML-based tuning agents have shown to be able to find better configurations than experienced database administrators. However, one critical yet challenging question remains unexplored -- how to make those ML-based tuning agents work collaboratively. Existing methods do not consider the dependencies among the multiple agents, and the model used by each agent only studies the effect of changing the configurations in a single component. To tune different components for DBMS, a coordinating mechanism is needed to make the multiple agents cognizant of each other. Also, we need to decide how to allocate the limited tuning budget among the agents to maximize the performance. Such a decision is difficult to make since the distribution of the reward for each agent is unknown and non-stationary. In this paper, we study the above question and present a unified coordinating framework to efficiently utilize existing ML-based agents. First, we propose a message propagation protocol that specifies the collaboration behaviors for agents and encapsulates the global tuning messages in each agent's model. Second, we combine Thompson Sampling, a well-studied reinforcement learning algorithm with a memory buffer so that our framework can allocate budget judiciously in a non-stationary environment. Our framework defines the interfaces adapted to a broad class of ML-based tuning agents, yet simple enough for integration with existing implementations and future extensions. We show that it can effectively utilize different ML-based agents and find better configurations with 1.4~14.1X speedups on the workload execution time compared with baselines.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.05710",
        "string": "[A Unified and Efficient Coordinating Framework for Autonomous DBMS Tuning](https://arxiv.org/pdf/2303.05710)"
    },
    "Beware of Instantaneous Dependence in Reinforcement Learning": {
        "abstract": "Playing an important role in Model-Based Reinforcement Learning (MBRL), environment models aim to predict future states based on the past. Existing works usually ignore instantaneous dependence in the state, that is, assuming that the future state variables are conditionally independent given the past states. However, instantaneous dependence is prevalent in many RL environments. For instance, in the stock market, instantaneous dependence can exist between two stocks because the fluctuation of one stock can quickly affect the other and the resolution of price change is lower than that of the effect. In this paper, we prove that with few exceptions, ignoring instantaneous dependence can result in suboptimal policy learning in MBRL. To address the suboptimality problem, we propose a simple plug-and-play method to enable existing MBRL algorithms to take instantaneous dependence into account. Through experiments on two benchmarks, we (1) confirm the existence of instantaneous dependence with visualization; (2) validate our theoretical findings that ignoring instantaneous dependence leads to suboptimal policy; (3) verify that our method effectively enables reinforcement learning with instantaneous dependence and improves policy performance.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.05458",
        "string": "[Beware of Instantaneous Dependence in Reinforcement Learning](https://arxiv.org/pdf/2303.05458)"
    },
    "Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning": {
        "abstract": "A compelling use case of offline reinforcement learning (RL) is to obtain a policy initialization from existing datasets, which allows efficient fine-tuning with limited amounts of active online interaction. However, several existing offline RL methods tend to exhibit poor online fine-tuning performance. On the other hand, online RL methods can learn effectively through online interaction, but struggle to incorporate offline data, which can make them very slow in settings where exploration is challenging or pre-training is necessary. In this paper, we devise an approach for learning an effective initialization from offline data that also enables fast online fine-tuning capabilities. Our approach, calibrated Q-learning (Cal-QL) accomplishes this by learning a conservative value function initialization that underestimates the value of the learned policy from offline data, while also being calibrated, in the sense that the learned Q-values are at a reasonable scale. We refer to this property as calibration, and define it formally as providing a lower bound on the true value function of the learned policy and an upper bound on the value of some other (suboptimal) reference policy, which may simply be the behavior policy. We show that offline RL algorithms that learn such calibrated value functions lead to effective online fine-tuning, enabling us to take the benefits of offline initializations in online fine-tuning. In practice, Cal-QL can be implemented on top of existing conservative methods for offline RL within a one-line code change. Empirically, Cal-QL outperforms state-of-the-art methods on 10/11 fine-tuning benchmark tasks that we study in this paper.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.05479",
        "string": "[Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning](https://arxiv.org/pdf/2303.05479)"
    },
    "Chance-Aware Lane Change with High-Level Model Predictive Control Through Curriculum Reinforcement Learning": {
        "abstract": "Lane change in dense traffic is considered a challenging problem that typically requires the recognization of an opportune and appropriate time for maneuvers. In this work, we propose a chance-aware lane-change strategy with high-level model predictive control (MPC) through curriculum reinforcement learning (CRL). The embodied high-level MPC in our proposed framework is parameterized with augmented decision variables, where full-state references and regulatory factors concerning their importance are introduced. In this sense, improved adaptiveness to dense and dynamic environments with high complexity is exhibited. Furthermore, to improve the convergence speed and ensure a high-quality policy, effective curriculum design is integrated into the reinforcement learning (RL) framework with policy transfer and enhancement. With comprehensive experiments towards the chance-aware lane-change scenario, accelerated convergence speed and improved reward performance are demonstrated through comparisons with representative baseline methods. It is noteworthy that, given a narrow chance in the dense and dynamic traffic flow, the proposed approach generates high-quality lane-change maneuvers such that the vehicle merges into the traffic flow with a high success rate.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.03723",
        "string": "[Chance-Aware Lane Change with High-Level Model Predictive Control Through Curriculum Reinforcement Learning](https://arxiv.org/pdf/2303.03723)"
    },
    "Cherry-Picking with Reinforcement Learning": {
        "abstract": "Grasping small objects surrounded by unstable or non-rigid material plays a crucial role in applications such as surgery, harvesting, construction, disaster recovery, and assisted feeding. This task is especially difficult when fine manipulation is required in the presence of sensor noise and perception errors; this inevitably triggers dynamic motion, which is challenging to model precisely. Circumventing the difficulty to build accurate models for contacts and dynamics, data-driven methods like reinforcement learning (RL) can optimize task performance via trial and error. Applying these methods to real robots, however, has been hindered by factors such as prohibitively high sample complexity or the high training infrastructure cost for providing resets on hardware. This work presents CherryBot, an RL system that uses chopsticks for fine manipulation that surpasses human reactiveness for some dynamic grasping tasks. By carefully designing the training paradigm and algorithm, we study how to make a real-world robot learning system sample efficient and general while reducing the human effort required for supervision. Our system shows continual improvement through 30 minutes of real-world interaction: through reactive retry, it achieves an almost 100% success rate on the demanding task of using chopsticks to grasp small objects swinging in the air. We demonstrate the reactiveness, robustness and generalizability of CherryBot to varying object shapes and dynamics (e.g., external disturbances like wind and human perturbations). Videos are available at https://goodcherrybot.github.io/.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.05508",
        "string": "[Cherry-Picking with Reinforcement Learning](https://arxiv.org/pdf/2303.05508)"
    },
    "Computably Continuous Reinforcement-Learning Objectives are PAC-learnable": {
        "abstract": "In reinforcement learning, the classic objectives of maximizing discounted and finite-horizon cumulative rewards are PAC-learnable: There are algorithms that learn a near-optimal policy with high probability using a finite amount of samples and computation. In recent years, researchers have introduced objectives and corresponding reinforcement-learning algorithms beyond the classic cumulative rewards, such as objectives specified as linear temporal logic formulas. However, questions about the PAC-learnability of these new objectives have remained open.\n  This work demonstrates the PAC-learnability of general reinforcement-learning objectives through sufficient conditions for PAC-learnability in two analysis settings. In particular, for the analysis that considers only sample complexity, we prove that if an objective given as an oracle is uniformly continuous, then it is PAC-learnable. Further, for the analysis that considers computational complexity, we prove that if an objective is computable, then it is PAC-learnable. In other words, if a procedure computes successive approximations of the objective's value, then the objective is PAC-learnable.\n  We give three applications of our condition on objectives from the literature with previously unknown PAC-learnability and prove that these objectives are PAC-learnable. Overall, our result helps verify existing objectives' PAC-learnability. Also, as some studied objectives that are not uniformly continuous have been shown to be not PAC-learnable, our results could guide the design of new PAC-learnable objectives.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.05518",
        "string": "[Computably Continuous Reinforcement-Learning Objectives are PAC-learnable](https://arxiv.org/pdf/2303.05518)"
    },
    "ConBaT: Control Barrier Transformer for Safe Policy Learning": {
        "abstract": "Large-scale self-supervised models have recently revolutionized our ability to perform a variety of tasks within the vision and language domains. However, using such models for autonomous systems is challenging because of safety requirements: besides executing correct actions, an autonomous agent must also avoid the high cost and potentially fatal critical mistakes. Traditionally, self-supervised training mainly focuses on imitating previously observed behaviors, and the training demonstrations carry no notion of which behaviors should be explicitly avoided. In this work, we propose Control Barrier Transformer (ConBaT), an approach that learns safe behaviors from demonstrations in a self-supervised fashion. ConBaT is inspired by the concept of control barrier functions in control theory and uses a causal transformer that learns to predict safe robot actions autoregressively using a critic that requires minimal safety data labeling. During deployment, we employ a lightweight online optimization to find actions that ensure future states lie within the learned safe set. We apply our approach to different simulated control tasks and show that our method results in safer control policies compared to other classical and learning-based methods such as imitation learning, reinforcement learning, and model predictive control.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.04212",
        "string": "[ConBaT: Control Barrier Transformer for Safe Policy Learning](https://arxiv.org/pdf/2303.04212)"
    },
    "Conceptual Reinforcement Learning for Language-Conditioned Tasks": {
        "abstract": "Despite the broad application of deep reinforcement learning (RL), transferring and adapting the policy to unseen but similar environments is still a significant challenge. Recently, the language-conditioned policy is proposed to facilitate policy transfer through learning the joint representation of observation and text that catches the compact and invariant information across environments. Existing studies of language-conditioned RL methods often learn the joint representation as a simple latent layer for the given instances (episode-specific observation and text), which inevitably includes noisy or irrelevant information and cause spurious correlations that are dependent on instances, thus hurting generalization performance and training efficiency. To address this issue, we propose a conceptual reinforcement learning (CRL) framework to learn the concept-like joint representation for language-conditioned policy. The key insight is that concepts are compact and invariant representations in human cognition through extracting similarities from numerous instances in real-world. In CRL, we propose a multi-level attention encoder and two mutual information constraints for learning compact and invariant concepts. Verified in two challenging environments, RTFM and Messenger, CRL significantly improves the training efficiency (up to 70%) and generalization ability (up to 30%) to the new environment dynamics.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.05069",
        "string": "[Conceptual Reinforcement Learning for Language-Conditioned Tasks](https://arxiv.org/pdf/2303.05069)"
    },
    "Constrained Reinforcement Learning and Formal Verification for Safe Colonoscopy Navigation": {
        "abstract": "The field of robotic Flexible Endoscopes (FEs) has progressed significantly, offering a promising solution to reduce patient discomfort. However, the limited autonomy of most robotic FEs results in non-intuitive and challenging manoeuvres, constraining their application in clinical settings. While previous studies have employed lumen tracking for autonomous navigation, they fail to adapt to the presence of obstructions and sharp turns when the endoscope faces the colon wall. In this work, we propose a Deep Reinforcement Learning (DRL)-based navigation strategy that eliminates the need for lumen tracking. However, the use of DRL methods poses safety risks as they do not account for potential hazards associated with the actions taken. To ensure safety, we exploit a Constrained Reinforcement Learning (CRL) method to restrict the policy in a predefined safety regime. Moreover, we present a model selection strategy that utilises Formal Verification (FV) to choose a policy that is entirely safe before deployment. We validate our approach in a virtual colonoscopy environment and report that out of the 300 trained policies, we could identify three policies that are entirely safe. Our work demonstrates that CRL, combined with model selection through FV, can improve the robustness and safety of robotic behaviour in surgical applications.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.03207",
        "string": "[Constrained Reinforcement Learning and Formal Verification for Safe Colonoscopy Navigation](https://arxiv.org/pdf/2303.03207)"
    },
    "Controlled Diversity with Preference : Towards Learning a Diverse Set of Desired Skills": {
        "abstract": "Autonomously learning diverse behaviors without an extrinsic reward signal has been a problem of interest in reinforcement learning. However, the nature of learning in such mechanisms is unconstrained, often resulting in the accumulation of several unusable, unsafe or misaligned skills. In order to avoid such issues and ensure the discovery of safe and human-aligned skills, it is necessary to incorporate humans into the unsupervised training process, which remains a largely unexplored research area. In this work, we propose Controlled Diversity with Preference (CDP), a novel, collaborative human-guided mechanism for an agent to learn a set of skills that is diverse as well as desirable. The key principle is to restrict the discovery of skills to those regions that are deemed to be desirable as per a preference model trained using human preference labels on trajectory pairs. We evaluate our approach on 2D navigation and Mujoco environments and demonstrate the ability to discover diverse, yet desirable skills.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.04592",
        "string": "[Controlled Diversity with Preference : Towards Learning a Diverse Set of Desired Skills](https://arxiv.org/pdf/2303.04592)"
    },
    "Decision-Making Under Uncertainty: Beyond Probabilities": {
        "abstract": "This position paper reflects on the state-of-the-art in decision-making under uncertainty. A classical assumption is that probabilities can sufficiently capture all uncertainty in a system. In this paper, the focus is on the uncertainty that goes beyond this classical interpretation, particularly by employing a clear distinction between aleatoric and epistemic uncertainty. The paper features an overview of Markov decision processes (MDPs) and extensions to account for partial observability and adversarial behavior. These models sufficiently capture aleatoric uncertainty but fail to account for epistemic uncertainty robustly. Consequently, we present a thorough overview of so-called uncertainty models that exhibit uncertainty in a more robust interpretation. We show several solution techniques for both discrete and continuous models, ranging from formal verification, over control-based abstractions, to reinforcement learning. As an integral part of this paper, we list and discuss several key challenges that arise when dealing with rich types of uncertainty in a model-based fashion.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.05848",
        "string": "[Decision-Making Under Uncertainty: Beyond Probabilities](https://arxiv.org/pdf/2303.05848)"
    },
    "Decoupling Skill Learning from Robotic Control for Generalizable Object Manipulation": {
        "abstract": "Recent works in robotic manipulation through reinforcement learning (RL) or imitation learning (IL) have shown potential for tackling a range of tasks e.g., opening a drawer or a cupboard. However, these techniques generalize poorly to unseen objects. We conjecture that this is due to the high-dimensional action space for joint control. In this paper, we take an alternative approach and separate the task of learning 'what to do' from 'how to do it' i.e., whole-body control. We pose the RL problem as one of determining the skill dynamics for a disembodied virtual manipulator interacting with articulated objects. The whole-body robotic kinematic control is optimized to execute the high-dimensional joint motion to reach the goals in the workspace. It does so by solving a quadratic programming (QP) model with robotic singularity and kinematic constraints. Our experiments on manipulating complex articulated objects show that the proposed approach is more generalizable to unseen objects with large intra-class variations, outperforming previous approaches. The evaluation results indicate that our approach generates more compliant robotic motion and outperforms the pure RL and IL baselines in task success rates. Additional information and videos are available at https://kl-research.github.io/decoupskill\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.04016",
        "string": "[Decoupling Skill Learning from Robotic Control for Generalizable Object Manipulation](https://arxiv.org/pdf/2303.04016)"
    },
    "Deep Occupancy-Predictive Representations for Autonomous Driving": {
        "abstract": "Manually specifying features that capture the diversity in traffic environments is impractical. Consequently, learning-based agents cannot realize their full potential as neural motion planners for autonomous vehicles. Instead, this work proposes to learn which features are task-relevant. Given its immediate relevance to motion planning, our proposed architecture encodes the probabilistic occupancy map as a proxy for obtaining pre-trained state representations. By leveraging a map-aware graph formulation of the environment, our agent-centric encoder generalizes to arbitrary road networks and traffic situations. We show that our approach significantly improves the downstream performance of a reinforcement learning agent operating in urban traffic environments.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.04218",
        "string": "[Deep Occupancy-Predictive Representations for Autonomous Driving](https://arxiv.org/pdf/2303.04218)"
    },
    "Deep Reinforcement Learning Based Power Allocation for Minimizing AoI and Energy Consumption in MIMO-NOMA IoT Systems": {
        "abstract": "Multi-input multi-out and non-orthogonal multiple access (MIMO-NOMA) internet-of-things (IoT) systems can improve channel capacity and spectrum efficiency distinctly to support the real-time applications. Age of information (AoI) is an important metric for real-time application, but there is no literature have minimized AoI of the MIMO-NOMA IoT system, which motivates us to conduct this work. In MIMO-NOMA IoT system, the base station (BS) determines the sample collection requirements and allocates the transmission power for each IoT device. Each device determines whether to sample data according to the sample collection requirements and adopts the allocated power to transmit the sampled data to the BS over MIMO-NOMA channel. Afterwards, the BS employs successive interference cancelation (SIC) technique to decode the signal of the data transmitted by each device. The sample collection requirements and power allocation would affect AoI and energy consumption of the system. It is critical to determine the optimal policy including sample collection requirements and power allocation to minimize the AoI and energy consumption of MIMO-NOMA IoT system, where the transmission rate is not a constant in the SIC process and the noise is stochastic in the MIMO-NOMA channel. In this paper, we propose the optimal power allocation to minimize the AoI and energy consumption of MIMO- NOMA IoT system based on deep reinforcement learning (DRL). Extensive simulations are carried out to demonstrate the superiority of the optimal power allocation.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.06411",
        "string": "[Deep Reinforcement Learning Based Power Allocation for Minimizing AoI and Energy Consumption in MIMO-NOMA IoT Systems](https://arxiv.org/pdf/2303.06411)"
    },
    "Deep Reinforcement Learning for Beam Angle Optimization of Intensity-Modulated Radiation Therapy": {
        "abstract": "Objective: Intensity-modulated radiation therapy (IMRT) beam angle optimization (BAO) is a challenging combinatorial optimization problem that is NP-hard. In this study, we aim to develop a personalized BAO algorithm for IMRT that improves the quality of the final treatment. Methods: To improve the quality of IMRT treatment planning, we propose a deep reinforcement learning (DRL)-based approach for IMRT BAO. We consider the task as a sequential decision-making problem and formulate it as a Markov Decision Process. To facilitate the training process, a 3D-Unet is designed to predict the dose distribution for the different number of beam angles, ranging from 1 to 9, to simulate the IMRT environment. By leveraging the simulation model, double deep-Q network (DDQN) and proximal policy optimization (PPO) are used to train agents to select the personalized beam angle sequentially within a few seconds. Results: The treatment plans with beam angles selected by DRL outperform those with clinically used evenly distributed beam angles. For DDQN, the overall average improvement of the CIs is 0.027, 0.032, and 0.03 for 5, 7, and 9 beam angles respectively. For PPO, the overall average improvement of CIs is 0.045, 0.051, and 0.025 for 5, 7, and 9 beam angles respectively. Conclusion: The proposed DRL-based beam angle selection strategy can generate personalized beam angles within a few seconds, and the resulting treatment plan is superior to that obtained using evenly distributed angles. Significance: A fast and automated personalized beam angle selection approach is been proposed for IMRT BAO.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.03812",
        "string": "[Deep Reinforcement Learning for Beam Angle Optimization of Intensity-Modulated Radiation Therapy](https://arxiv.org/pdf/2303.03812)"
    },
    "Deep symbolic regression for physics guided by units constraints: toward the automated discovery of physical laws": {
        "abstract": "Symbolic Regression is the study of algorithms that automate the search for analytic expressions that fit data. While recent advances in deep learning have generated renewed interest in such approaches, efforts have not been focused on physics, where we have important additional constraints due to the units associated with our data. Here we present $\u03a6$-SO, a Physical Symbolic Optimization framework for recovering analytical symbolic expressions from physics data using deep reinforcement learning techniques by learning units constraints. Our system is built, from the ground up, to propose solutions where the physical units are consistent by construction. This is useful not only in eliminating physically impossible solutions, but because it restricts enormously the freedom of the equation generator, thus vastly improving performance. The algorithm can be used to fit noiseless data, which can be useful for instance when attempting to derive an analytical property of a physical model, and it can also be used to obtain analytical approximations to noisy data. We showcase our machinery on a panel of examples from astrophysics.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.03192",
        "string": "[Deep symbolic regression for physics guided by units constraints: toward the automated discovery of physical laws](https://arxiv.org/pdf/2303.03192)"
    },
    "Dextrous Tactile In-Hand Manipulation Using a Modular Reinforcement Learning Architecture": {
        "abstract": "Dextrous in-hand manipulation with a multi-fingered robotic hand is a challenging task, esp. when performed with the hand oriented upside down, demanding permanent force-closure, and when no external sensors are used. For the task of reorienting an object to a given goal orientation (vs. infinitely spinning it around an axis), the lack of external sensors is an additional fundamental challenge as the state of the object has to be estimated all the time, e.g., to detect when the goal is reached. In this paper, we show that the task of reorienting a cube to any of the 24 possible goal orientations in a $\u03c0$/2-raster using the torque-controlled DLR-Hand II is possible. The task is learned in simulation using a modular deep reinforcement learning architecture: the actual policy has only a small observation time window of 0.5s but gets the cube state as an explicit input which is estimated via a deep differentiable particle filter trained on data generated by running the policy. In simulation, we reach a success rate of 92% while applying significant domain randomization. Via zero-shot Sim2Real-transfer on the real robotic system, all 24 goal orientations can be reached with a high success rate.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.04705",
        "string": "[Dextrous Tactile In-Hand Manipulation Using a Modular Reinforcement Learning Architecture](https://arxiv.org/pdf/2303.04705)"
    },
    "Diminishing Return of Value Expansion Methods in Model-Based Reinforcement Learning": {
        "abstract": "Model-based reinforcement learning is one approach to increase sample efficiency. However, the accuracy of the dynamics model and the resulting compounding error over modelled trajectories are commonly regarded as key limitations. A natural question to ask is: How much more sample efficiency can be gained by improving the learned dynamics models? Our paper empirically answers this question for the class of model-based value expansion methods in continuous control problems. Value expansion methods should benefit from increased model accuracy by enabling longer rollout horizons and better value function approximations. Our empirical study, which leverages oracle dynamics models to avoid compounding model errors, shows that (1) longer horizons increase sample efficiency, but the gain in improvement decreases with each additional expansion step, and (2) the increased model accuracy only marginally increases the sample efficiency compared to learned models with identical horizons. Therefore, longer horizons and increased model accuracy yield diminishing returns in terms of sample efficiency. These improvements in sample efficiency are particularly disappointing when compared to model-free value expansion methods. Even though they introduce no computational overhead, we find their performance to be on-par with model-based value expansion methods. Therefore, we conclude that the limitation of model-based value expansion methods is not the model accuracy of the learned models. While higher model accuracy is beneficial, our experiments show that even a perfect model will not provide an un-rivalled sample efficiency but that the bottleneck lies elsewhere.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.03955",
        "string": "[Diminishing Return of Value Expansion Methods in Model-Based Reinforcement Learning](https://arxiv.org/pdf/2303.03955)"
    },
    "Domain Randomization for Robust, Affordable and Effective Closed-loop Control of Soft Robots": {
        "abstract": "Soft robots are becoming extremely popular thanks to their intrinsic safety to contacts and adaptability. However, the potentially infinite number of Degrees of Freedom makes their modeling a daunting task, and in many cases only an approximated description is available. This challenge makes reinforcement learning (RL) based approaches inefficient when deployed on a realistic scenario, due to the large domain gap between models and the real platform. In this work, we demonstrate, for the first time, how Domain Randomization (DR) can solve this problem by enhancing RL policies with: i) a higher robustness w.r.t. environmental changes; ii) a higher affordability of learned policies when the target model differs significantly from the training model; iii) a higher effectiveness of the policy, which can even autonomously learn to exploit the environment to increase the robot capabilities (environmental constraints exploitation). Moreover, we introduce a novel algorithmic extension of previous adaptive domain randomization methods for the automatic inference of dynamics parameters for deformable objects. We provide results on four different tasks and two soft robot designs, opening interesting perspectives for future research on Reinforcement Learning for closed-loop soft robot control.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.04136",
        "string": "[Domain Randomization for Robust, Affordable and Effective Closed-loop Control of Soft Robots](https://arxiv.org/pdf/2303.04136)"
    },
    "Dual-Attention Deep Reinforcement Learning for Multi-MAP 3D Trajectory Optimization in Dynamic 5G Networks": {
        "abstract": "5G and beyond networks need to provide dynamic and efficient infrastructure management to better adapt to time-varying user behaviors (e.g., user mobility, interference, user traffic and evolution of the network topology). In this paper, we propose to manage the trajectory of Mobile Access Points (MAPs) under all these dynamic constraints with reduced complexity. We first formulate the placement problem to manage MAPs over time. Our solution addresses time-varying user traffic and user mobility through a Multi-Agent Deep Reinforcement Learning (MADRL). To achieve real-time behavior, the proposed solution learns to perform distributed assignment of MAP-user positions and schedules the MAP path among all users without centralized user's clustering feedback. Our solution exploits a dual-attention MADRL model via proximal policy optimization to dynamically move MAPs in 3D. The dual-attention takes into account information from both users and MAPs. The cooperation mechanism of our solution allows to manage different scenarios, without a priory information and without re-training, which significantly reduces complexity.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.05233",
        "string": "[Dual-Attention Deep Reinforcement Learning for Multi-MAP 3D Trajectory Optimization in Dynamic 5G Networks](https://arxiv.org/pdf/2303.05233)"
    },
    "ENTROPY: Environment Transformer and Offline Policy Optimization": {
        "abstract": "Model-based methods provide an effective approach to offline reinforcement learning (RL). They learn an environmental dynamics model from interaction experiences and then perform policy optimization based on the learned model. However, previous model-based offline RL methods lack long-term prediction capability, resulting in large errors when generating multi-step trajectories. We address this issue by developing a sequence modeling architecture, Environment Transformer, which can generate reliable long-horizon trajectories based on offline datasets. We then propose a novel model-based offline RL algorithm, ENTROPY, that learns the dynamics model and reward function by ENvironment TRansformer and performs Offline PolicY optimization. We evaluate the proposed method on MuJoCo continuous control RL environments. Results show that ENTROPY performs comparably or better than the state-of-the-art model-based and model-free offline RL methods and demonstrates more powerful long-term trajectory prediction capability compared to existing model-based offline methods.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.03811",
        "string": "[ENTROPY: Environment Transformer and Offline Policy Optimization](https://arxiv.org/pdf/2303.03811)"
    },
    "Efficient Skill Acquisition for Complex Manipulation Tasks in Obstructed Environments": {
        "abstract": "Data efficiency in robotic skill acquisition is crucial for operating robots in varied small-batch assembly settings. To operate in such environments, robots must have robust obstacle avoidance and versatile goal conditioning acquired from only a few simple demonstrations. Existing approaches, however, fall short of these requirements. Deep reinforcement learning (RL) enables a robot to learn complex manipulation tasks but is often limited to small task spaces in the real world due to sample inefficiency and safety concerns. Motion planning (MP) can generate collision-free paths in obstructed environments, but cannot solve complex manipulation tasks and requires goal states often specified by a user or object-specific pose estimator. In this work, we propose a system for efficient skill acquisition that leverages an object-centric generative model (OCGM) for versatile goal identification to specify a goal for MP combined with RL to solve complex manipulation tasks in obstructed environments. Specifically, OCGM enables one-shot target object identification and re-identification in new scenes, allowing MP to guide the robot to the target object while avoiding obstacles. This is combined with a skill transition network, which bridges the gap between terminal states of MP and feasible start states of a sample-efficient RL policy. The experiments demonstrate that our OCGM-based one-shot goal identification provides competitive accuracy to other baseline approaches and that our modular framework outperforms competitive baselines, including a state-of-the-art RL algorithm, by a significant margin for complex manipulation tasks in obstructed environments.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.03365",
        "string": "[Efficient Skill Acquisition for Complex Manipulation Tasks in Obstructed Environments](https://arxiv.org/pdf/2303.03365)"
    },
    "End-to-End Deep Visual Control for Mastering Needle-Picking Skills With World Models and Behavior Cloning": {
        "abstract": "Needle picking is a challenging surgical task in robot-assisted surgery due to the characteristics of small slender shapes of needles, needles' variations in shapes and sizes, and demands for millimeter-level control. Prior works, heavily relying on the prior of needles (e.g., geometric models), are hard to scale to unseen needles' variations. In addition, visual tracking errors can not be minimized online using their approaches. In this paper, we propose an end-to-end deep visual learning framework for needle-picking tasks where both visual and control components can be learned jointly online. Our proposed framework integrates a state-of-the-art reinforcement learning framework, Dreamer, with behavior cloning (BC). Besides, two novel techniques, i.e., Virtual Clutch and Dynamic Spotlight Adaptation (DSA), are introduced to our end-to-end visual controller for needle-picking tasks. We conducted extensive experiments in simulation to evaluate the performance, robustness, variation adaptation, and effectiveness of individual components of our method. Our approach, trained by 8k demonstration timesteps and 140k online policy timesteps, can achieve a remarkable success rate of 80%, a new state-of-the-art with end-to-end vision-based surgical robot learning for delicate operations tasks. Furthermore, our method effectively demonstrated its superiority in generalization to unseen dynamic scenarios with needle variations and image disturbance, highlighting its robustness and versatility. Codes and videos are available at https://sites.google.com/view/dreamerbc.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.03675",
        "string": "[End-to-End Deep Visual Control for Mastering Needle-Picking Skills With World Models and Behavior Cloning](https://arxiv.org/pdf/2303.03675)"
    },
    "Evolutionary Reinforcement Learning: A Survey": {
        "abstract": "Reinforcement learning (RL) is a machine learning approach that trains agents to maximize cumulative rewards through interactions with environments. The integration of RL with deep learning has recently resulted in impressive achievements in a wide range of challenging tasks, including board games, arcade games, and robot control. Despite these successes, there remain several crucial challenges, including brittle convergence properties caused by sensitive hyperparameters, difficulties in temporal credit assignment with long time horizons and sparse rewards, a lack of diverse exploration, especially in continuous search space scenarios, difficulties in credit assignment in multi-agent reinforcement learning, and conflicting objectives for rewards. Evolutionary computation (EC), which maintains a population of learning agents, has demonstrated promising performance in addressing these limitations. This article presents a comprehensive survey of state-of-the-art methods for integrating EC into RL, referred to as evolutionary reinforcement learning (EvoRL). We categorize EvoRL methods according to key research fields in RL, including hyperparameter optimization, policy search, exploration, reward shaping, meta-RL, and multi-objective RL. We then discuss future research directions in terms of efficient methods, benchmarks, and scalable platforms. This survey serves as a resource for researchers and practitioners interested in the field of EvoRL, highlighting the important challenges and opportunities for future research. With the help of this survey, researchers and practitioners can develop more efficient methods and tailored benchmarks for EvoRL, further advancing this promising cross-disciplinary research field.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.04150",
        "string": "[Evolutionary Reinforcement Learning: A Survey](https://arxiv.org/pdf/2303.04150)"
    },
    "Exploiting Contextual Structure to Generate Useful Auxiliary Tasks": {
        "abstract": "Reinforcement learning requires interaction with an environment, which is expensive for robots. This constraint necessitates approaches that work with limited environmental interaction by maximizing the reuse of previous experiences. We propose an approach that maximizes experience reuse while learning to solve a given task by generating and simultaneously learning useful auxiliary tasks. To generate these tasks, we construct an abstract temporal logic representation of the given task and leverage large language models to generate context-aware object embeddings that facilitate object replacements. Counterfactual reasoning and off-policy methods allow us to simultaneously learn these auxiliary tasks while solving the given target task. We combine these insights into a novel framework for multitask reinforcement learning and experimentally show that our generated auxiliary tasks share similar underlying exploration requirements as the given task, thereby maximizing the utility of directed exploration. Our approach allows agents to automatically learn additional useful policies without extra environment interaction.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.05038",
        "string": "[Exploiting Contextual Structure to Generate Useful Auxiliary Tasks](https://arxiv.org/pdf/2303.05038)"
    },
    "Exploration via Epistemic Value Estimation": {
        "abstract": "How to efficiently explore in reinforcement learning is an open problem. Many exploration algorithms employ the epistemic uncertainty of their own value predictions -- for instance to compute an exploration bonus or upper confidence bound. Unfortunately the required uncertainty is difficult to estimate in general with function approximation.\n  We propose epistemic value estimation (EVE): a recipe that is compatible with sequential decision making and with neural network function approximators. It equips agents with a tractable posterior over all their parameters from which epistemic value uncertainty can be computed efficiently.\n  We use the recipe to derive an epistemic Q-Learning agent and observe competitive performance on a series of benchmarks. Experiments confirm that the EVE recipe facilitates efficient exploration in hard exploration tasks.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.04012",
        "string": "[Exploration via Epistemic Value Estimation](https://arxiv.org/pdf/2303.04012)"
    },
    "FaaSched: A Jitter-Aware Serverless Scheduler": {
        "abstract": "Serverless computing systems are becoming very popular. Large corporations such as Netflix, Airbnb, and Coca-Cola use such systems for running their websites and IT systems. The advantages of such systems include superior support for auto-scaling, load balancing, and fast distributed processing. These are multi-QoS systems where different classes of applications have different latency and jitter (variation in the latency) requirements: we consider a mix of latency-sensitive (LS) and latency-desirable (LD) applications. Ensuring proper schedulability and QoS enforcement of LS applications is non-trivial. We need to minimize the jitter without increasing the response latency of LS applications, and we also need to keep the degradation of the response latency of LD applications in check.\n  This is the first paper in this domain that achieves a trade-off between the jitter suffered by LS applications and the response latency of LD applications. We minimize the former with a bound on the latter using a reinforcement learning (RL) based scheme. To design such an RL scheme, we performed detailed characterization studies to find the input variables of interest, defined novel state representations, and proposed a bespoke reward function that allows us to achieve this trade-off. For an aggressive use case comprising five popular LS and LD applications each, we show a reduction in response time variance and mean latency of 50.31% and 27.4%, respectively, for LS applications. The mean degradation in the execution latency of LD applications was limited to 19.88%.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.06473",
        "string": "[FaaSched: A Jitter-Aware Serverless Scheduler](https://arxiv.org/pdf/2303.06473)"
    },
    "Flexible Gear Assembly With Visual Servoing and Force Feedback": {
        "abstract": "Gear assembly is an essential but challenging task in industrial automation. This paper presents a novel two-stage approach for achieving high-precision and flexible gear assembly. The proposed approach integrates YOLO to coarsely localize the workpiece in a searching phase and deep reinforcement learning (DRL) to complete the insertion. Specifically, DRL addresses the challenge of partial visibility when the on-wrist camera is too close to the workpiece. Additionally, force feedback is used to smoothly transit the process from the first phase to the second phase. To reduce the data collection effort for training deep neural networks, we use synthetic RGB images for training YOLO and construct an offline interaction environment leveraging sampled real-world data for training DRL agents. We evaluate the proposed approach in a gear assembly experiment with a precision tolerance of 0.3mm. The results show that our method can robustly and efficiently complete searching and insertion from arbitrary positions within an average of 15 seconds.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.03153",
        "string": "[Flexible Gear Assembly With Visual Servoing and Force Feedback](https://arxiv.org/pdf/2303.03153)"
    },
    "Foundation Models for Decision Making: Problems, Methods, and Opportunities": {
        "abstract": "Foundation models pretrained on diverse data at scale have demonstrated extraordinary capabilities in a wide range of vision and language tasks. When such models are deployed in real world environments, they inevitably interface with other entities and agents. For example, language models are often used to interact with human beings through dialogue, and visual perception models are used to autonomously navigate neighborhood streets. In response to these developments, new paradigms are emerging for training foundation models to interact with other agents and perform long-term reasoning. These paradigms leverage the existence of ever-larger datasets curated for multimodal, multitask, and generalist interaction. Research at the intersection of foundation models and decision making holds tremendous promise for creating powerful new systems that can interact effectively across a diverse range of applications such as dialogue, autonomous driving, healthcare, education, and robotics. In this manuscript, we examine the scope of foundation models for decision making, and provide conceptual tools and technical background for understanding the problem space and exploring new research directions. We review recent approaches that ground foundation models in practical decision making applications through a variety of methods such as prompting, conditional generative modeling, planning, optimal control, and reinforcement learning, and discuss common challenges and open problems in the field.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.04129",
        "string": "[Foundation Models for Decision Making: Problems, Methods, and Opportunities](https://arxiv.org/pdf/2303.04129)"
    },
    "GOATS: Goal Sampling Adaptation for Scooping with Curriculum Reinforcement Learning": {
        "abstract": "In this work, we first formulate the problem of goal-conditioned robotic water scooping with reinforcement learning. This task is challenging due to the complex dynamics of fluid and multi-modal goal-reaching. The policy is required to achieve both position goals and water amount goals, which leads to a large convoluted goal state space. To address these challenges, we introduce Goal Sampling Adaptation for Scooping (GOATS), a curriculum reinforcement learning method that can learn an effective and generalizable policy for robot scooping tasks. Specifically, we use a goal-factorized reward formulation and interpolate position goal distributions and amount goal distributions to create curriculum through the learning process. As a result, our proposed method can outperform the baselines in simulation and achieves 5.46% and 8.71% amount errors on bowl scooping and bucket scooping tasks, respectively, under 1000 variations of initial water states in the tank and a large goal state space. Besides being effective in simulation environments, our method can efficiently generalize to noisy real-robot water-scooping scenarios with different physical configurations and unseen settings, demonstrating superior efficacy and generalizability. The videos of this work are available on our project page: https://sites.google.com/view/goatscooping.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.05193",
        "string": "[GOATS: Goal Sampling Adaptation for Scooping with Curriculum Reinforcement Learning](https://arxiv.org/pdf/2303.05193)"
    },
    "Graph Decision Transformer": {
        "abstract": "Offline reinforcement learning (RL) is a challenging task, whose objective is to learn policies from static trajectory data without interacting with the environment. Recently, offline RL has been viewed as a sequence modeling problem, where an agent generates a sequence of subsequent actions based on a set of static transition experiences. However, existing approaches that use transformers to attend to all tokens naively can overlook the dependencies between different tokens and limit long-term dependency learning. In this paper, we propose the Graph Decision Transformer (GDT), a novel offline RL approach that models the input sequence into a causal graph to capture potential dependencies between fundamentally different concepts and facilitate temporal and causal relationship learning. GDT uses a graph transformer to process the graph inputs with relation-enhanced mechanisms, and an optional sequence transformer to handle fine-grained spatial information in visual tasks. Our experiments show that GDT matches or surpasses the performance of state-of-the-art offline RL methods on image-based Atari and OpenAI Gym.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.03747",
        "string": "[Graph Decision Transformer](https://arxiv.org/pdf/2303.03747)"
    },
    "Graph Neural Network Autoencoders for Efficient Quantum Circuit Optimisation": {
        "abstract": "Reinforcement learning (RL) is a promising method for quantum circuit optimisation. However, the state space that has to be explored by an RL agent is extremely large when considering all the possibilities in which a quantum circuit can be transformed through local rewrite operations. This state space explosion slows down the learning of RL-based optimisation strategies. We present for the first time how to use graph neural network (GNN) autoencoders for the optimisation of quantum circuits. We construct directed acyclic graphs from the quantum circuits, encode the graphs and use the encodings to represent RL states. We illustrate our proof of concept implementation on Bernstein-Vazirani circuits and, from preliminary results, we conclude that our autoencoder approach: a) maintains the optimality of the original RL method; b) reduces by 20 \\% the size of the table that encodes the learned optimisation strategy. Our method is the first realistic first step towards very large scale RL quantum circuit optimisation.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.03280",
        "string": "[Graph Neural Network Autoencoders for Efficient Quantum Circuit Optimisation](https://arxiv.org/pdf/2303.03280)"
    },
    "Intent-based Deep Reinforcement Learning for Multi-agent Informative Path Planning": {
        "abstract": "In multi-agent informative path planning (MAIPP), agents must collectively construct a global belief map of an underlying distribution of interest (e.g., gas concentration, light intensity, or pollution levels) over a given domain, based on measurements taken along their trajectory. They must frequently replan their path to balance the distributed exploration of new areas and the collective, meticulous exploitation of known high-interest areas, to maximize the information gained within a predefined budget (e.g., path length or working time). A common approach to achieving such cooperation relies on planning the agents' paths reactively, conditioned on other agents' future actions. However, as the agent's belief is updated continuously, these predicted future actions may not end up being the ones executed by agents, introducing a form of noise/inaccuracy in the system and often decreasing performance. In this work, we propose a decentralized deep reinforcement learning (DRL) approach to MAIPP, which relies on an attention-based neural network, where agents optimize long-term individual and cooperative objectives by explicitly sharing their intent (i.e., medium-/long-term future positions distribution, obtained from their individual policy) in a reactive, asynchronous manner. That is, in our work, intent sharing allows agents to learn to claim/avoid broader areas of the world. Moreover, since our approach relies on learned attention over these shared intents, agents are able to learn to recognize the useful portion(s) of these (imperfect) predictions to maximize cooperation even in the presence of imperfect information. Our comparison experiments demonstrate the performance of our approach compared to its variants and high-quality baselines over a large set of MAIPP simulations.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.05351",
        "string": "[Intent-based Deep Reinforcement Learning for Multi-agent Informative Path Planning](https://arxiv.org/pdf/2303.05351)"
    },
    "Learned Parameter Selection for Robotic Information Gathering": {
        "abstract": "When robots are deployed in the field for environmental monitoring they typically execute pre-programmed motions, such as lawnmower paths, instead of adaptive methods, such as informative path planning. One reason for this is that adaptive methods are dependent on parameter choices that are both critical to set correctly and difficult for the non-specialist to choose. Here, we show how to automatically configure a planner for informative path planning by training a reinforcement learning agent to select planner parameters at each iteration of informative path planning. We demonstrate our method with 37 instances of 3 distinct environments, and compare it against pure (end-to-end) reinforcement learning techniques, as well as approaches that do not use a learned model to change the planner parameters. Our method shows a 9.53% mean improvement in the cumulative reward across diverse environments when compared to end-to-end learning based methods; we also demonstrate via a field experiment how it can be readily used to facilitate high performance deployment of an information gathering robot.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.05022",
        "string": "[Learned Parameter Selection for Robotic Information Gathering](https://arxiv.org/pdf/2303.05022)"
    },
    "Learning Bipedal Walking for Humanoids with Current Feedback": {
        "abstract": "Recent advances in deep reinforcement learning (RL) based techniques combined with training in simulation have offered a new approach to developing control policies for legged robots. However, the application of such approaches to real hardware has largely been limited to quadrupedal robots with direct-drive actuators and light-weight bipedal robots with low gear-ratio transmission systems. Application to life-sized humanoid robots has been elusive due to the large sim-to-real gap arising from their large size, heavier limbs, and a high gear-ratio transmission systems. In this paper, we present an approach for effectively overcoming the sim-to-real gap issue for humanoid robots arising from inaccurate torque tracking at the actuator level. Our key idea is to utilize the current feedback from the motors on the real robot, after training the policy in a simulation environment artificially degraded with poor torque tracking. Our approach successfully trains an end-to-end policy in simulation that can be deployed on a real HRP-5P humanoid robot for bipedal locomotion on challenging terrain. We also perform robustness tests on the RL policy and compare its performance against a conventional model-based controller for walking on uneven terrain. YouTube video: https://youtu.be/IeUaSsBRbNY\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.03724",
        "string": "[Learning Bipedal Walking for Humanoids with Current Feedback](https://arxiv.org/pdf/2303.03724)"
    },
    "Learning Environment-Aware Control Barrier Functions for Safe and Feasible Multi-Robot Navigation": {
        "abstract": "Control Barrier Functions (CBFs) have been applied to provide safety guarantees for robot navigation. Traditional approaches consider fixed CBFs during navigation and hand-tune the underlying parameters apriori. Such approaches are inefficient and vulnerable to changes in the environment. The goal of this paper is to learn CBFs for multi-robot navigation based on what robots perceive about their environment. In order to guarantee the feasibility of the navigation task, while ensuring robot safety, we pursue a trade-off between conservativeness and aggressiveness in robot behavior by defining dynamic environment-aware CBF constraints. Since the explicit relationship between CBF constraints and navigation performance is challenging to model, we leverage reinforcement learning to learn time-varying CBFs in a model-free manner. We parameterize the CBF policy with graph neural networks (GNNs), and design GNNs that are translation invariant and permutation equivariant, to synthesize decentralized policies that generalize across environments. The proposed approach maintains safety guarantees (due to the underlying CBFs), while optimizing navigation performance (due to the reward-based learning). We perform simulations that compare the proposed approach with fixed CBFs tuned by exhaustive grid-search. The results show that environment-aware CBFs are capable of adapting to robot movements and obstacle changes, yielding improved navigation performance and robust generalization.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.04313",
        "string": "[Learning Environment-Aware Control Barrier Functions for Safe and Feasible Multi-Robot Navigation](https://arxiv.org/pdf/2303.04313)"
    },
    "Learning Humanoid Locomotion with Transformers": {
        "abstract": "We present a sim-to-real learning-based approach for real-world humanoid locomotion. Our controller is a causal Transformer trained by autoregressive prediction of future actions from the history of observations and actions. We hypothesize that the observation-action history contains useful information about the world that a powerful Transformer model can use to adapt its behavior in-context, without updating its weights. We do not use state estimation, dynamics models, trajectory optimization, reference trajectories, or pre-computed gait libraries. Our controller is trained with large-scale model-free reinforcement learning on an ensemble of randomized environments in simulation and deployed to the real world in a zero-shot fashion. We evaluate our approach in high-fidelity simulation and successfully deploy it to the real robot as well. To the best of our knowledge, this is the first demonstration of a fully learning-based method for real-world full-sized humanoid locomotion.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.03381",
        "string": "[Learning Humanoid Locomotion with Transformers](https://arxiv.org/pdf/2303.03381)"
    },
    "Learning When to Treat Business Processes: Prescriptive Process Monitoring with Causal Inference and Reinforcement Learning": {
        "abstract": "Increasing the success rate of a process, i.e. the percentage of cases that end in a positive outcome, is a recurrent process improvement goal. At runtime, there are often certain actions (a.k.a. treatments) that workers may execute to lift the probability that a case ends in a positive outcome. For example, in a loan origination process, a possible treatment is to issue multiple loan offers to increase the probability that the customer takes a loan. Each treatment has a cost. Thus, when defining policies for prescribing treatments to cases, managers need to consider the net gain of the treatments. Also, the effect of a treatment varies over time: treating a case earlier may be more effective than later in a case. This paper presents a prescriptive monitoring method that automates this decision-making task. The method combines causal inference and reinforcement learning to learn treatment policies that maximize the net gain. The method leverages a conformal prediction technique to speed up the convergence of the reinforcement learning mechanism by separating cases that are likely to end up in a positive or negative outcome, from uncertain cases. An evaluation on two real-life datasets shows that the proposed method outperforms a state-of-the-art baseline.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.03572",
        "string": "[Learning When to Treat Business Processes: Prescriptive Process Monitoring with Causal Inference and Reinforcement Learning](https://arxiv.org/pdf/2303.03572)"
    },
    "Learning to Backdoor Federated Learning": {
        "abstract": "In a federated learning (FL) system, malicious participants can easily embed backdoors into the aggregated model while maintaining the model's performance on the main task. To this end, various defenses, including training stage aggregation-based defenses and post-training mitigation defenses, have been proposed recently. While these defenses obtain reasonable performance against existing backdoor attacks, which are mainly heuristics based, we show that they are insufficient in the face of more advanced attacks. In particular, we propose a general reinforcement learning-based backdoor attack framework where the attacker first trains a (non-myopic) attack policy using a simulator built upon its local data and common knowledge on the FL system, which is then applied during actual FL training. Our attack framework is both adaptive and flexible and achieves strong attack performance and durability even under state-of-the-art defenses.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.03320",
        "string": "[Learning to Backdoor Federated Learning](https://arxiv.org/pdf/2303.03320)"
    },
    "Learning to Select Camera Views: Efficient Multiview Understanding at Few Glances": {
        "abstract": "Multiview camera setups have proven useful in many computer vision applications for reducing ambiguities, mitigating occlusions, and increasing field-of-view coverage. However, the high computational cost associated with multiple views poses a significant challenge for end devices with limited computational resources. To address this issue, we propose a view selection approach that analyzes the target object or scenario from given views and selects the next best view for processing. Our approach features a reinforcement learning based camera selection module, MVSelect, that not only selects views but also facilitates joint training with the task network. Experimental results on multiview classification and detection tasks show that our approach achieves promising performance while using only 2 or 3 out of N available views, significantly reducing computational costs. Furthermore, analysis on the selected views reveals that certain cameras can be shut off with minimal performance impact, shedding light on future camera layout optimization for multiview systems. Code is available at https://github.com/hou-yz/MVSelect.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.06145",
        "string": "[Learning to Select Camera Views: Efficient Multiview Understanding at Few Glances](https://arxiv.org/pdf/2303.06145)"
    },
    "MAESTRO: Open-Ended Environment Design for Multi-Agent Reinforcement Learning": {
        "abstract": "Open-ended learning methods that automatically generate a curriculum of increasingly challenging tasks serve as a promising avenue toward generally capable reinforcement learning agents. Existing methods adapt curricula independently over either environment parameters (in single-agent settings) or co-player policies (in multi-agent settings). However, the strengths and weaknesses of co-players can manifest themselves differently depending on environmental features. It is thus crucial to consider the dependency between the environment and co-player when shaping a curriculum in multi-agent domains. In this work, we use this insight and extend Unsupervised Environment Design (UED) to multi-agent environments. We then introduce Multi-Agent Environment Design Strategist for Open-Ended Learning (MAESTRO), the first multi-agent UED approach for two-player zero-sum settings. MAESTRO efficiently produces adversarial, joint curricula over both environments and co-players and attains minimax-regret guarantees at Nash equilibrium. Our experiments show that MAESTRO outperforms a number of strong baselines on competitive two-player games, spanning discrete and continuous control settings.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.03376",
        "string": "[MAESTRO: Open-Ended Environment Design for Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/2303.03376)"
    },
    "MAP-Elites with Descriptor-Conditioned Gradients and Archive Distillation into a Single Policy": {
        "abstract": "Quality-Diversity algorithms, such as MAP-Elites, are a branch of Evolutionary Computation generating collections of diverse and high-performing solutions, that have been successfully applied to a variety of domains and particularly in evolutionary robotics. However, MAP-Elites performs a divergent search based on random mutations originating from Genetic Algorithms, and thus, is limited to evolving populations of low-dimensional solutions. PGA-MAP-Elites overcomes this limitation by integrating a gradient-based variation operator inspired by Deep Reinforcement Learning which enables the evolution of large neural networks. Although high-performing in many environments, PGA-MAP-Elites fails on several tasks where the convergent search of the gradient-based operator does not direct mutations towards archive-improving solutions. In this work, we present two contributions: (1) we enhance the Policy Gradient variation operator with a descriptor-conditioned critic that improves the archive across the entire descriptor space, (2) we exploit the actor-critic training to learn a descriptor-conditioned policy at no additional cost, distilling the knowledge of the archive into one single versatile policy that can execute the entire range of behaviors contained in the archive. Our algorithm, DCG-MAP-Elites improves the QD score over PGA-MAP-Elites by 82% on average, on a set of challenging locomotion tasks.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.03832",
        "string": "[MAP-Elites with Descriptor-Conditioned Gradients and Archive Distillation into a Single Policy](https://arxiv.org/pdf/2303.03832)"
    },
    "MCTS-GEB: Monte Carlo Tree Search is a Good E-graph Builder": {
        "abstract": "Rewrite systems [6, 10, 12] have been widely employing equality saturation [9], which is an optimisation methodology that uses a saturated e-graph to represent all possible sequences of rewrite simultaneously, and then extracts the optimal one. As such, optimal results can be achieved by avoiding the phase-ordering problem. However, we observe that when the e-graph is not saturated, it cannot represent all possible rewrite opportunities and therefore the phase-ordering problem is re-introduced during the construction phase of the e-graph. To address this problem, we propose MCTS-GEB, a domain-general rewrite system that applies reinforcement learning (RL) to e-graph construction. At its core, MCTS-GEB uses a Monte Carlo Tree Search (MCTS) [3] to efficiently plan for the optimal e-graph construction, and therefore it can effectively eliminate the phase-ordering problem at the construction phase and achieve better performance within a reasonable time. Evaluation in two different domains shows MCTS-GEB can outperform the state-of-the-art rewrite systems by up to 49x, while the optimisation can generally take less than an hour, indicating MCTS-GEB is a promising building block for the future generation of rewrite systems.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.04651",
        "string": "[MCTS-GEB: Monte Carlo Tree Search is a Good E-graph Builder](https://arxiv.org/pdf/2303.04651)"
    },
    "Mastering Strategy Card Game (Legends of Code and Magic) via End-to-End Policy and Optimistic Smooth Fictitious Play": {
        "abstract": "Deep Reinforcement Learning combined with Fictitious Play shows impressive results on many benchmark games, most of which are, however, single-stage. In contrast, real-world decision making problems may consist of multiple stages, where the observation spaces and the action spaces can be completely different across stages. We study a two-stage strategy card game Legends of Code and Magic and propose an end-to-end policy to address the difficulties that arise in multi-stage game. We also propose an optimistic smooth fictitious play algorithm to find the Nash Equilibrium for the two-player game. Our approach wins double championships of COG2022 competition. Extensive studies verify and show the advancement of our approach.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.04096",
        "string": "[Mastering Strategy Card Game (Legends of Code and Magic) via End-to-End Policy and Optimistic Smooth Fictitious Play](https://arxiv.org/pdf/2303.04096)"
    },
    "Multiple Hands Make Light Work: Enhancing Quality and Diversity using MAP-Elites with Multiple Parallel Evolution Strategies": {
        "abstract": "With the development of hardware accelerators and their corresponding tools, evaluations have become more affordable through fast and massively parallel evaluations in some applications. This advancement has drastically sped up the runtime of evolution-inspired algorithms such as Quality-Diversity optimization, creating tremendous potential for algorithmic innovation through scale. In this work, we propose MAP-Elites-Multi-ES (MEMES), a novel QD algorithm based on Evolution Strategies (ES) designed for fast parallel evaluations. ME-Multi-ES builds on top of the existing MAP-Elites-ES algorithm, scaling it by maintaining multiple independent ES threads with massive parallelization. We also introduce a new dynamic reset procedure for the lifespan of the independent ES to autonomously maximize the improvement of the QD population. We show experimentally that MEMES outperforms existing gradient-based and objective-agnostic QD algorithms when compared in terms of generations. We perform this comparison on both black-box optimization and QD-Reinforcement Learning tasks, demonstrating the benefit of our approach across different problems and domains. Finally, we also find that our approach intrinsically enables optimization of fitness locally around a niche, a phenomenon not observed in other QD algorithms.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.06137",
        "string": "[Multiple Hands Make Light Work: Enhancing Quality and Diversity using MAP-Elites with Multiple Parallel Evolution Strategies](https://arxiv.org/pdf/2303.06137)"
    },
    "New Record-Breaking Condorcet Domains on 10 and 11 Alternatives": {
        "abstract": "We report on discovering new record-breaking Condorcet domains on $n=10$ and n=11 alternatives, challenging long-standing voting theory results. Our work presents new records with sizes of 1082 (previous record 1069) for n=10 and 2349 (previous record 2324) for $n=11$, which appear sporadic and do not fit into the existing alternating schema discovered in 1996.\n  While the method used to discover these domains was inspired by the application of value functions in reinforcement learning, a subcategory of artificial intelligence, the current version of the method is somewhat ad-hoc and unstable. Therefore, we will not expound on the search method in this paper. Instead, we outline the key components that contribute to the success of our approach. We will also discuss the theoretical implications of our findings and explore the structure of the new Condorcet domains, raising several open questions related to them.\n  Our results contribute to the ongoing investigation of Condorcet domains and their mathematical properties, potentially demonstrating the power of artificial intelligence-inspired problem-solving methods in advancing mathematical research.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.06524",
        "string": "[New Record-Breaking Condorcet Domains on 10 and 11 Alternatives](https://arxiv.org/pdf/2303.06524)"
    },
    "On the Sample Complexity of Vanilla Model-Based Offline Reinforcement Learning with Dependent Samples": {
        "abstract": "Offline reinforcement learning (offline RL) considers problems where learning is performed using only previously collected samples and is helpful for the settings in which collecting new data is costly or risky. In model-based offline RL, the learner performs estimation (or optimization) using a model constructed according to the empirical transition frequencies. We analyze the sample complexity of vanilla model-based offline RL with dependent samples in the infinite-horizon discounted-reward setting. In our setting, the samples obey the dynamics of the Markov decision process and, consequently, may have interdependencies. Under no assumption of independent samples, we provide a high-probability, polynomial sample complexity bound for vanilla model-based off-policy evaluation that requires partial or uniform coverage. We extend this result to the off-policy optimization under uniform coverage. As a comparison to the model-based approach, we analyze the sample complexity of off-policy evaluation with vanilla importance sampling in the infinite-horizon setting. Finally, we provide an estimator that outperforms the sample-mean estimator for almost deterministic dynamics that are prevalent in reinforcement learning.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.04268",
        "string": "[On the Sample Complexity of Vanilla Model-Based Offline Reinforcement Learning with Dependent Samples](https://arxiv.org/pdf/2303.04268)"
    },
    "Optimal foraging strategies can be learned and outperform L\u00e9vy walks": {
        "abstract": "L\u00e9vy walks and other theoretical models of optimal foraging have been successfully used to describe real-world scenarios, attracting attention in several fields such as economy, physics, ecology, and evolutionary biology. However, it remains unclear in most cases which strategies maximize foraging efficiency and whether such strategies can be learned by living organisms. To address these questions, we model foragers as reinforcement learning agents. We first prove theoretically that maximizing rewards in our reinforcement learning model is equivalent to optimizing foraging efficiency. We then show with numerical experiments that our agents learn foraging strategies which outperform the efficiency of known strategies such as L\u00e9vy walks.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.06050",
        "string": "[Optimal foraging strategies can be learned and outperform L\u00e9vy walks](https://arxiv.org/pdf/2303.06050)"
    },
    "Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback": {
        "abstract": "Large language models (LLMs) are used to generate content for a wide range of tasks, and are set to reach a growing audience in coming years due to integration in product interfaces like ChatGPT or search engines like Bing. This intensifies the need to ensure that models are aligned with human preferences and do not produce unsafe, inaccurate or toxic outputs. While alignment techniques like reinforcement learning with human feedback (RLHF) and red-teaming can mitigate some safety concerns and improve model capabilities, it is unlikely that an aggregate fine-tuning process can adequately represent the full range of users' preferences and values. Different people may legitimately disagree on their preferences for language and conversational norms, as well as on values or ideologies which guide their communication. Personalising LLMs through micro-level preference learning processes may result in models that are better aligned with each user. However, there are several normative challenges in defining the bounds of a societally-acceptable and safe degree of personalisation. In this paper, we ask how, and in what ways, LLMs should be personalised. First, we review literature on current paradigms for aligning LLMs with human feedback, and identify issues including (i) a lack of clarity regarding what alignment means; (ii) a tendency of technology providers to prescribe definitions of inherently subjective preferences and values; and (iii) a 'tyranny of the crowdworker', exacerbated by a lack of documentation in who we are really aligning to. Second, we present a taxonomy of benefits and risks associated with personalised LLMs, for individuals and society at large. Finally, we propose a three-tiered policy framework that allows users to experience the benefits of personalised alignment, while restraining unsafe and undesirable LLM-behaviours within (supra-)national and organisational bounds.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.05453",
        "string": "[Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback](https://arxiv.org/pdf/2303.05453)"
    },
    "Power and Interference Control for VLC-Based UDN: A Reinforcement Learning Approach": {
        "abstract": "Visible light communication (VLC) has been widely applied as a promising solution for modern short range communication. When it comes to the deployment of LED arrays in VLC networks, the emerging ultra-dense network (UDN) technology can be adopted to expand the VLC network's capacity. However, the problem of inter-cell interference (ICI) mitigation and efficient power control in the VLC-based UDN is still a critical challenge. To this end, a reinforcement learning (RL) based VLC UDN architecture is devised in this paper. The deployment of the cells is optimized via spatial reuse to mitigate ICI. An RL-based algorithm is proposed to dynamically optimize the policy of power and interference control, maximizing the system utility in the complicated and dynamic environment. Simulation results demonstrate the superiority of the proposed scheme, it increase the system utility and achievable data rate while reducing the energy consumption and ICI, which outperforms the benchmark scheme.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.05448",
        "string": "[Power and Interference Control for VLC-Based UDN: A Reinforcement Learning Approach](https://arxiv.org/pdf/2303.05448)"
    },
    "Proactive Multi-Camera Collaboration For 3D Human Pose Estimation": {
        "abstract": "This paper presents a multi-agent reinforcement learning (MARL) scheme for proactive Multi-Camera Collaboration in 3D Human Pose Estimation in dynamic human crowds. Traditional fixed-viewpoint multi-camera solutions for human motion capture (MoCap) are limited in capture space and susceptible to dynamic occlusions. Active camera approaches proactively control camera poses to find optimal viewpoints for 3D reconstruction. However, current methods still face challenges with credit assignment and environment dynamics. To address these issues, our proposed method introduces a novel Collaborative Triangulation Contribution Reward (CTCR) that improves convergence and alleviates multi-agent credit assignment issues resulting from using 3D reconstruction accuracy as the shared reward. Additionally, we jointly train our model with multiple world dynamics learning tasks to better capture environment dynamics and encourage anticipatory behaviors for occlusion avoidance. We evaluate our proposed method in four photo-realistic UE4 environments to ensure validity and generalizability. Empirical results show that our method outperforms fixed and active baselines in various scenarios with different numbers of cameras and humans.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.03767",
        "string": "[Proactive Multi-Camera Collaboration For 3D Human Pose Estimation](https://arxiv.org/pdf/2303.03767)"
    },
    "Provably Efficient Model-Free Algorithms for Non-stationary CMDPs": {
        "abstract": "We study model-free reinforcement learning (RL) algorithms in episodic non-stationary constrained Markov Decision Processes (CMDPs), in which an agent aims to maximize the expected cumulative reward subject to a cumulative constraint on the expected utility (cost). In the non-stationary environment, reward, utility functions, and transition kernels can vary arbitrarily over time as long as the cumulative variations do not exceed certain variation budgets. We propose the first model-free, simulator-free RL algorithms with sublinear regret and zero constraint violation for non-stationary CMDPs in both tabular and linear function approximation settings with provable performance guarantees. Our results on regret bound and constraint violation for the tabular case match the corresponding best results for stationary CMDPs when the total budget is known. Additionally, we present a general framework for addressing the well-known challenges associated with analyzing non-stationary CMDPs, without requiring prior knowledge of the variation budget. We apply the approach for both tabular and linear approximation settings.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.05733",
        "string": "[Provably Efficient Model-Free Algorithms for Non-stationary CMDPs](https://arxiv.org/pdf/2303.05733)"
    },
    "Quantum Machine Learning Implementations: Proposals and Experiments": {
        "abstract": "This article gives an overview and a perspective of recent theoretical proposals and their experimental implementations in the field of quantum machine learning. Without an aim to being exhaustive, the article reviews specific high-impact topics such as quantum reinforcement learning, quantum autoencoders, and quantum memristors, and their experimental realizations in the platforms of quantum photonics and superconducting circuits. The field of quantum machine learning could be among the first quantum technologies producing results that are beneficial for industry and, in turn, to society. Therefore, it is necessary to push forward initial quantum implementations of this technology, in Noisy Intermediate-Scale Quantum Computers, aiming for achieving fruitful calculations in machine learning that are better than with any other current or future computing paradigm.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.06263",
        "string": "[Quantum Machine Learning Implementations: Proposals and Experiments](https://arxiv.org/pdf/2303.06263)"
    },
    "Quantum Power Electronics: From Theory to Implementation": {
        "abstract": "While impressive progress has been already achieved in wide-bandgap (WBG) semicon-ductors such as 4H-SiC and GaN technologies, the lack of intelligent methodologies to control the gate drives prevented to the exploit the maximum potential of semiconductor chips, from obtaining the desired devices operations. Thus, a potent ongoing trend is to design a fast gate driver switching scheme to upgrade the performance of electronic equipment at the system level. To address this issue, this work proposes a novel intelligent scheme for the control of gate driver switching using the concept of quantum computation in machine learning. In particular, the quantum principle is incorporated into deep reinforcement learning (DRL) to address the hardware limitations of con-ventional computers and the growing amount of data sets. Taking potential benefits of quantum theory, the DRL algorithm influenced by quantum specifications (referred to as QDRL) will not only ameliorate the performance of the native algorithm on traditional computers, but also enhance the progress of relevant research fields like quantum computing and machine learning. To test the prac-ticability and usefulness of QDRL, a dc/dc parallel boost converters feeding constant power loads (CPLs) is chosen as the case study, and several Power Hard-ware-in-the-loop (PHiL) experiments and comparative analysis are given.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.04763",
        "string": "[Quantum Power Electronics: From Theory to Implementation](https://arxiv.org/pdf/2303.04763)"
    },
    "RACCER: Towards Reachable and Certain Counterfactual Explanations for Reinforcement Learning": {
        "abstract": "While reinforcement learning (RL) algorithms have been successfully applied to numerous tasks, their reliance on neural networks makes their behavior difficult to understand and trust. Counterfactual explanations are human-friendly explanations that offer users actionable advice on how to alter the model inputs to achieve the desired output from a black-box system. However, current approaches to generating counterfactuals in RL ignore the stochastic and sequential nature of RL tasks and can produce counterfactuals which are difficult to obtain or do not deliver the desired outcome. In this work, we propose RACCER, the first RL-specific approach to generating counterfactual explanations for the behaviour of RL agents. We first propose and implement a set of RL-specific counterfactual properties that ensure easily reachable counterfactuals with highly-probable desired outcomes. We use a heuristic tree search of agent's execution trajectories to find the most suitable counterfactuals based on the defined properties. We evaluate RACCER in two tasks as well as conduct a user study to show that RL-specific counterfactuals help users better understand agent's behavior compared to the current state-of-the-art approaches.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.04475",
        "string": "[RACCER: Towards Reachable and Certain Counterfactual Explanations for Reinforcement Learning](https://arxiv.org/pdf/2303.04475)"
    },
    "Real-time scheduling of renewable power systems through planning-based reinforcement learning": {
        "abstract": "The growing renewable energy sources have posed significant challenges to traditional power scheduling. It is difficult for operators to obtain accurate day-ahead forecasts of renewable generation, thereby requiring the future scheduling system to make real-time scheduling decisions aligning with ultra-short-term forecasts. Restricted by the computation speed, traditional optimization-based methods can not solve this problem. Recent developments in reinforcement learning (RL) have demonstrated the potential to solve this challenge. However, the existing RL methods are inadequate in terms of constraint complexity, algorithm performance, and environment fidelity. We are the first to propose a systematic solution based on the state-of-the-art reinforcement learning algorithm and the real power grid environment. The proposed approach enables planning and finer time resolution adjustments of power generators, including unit commitment and economic dispatch, thus increasing the grid's ability to admit more renewable energy. The well-trained scheduling agent significantly reduces renewable curtailment and load shedding, which are issues arising from traditional scheduling's reliance on inaccurate day-ahead forecasts. High-frequency control decisions exploit the existing units' flexibility, reducing the power grid's dependence on hardware transformations and saving investment and operating costs, as demonstrated in experimental results. This research exhibits the potential of reinforcement learning in promoting low-carbon and intelligent power systems and represents a solid step toward sustainable electricity generation.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.05205",
        "string": "[Real-time scheduling of renewable power systems through planning-based reinforcement learning](https://arxiv.org/pdf/2303.05205)"
    },
    "Recent Advances of Deep Robotic Affordance Learning: A Reinforcement Learning Perspective": {
        "abstract": "As a popular concept proposed in the field of psychology, affordance has been regarded as one of the important abilities that enable humans to understand and interact with the environment. Briefly, it captures the possibilities and effects of the actions of an agent applied to a specific object or, more generally, a part of the environment. This paper provides a short review of the recent developments of deep robotic affordance learning (DRAL), which aims to develop data-driven methods that use the concept of affordance to aid in robotic tasks. We first classify these papers from a reinforcement learning (RL) perspective, and draw connections between RL and affordances. The technical details of each category are discussed and their limitations identified. We further summarise them and identify future challenges from the aspects of observations, actions, affordance representation, data-collection and real-world deployment. A final remark is given at the end to propose a promising future direction of the RL-based affordance definition to include the predictions of arbitrary action consequences.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.05344",
        "string": "[Recent Advances of Deep Robotic Affordance Learning: A Reinforcement Learning Perspective](https://arxiv.org/pdf/2303.05344)"
    },
    "Reducing Safety Interventions in Provably Safe Reinforcement Learning": {
        "abstract": "Deep Reinforcement Learning (RL) has shown promise in addressing complex robotic challenges. In real-world applications, RL is often accompanied by failsafe controllers as a last resort to avoid catastrophic events. While necessary for safety, these interventions can result in undesirable behaviors, such as abrupt braking or aggressive steering. This paper proposes two safety intervention reduction methods: action replacement and projection, which change the agent's action if it leads to an unsafe state. These approaches are compared to the state-of-the-art constrained RL on the OpenAI safety gym benchmark and a human-robot collaboration task. Our study demonstrates that the combination of our method with provably safe RL leads to high-performing policies with zero safety violations and a low number of failsafe interventions. Our versatile method can be applied to a wide range of real-world robotics tasks, while effectively improving safety without sacrificing task performance.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.03339",
        "string": "[Reducing Safety Interventions in Provably Safe Reinforcement Learning](https://arxiv.org/pdf/2303.03339)"
    },
    "Reinforcement Learning Based Self-play and State Stacking Techniques for Noisy Air Combat Environment": {
        "abstract": "Reinforcement learning (RL) has recently proven itself as a powerful instrument for solving complex problems and even surpassed human performance in several challenging applications. This signifies that RL algorithms can be used in the autonomous air combat problem, which has been studied for many years. The complexity of air combat arises from aggressive close-range maneuvers and agile enemy behaviors. In addition to these complexities, there may be uncertainties in real-life scenarios due to sensor errors, which prevent estimation of the actual position of the enemy. In this case, autonomous aircraft should be successful even in the noisy environments. In this study, we developed an air combat simulation, which provides noisy observations to the agents, therefore, make the air combat problem even more challenging. Thus, we present a state stacking method for noisy RL environments as a noise reduction technique. In our extensive set of experiments, the proposed method significantly outperforms the baseline algorithms in terms of the winning ratio, where the performance improvement is even more pronounced in the high noise levels. In addition, we incorporate a self-play scheme to our training process by periodically updating the enemy with a frozen copy of the training agent. By this way, the training agent performs air combat simulations to an enemy with smarter strategies, which improves the performance and robustness of the agents. In our simulations, we demonstrate that the self-play scheme provides important performance gains compared to the classical RL training.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.03068",
        "string": "[Reinforcement Learning Based Self-play and State Stacking Techniques for Noisy Air Combat Environment](https://arxiv.org/pdf/2303.03068)"
    },
    "Reinforcement Learning Versus Model Predictive Control on Greenhouse Climate Control": {
        "abstract": "Greenhouse is an important protected horticulture system for feeding the world with enough fresh food. However, to maintain an ideal growing climate in a greenhouse requires resources and operational costs. In order to achieve economical and sustainable crop growth, efficient climate control of greenhouse production becomes essential. Model Predictive Control (MPC) is the most commonly used approach in the scientific literature for greenhouse climate control. However, with the developments of sensing and computing techniques, reinforcement learning (RL) is getting increasing attention recently. With each control method having its own way to state the control problem, define control goals, and seek for optimal control actions, MPC and RL are representatives of model-based and learning-based control approaches, respectively. Although researchers have applied certain forms of MPC and RL to control the greenhouse climate, very few effort has been allocated to analyze connections, differences, pros and cons between MPC and RL either from a mathematical or performance perspective. Therefore, this paper will 1) propose MPC and RL approaches for greenhouse climate control in an unified framework; 2) analyze connections and differences between MPC and RL from a mathematical perspective; 3) compare performance of MPC and RL in a simulation study and afterwards present and interpret comparative results into insights for the application of the different control approaches in different scenarios.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.06110",
        "string": "[Reinforcement Learning Versus Model Predictive Control on Greenhouse Climate Control](https://arxiv.org/pdf/2303.06110)"
    },
    "Reinforcement Learning-based Counter-Misinformation Response Generation: A Case Study of COVID-19 Vaccine Misinformation": {
        "abstract": "The spread of online misinformation threatens public health, democracy, and the broader society. While professional fact-checkers form the first line of defense by fact-checking popular false claims, they do not engage directly in conversations with misinformation spreaders. On the other hand, non-expert ordinary users act as eyes-on-the-ground who proactively counter misinformation -- recent research has shown that 96% counter-misinformation responses are made by ordinary users. However, research also found that 2/3 times, these responses are rude and lack evidence. This work seeks to create a counter-misinformation response generation model to empower users to effectively correct misinformation. This objective is challenging due to the absence of datasets containing ground-truth of ideal counter-misinformation responses, and the lack of models that can generate responses backed by communication theories. In this work, we create two novel datasets of misinformation and counter-misinformation response pairs from in-the-wild social media and crowdsourcing from college-educated students. We annotate the collected data to distinguish poor from ideal responses that are factual, polite, and refute misinformation. We propose MisinfoCorrect, a reinforcement learning-based framework that learns to generate counter-misinformation responses for an input misinformation post. The model rewards the generator to increase the politeness, factuality, and refutation attitude while retaining text fluency and relevancy. Quantitative and qualitative evaluation shows that our model outperforms several baselines by generating high-quality counter-responses. This work illustrates the promise of generative text models for social good -- here, to help create a safe and reliable information ecosystem. The code and data is accessible on https://github.com/claws-lab/MisinfoCorrect.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.06433",
        "string": "[Reinforcement Learning-based Counter-Misinformation Response Generation: A Case Study of COVID-19 Vaccine Misinformation](https://arxiv.org/pdf/2303.06433)"
    },
    "Reward Informed Dreamer for Task Generalization in Reinforcement Learning": {
        "abstract": "A long-standing goal of reinforcement learning is that algorithms can learn on training tasks and generalize well on unseen tasks like humans, where different tasks share similar dynamic with different reward functions. A general challenge is that it is nontrivial to quantitatively measure the similarities between these different tasks, which is vital for analyzing the task distribution and further designing algorithms with stronger generalization. To address this, we present a novel metric named Task Distribution Relevance (TDR) via optimal Q functions to capture the relevance of the task distribution quantitatively. In the case of tasks with a high TDR, i.e., the tasks differ significantly, we demonstrate that the Markovian policies cannot distinguish them, yielding poor performance accordingly. Based on this observation, we propose a framework of Reward Informed Dreamer (RID) with reward-informed world models, which captures invariant latent features over tasks and encodes reward signals into policies for distinguishing different tasks. In RID, we calculate the corresponding variational lower bound of the log-likelihood on the data, which includes a novel term to distinguish different tasks via states, based on reward-informed world models. Finally, extensive experiments in DeepMind control suite demonstrate that RID can significantly improve the performance of handling different tasks at the same time, especially for those with high TDR, and further generalize to unseen tasks effectively.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.05092",
        "string": "[Reward Informed Dreamer for Task Generalization in Reinforcement Learning](https://arxiv.org/pdf/2303.05092)"
    },
    "SOCIALGYM 2.0: Simulator for Multi-Agent Social Robot Navigation in Shared Human Spaces": {
        "abstract": "We present SocialGym 2, a multi-agent navigation simulator for social robot research. Our simulator models multiple autonomous agents, replicating real-world dynamics in complex environments, including doorways, hallways, intersections, and roundabouts. Unlike traditional simulators that concentrate on single robots with basic kinematic constraints in open spaces, SocialGym 2 employs multi-agent reinforcement learning (MARL) to develop optimal navigation policies for multiple robots with diverse, dynamic constraints in complex environments. Built on the PettingZoo MARL library and Stable Baselines3 API, SocialGym 2 offers an accessible python interface that integrates with a navigation stack through ROS messaging. SocialGym 2 can be easily installed and is packaged in a docker container, and it provides the capability to swap and evaluate different MARL algorithms, as well as customize observation and reward functions. We also provide scripts to allow users to create their own environments and have conducted benchmarks using various social navigation algorithms, reporting a broad range of social navigation metrics. Projected hosted at: https://amrl.cs.utexas.edu/social_gym/index.html\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.05584",
        "string": "[SOCIALGYM 2.0: Simulator for Multi-Agent Social Robot Navigation in Shared Human Spaces](https://arxiv.org/pdf/2303.05584)"
    },
    "Safe Reinforcement Learning via Probabilistic Logic Shields": {
        "abstract": "Safe Reinforcement learning (Safe RL) aims at learning optimal policies while staying safe. A popular solution to Safe RL is shielding, which uses a logical safety specification to prevent an RL agent from taking unsafe actions. However, traditional shielding techniques are difficult to integrate with continuous, end-to-end deep RL methods. To this end, we introduce Probabilistic Logic Policy Gradient (PLPG). PLPG is a model-based Safe RL technique that uses probabilistic logic programming to model logical safety constraints as differentiable functions. Therefore, PLPG can be seamlessly applied to any policy gradient algorithm while still providing the same convergence guarantees. In our experiments, we show that PLPG learns safer and more rewarding policies compared to other state-of-the-art shielding techniques.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.03226",
        "string": "[Safe Reinforcement Learning via Probabilistic Logic Shields](https://arxiv.org/pdf/2303.03226)"
    },
    "Sample-efficient Real-time Planning with Curiosity Cross-Entropy Method and Contrastive Learning": {
        "abstract": "Model-based reinforcement learning (MBRL) with real-time planning has shown great potential in locomotion and manipulation control tasks. However, the existing planning methods, such as the Cross-Entropy Method (CEM), do not scale well to complex high-dimensional environments. One of the key reasons for underperformance is the lack of exploration, as these planning methods only aim to maximize the cumulative extrinsic reward over the planning horizon. Furthermore, planning inside the compact latent space in the absence of observations makes it challenging to use curiosity-based intrinsic motivation. We propose Curiosity CEM (CCEM), an improved version of the CEM algorithm for encouraging exploration via curiosity. Our proposed method maximizes the sum of state-action Q values over the planning horizon, in which these Q values estimate the future extrinsic and intrinsic reward, hence encouraging reaching novel observations. In addition, our model uses contrastive representation learning to efficiently learn latent representations. Experiments on image-based continuous control tasks from the DeepMind Control suite show that CCEM is by a large margin more sample-efficient than previous MBRL algorithms and compares favorably with the best model-free RL methods.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.03787",
        "string": "[Sample-efficient Real-time Planning with Curiosity Cross-Entropy Method and Contrastive Learning](https://arxiv.org/pdf/2303.03787)"
    },
    "Sampling-based Exploration for Reinforcement Learning of Dexterous Manipulation": {
        "abstract": "In this paper, we present a novel method for achieving dexterous manipulation of complex objects, while simultaneously securing the object without the use of passive support surfaces. We posit that a key difficulty for training such policies in a Reinforcement Learning framework is the difficulty of exploring the problem state space, as the accessible regions of this space form a complex structure along manifolds of a high-dimensional space. To address this challenge, we use two versions of the non-holonomic Rapidly-Exploring Random Trees algorithm; one version is more general, but requires explicit use of the environment's transition function, while the second version uses manipulation-specific kinematic constraints to attain better sample efficiency. In both cases, we use states found via sampling-based exploration to generate reset distributions that enable training control policies under full dynamic constraints via model-free Reinforcement Learning. We show that these policies are effective at manipulation problems of higher difficulty than previously shown, and also transfer effectively to real robots. Videos of the real-hand demonstrations can be found on the project website: https://sbrl.cs.columbia.edu/\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.03486",
        "string": "[Sampling-based Exploration for Reinforcement Learning of Dexterous Manipulation](https://arxiv.org/pdf/2303.03486)"
    },
    "Scalable and Cost-effective Data Flow Analysis for Distributed Software: Algorithms and Applications": {
        "abstract": "More and more distributed software systems are being developed and deployed today. Like other software, distributed software systems also need very strong quality assurance support. Distributed software is often very large/complex, has distributed components, and does not have a global clock. All these characteristics make it very challenging to analyze the information flow of such systems to support the software quality assurance. One challenge is that existing dynamic analysis techniques hardly scale to large distributed software systems in the real world. It is also challenging to develop cost-effective dynamic analysis approaches. There are also applicability and portability challenges for dynamic analysis algorithms/applications of distributed software. My dissertation addresses these challenges via three novel approaches to data flow analysis for distributed software. My first approach is based on measuring interprocess communications to understand distributed software behaviors and predict distributed software quality. Then, I developed a particular approach that can actually pinpoint sensitive information via multi-staged and refinement-based dynamic information flow analysis for distributed software. Finally, I explored dynamic dependence analysis for distributed systems, utilizing reinforcement learning to automatically adjust analysis configurations for scalability and better cost-effectiveness tradeoffs.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.03659",
        "string": "[Scalable and Cost-effective Data Flow Analysis for Distributed Software: Algorithms and Applications](https://arxiv.org/pdf/2303.03659)"
    },
    "Soft Actor-Critic Algorithm with Truly Inequality Constraint": {
        "abstract": "Soft actor-critic (SAC) in reinforcement learning is expected to be one of the next-generation robot control schemes. Its ability to maximize policy entropy would make a robotic controller robust to noise and perturbation, which is useful for real-world robot applications. However, the priority of maximizing the policy entropy is automatically tuned in the current implementation, the rule of which can be interpreted as one for equality constraint, binding the policy entropy into its specified target value. The current SAC is therefore no longer maximize the policy entropy, contrary to our expectation. To resolve this issue in SAC, this paper improves its implementation with a slack variable for appropriately handling the inequality constraint to maximize the policy entropy. In Mujoco and Pybullet simulators, the modified SAC achieved the higher robustness and the more stable learning than before while regularizing the norm of action. In addition, a real-robot variable impedance task was demonstrated for showing the applicability of the modified SAC to real-world robot control.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.04356",
        "string": "[Soft Actor-Critic Algorithm with Truly Inequality Constraint](https://arxiv.org/pdf/2303.04356)"
    },
    "Spatio-Temporal Attention Network for Persistent Monitoring of Multiple Mobile Targets": {
        "abstract": "This work focuses on the persistent monitoring problem, where a set of targets moving based on an unknown model must be monitored by an autonomous mobile robot with a limited sensing range. To keep each target's position estimate as accurate as possible, the robot needs to adaptively plan its path to (re-)visit all the targets and update its belief from measurements collected along the way. In doing so, the main challenge is to strike a balance between exploitation, i.e., re-visiting previously-located targets, and exploration, i.e., finding new targets or re-acquiring lost ones. Encouraged by recent advances in deep reinforcement learning, we introduce an attention-based neural solution to the persistent monitoring problem, where the agent can learn the inter-dependencies between targets, i.e., their spatial and temporal correlations, conditioned on past measurements. This endows the agent with the ability to determine which target, time, and location to attend to across multiple scales, which we show also helps relax the usual limitations of a finite target set. We experimentally demonstrate that our method outperforms other baselines in terms of number of targets visits and average estimation error in complex environments. Finally, we implement and validate our model in a drone-based simulation experiment to monitor mobile ground targets in a high-fidelity simulator.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.06350",
        "string": "[Spatio-Temporal Attention Network for Persistent Monitoring of Multiple Mobile Targets](https://arxiv.org/pdf/2303.06350)"
    },
    "Structured State Space Models for In-Context Reinforcement Learning": {
        "abstract": "Structured state space sequence (S4) models have recently achieved state-of-the-art performance on long-range sequence modeling tasks. These models also have fast inference speeds and parallelisable training, making them potentially useful in many reinforcement learning settings. We propose a modification to a variant of S4 that enables us to initialise and reset the hidden state in parallel, allowing us to tackle reinforcement learning tasks. We show that our modified architecture runs asymptotically faster than Transformers and performs better than LSTM models on a simple memory-based task. Then, by leveraging the model's ability to handle long-range sequences, we achieve strong performance on a challenging meta-learning task in which the agent is given a randomly-sampled continuous control environment, combined with a randomly-sampled linear projection of the environment's observations and actions. Furthermore, we show the resulting model can adapt to out-of-distribution held-out tasks. Overall, the results presented in this paper suggest that the S4 models are a strong contender for the default architecture used for in-context reinforcement learning\n\u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.03982",
        "string": "[Structured State Space Models for In-Context Reinforcement Learning](https://arxiv.org/pdf/2303.03982)"
    },
    "Towards Practical Multi-Robot Hybrid Tasks Allocation for Autonomous Cleaning": {
        "abstract": "Task allocation plays a vital role in multi-robot autonomous cleaning systems, where multiple robots work together to clean a large area. However, there are several problems in relevant research to date. Most current studies mainly focus on deterministic, single-task allocation for cleaning robots, without considering hybrid tasks in uncertain working environments. Moreover, there is a lack of datasets and benchmarks for relevant research. In this paper, we contribute to multi-robot hybrid-task allocation for uncertain autonomous cleaning systems by addressing these problems. First, we model the uncertainties in the cleaning environment via robust optimization and propose a novel robust mixed-integer linear programming model with practical constraints including hybrid cleaning task order and robot's ability. Second, we establish a dataset of 100 instances made from floor plans, each of which has 2D manually-labeled images and a 3D model. Third, we provide comprehensive results on the collected dataset using three traditional optimization approaches and a deep reinforcement learning-based solver. The evaluation results show that our formulation meets the needs of multi-robot cleaning task allocation and the robust solver can protect the system from the worst cases with little additional cost. The benchmark will be available at {https://github.com/iamwangyabin/Multi-robot-Cleaning-Task-Allocation}.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.06531",
        "string": "[Towards Practical Multi-Robot Hybrid Tasks Allocation for Autonomous Cleaning](https://arxiv.org/pdf/2303.06531)"
    },
    "TrafficBots: Towards World Models for Autonomous Driving Simulation and Motion Prediction": {
        "abstract": "Data-driven simulation has become a favorable way to train and test autonomous driving algorithms. The idea of replacing the actual environment with a learned simulator has also been explored in model-based reinforcement learning in the context of world models. In this work, we show data-driven traffic simulation can be formulated as a world model. We present TrafficBots, a multi-agent policy built upon motion prediction and end-to-end driving, and based on TrafficBots we obtain a world model tailored for the planning module of autonomous vehicles. Existing data-driven traffic simulators are lacking configurability and scalability. To generate configurable behaviors, for each agent we introduce a destination as navigational information, and a time-invariant latent personality that specifies the behavioral style. To improve the scalability, we present a new scheme of positional encoding for angles, allowing all agents to share the same vectorized context and the use of an architecture based on dot-product attention. As a result, we can simulate all traffic participants seen in dense urban scenarios. Experiments on the Waymo open motion dataset show TrafficBots can simulate realistic multi-agent behaviors and achieve good performance on the motion prediction task.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.04116",
        "string": "[TrafficBots: Towards World Models for Autonomous Driving Simulation and Motion Prediction](https://arxiv.org/pdf/2303.04116)"
    },
    "Understanding the Synergies between Quality-Diversity and Deep Reinforcement Learning": {
        "abstract": "The synergies between Quality-Diversity (QD) and Deep Reinforcement Learning (RL) have led to powerful hybrid QD-RL algorithms that have shown tremendous potential, and brings the best of both fields. However, only a single deep RL algorithm (TD3) has been used in prior hybrid methods despite notable progress made by other RL algorithms. Additionally, there are fundamental differences in the optimization procedures between QD and RL which would benefit from a more principled approach. We propose Generalized Actor-Critic QD-RL, a unified modular framework for actor-critic deep RL methods in the QD-RL setting. This framework provides a path to study insights from Deep RL in the QD-RL setting, which is an important and efficient way to make progress in QD-RL. We introduce two new algorithms, PGA-ME (SAC) and PGA-ME (DroQ) which apply recent advancements in Deep RL to the QD-RL setting, and solves the humanoid environment which was not possible using existing QD-RL algorithms. However, we also find that not all insights from Deep RL can be effectively translated to QD-RL. Critically, this work also demonstrates that the actor-critic models in QD-RL are generally insufficiently trained and performance gains can be achieved without any additional environment evaluations.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.06164",
        "string": "[Understanding the Synergies between Quality-Diversity and Deep Reinforcement Learning](https://arxiv.org/pdf/2303.06164)"
    },
    "User Retention-oriented Recommendation with Decision Transformer": {
        "abstract": "Improving user retention with reinforcement learning~(RL) has attracted increasing attention due to its significant importance in boosting user engagement. However, training the RL policy from scratch without hurting users' experience is unavoidable due to the requirement of trial-and-error searches. Furthermore, the offline methods, which aim to optimize the policy without online interactions, suffer from the notorious stability problem in value estimation or unbounded variance in counterfactual policy evaluation. To this end, we propose optimizing user retention with Decision Transformer~(DT), which avoids the offline difficulty by translating the RL as an autoregressive problem. However, deploying the DT in recommendation is a non-trivial problem because of the following challenges: (1) deficiency in modeling the numerical reward value; (2) data discrepancy between the policy learning and recommendation generation; (3) unreliable offline performance evaluation. In this work, we, therefore, contribute a series of strategies for tackling the exposed issues. We first articulate an efficient reward prompt by weighted aggregation of meta embeddings for informative reward embedding. Then, we endow a weighted contrastive learning method to solve the discrepancy between training and inference. Furthermore, we design two robust offline metrics to measure user retention. Finally, the significant improvement in the benchmark datasets demonstrates the superiority of the proposed method.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.06347",
        "string": "[User Retention-oriented Recommendation with Decision Transformer](https://arxiv.org/pdf/2303.06347)"
    },
    "Using Memory-Based Learning to Solve Tasks with State-Action Constraints": {
        "abstract": "Tasks where the set of possible actions depend discontinuously on the state pose a significant challenge for current reinforcement learning algorithms. For example, a locked door must be first unlocked, and then the handle turned before the door can be opened. The sequential nature of these tasks makes obtaining final rewards difficult, and transferring information between task variants using continuous learned values such as weights rather than discrete symbols can be inefficient. Our key insight is that agents that act and think symbolically are often more effective in dealing with these tasks. We propose a memory-based learning approach that leverages the symbolic nature of constraints and temporal ordering of actions in these tasks to quickly acquire and transfer high-level information. We evaluate the performance of memory-based learning on both real and simulated tasks with approximately discontinuous constraints between states and actions, and show our method learns to solve these tasks an order of magnitude faster than both model-based and model-free deep reinforcement learning methods.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.04327",
        "string": "[Using Memory-Based Learning to Solve Tasks with State-Action Constraints](https://arxiv.org/pdf/2303.04327)"
    },
    "Value Guided Exploration with Sub-optimal Controllers for Learning Dexterous Manipulation": {
        "abstract": "Recently, reinforcement learning has allowed dexterous manipulation skills with increasing complexity. Nonetheless, learning these skills in simulation still exhibits poor sample-efficiency which stems from the fact these skills are learned from scratch without the benefit of any domain expertise. In this work, we aim to improve the sample-efficiency of learning dexterous in-hand manipulation skills using sub-optimal controllers available via domain knowledge. Our framework optimally queries the sub-optimal controllers and guides exploration toward state-space relevant to the task thereby demonstrating improved sample complexity. We show that our framework allows learning from highly sub-optimal controllers and we are the first to demonstrate learning hard-to-explore finger-gaiting in-hand manipulation skills without the use of an exploratory reset distribution.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.03533",
        "string": "[Value Guided Exploration with Sub-optimal Controllers for Learning Dexterous Manipulation](https://arxiv.org/pdf/2303.03533)"
    },
    "Variance-aware robust reinforcement learning with linear function approximation under heavy-tailed rewards": {
        "abstract": "This paper presents two algorithms, AdaOFUL and VARA, for online sequential decision-making in the presence of heavy-tailed rewards with only finite variances. For linear stochastic bandits, we address the issue of heavy-tailed rewards by modifying the adaptive Huber regression and proposing AdaOFUL. AdaOFUL achieves a state-of-the-art regret bound of $\\widetilde{O}\\big(d\\big(\\sum_{t=1}^T \u03bd_{t}^2\\big)^{1/2}+d\\big)$ as if the rewards were uniformly bounded, where $\u03bd_{t}^2$ is the observed conditional variance of the reward at round $t$, $d$ is the feature dimension, and $\\widetilde{O}(\\cdot)$ hides logarithmic dependence. Building upon AdaOFUL, we propose VARA for linear MDPs, which achieves a tighter variance-aware regret bound of $\\widetilde{O}(d\\sqrt{HG^*K})$. Here, $H$ is the length of episodes, $K$ is the number of episodes, and $G^*$ is a smaller instance-dependent quantity that can be bounded by other instance-dependent quantities when additional structural conditions on the MDP are satisfied. Our regret bound is superior to the current state-of-the-art bounds in three ways: (1) it depends on a tighter instance-dependent quantity and has optimal dependence on $d$ and $H$, (2) we can obtain further instance-dependent bounds of $G^*$ under additional structural conditions on the MDP, and (3) our regret bound is valid even when rewards have only finite variances, achieving a level of generality unmatched by previous works. Overall, our modified adaptive Huber regression algorithm may serve as a useful building block in the design of algorithms for online problems with heavy-tailed rewards.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.05606",
        "string": "[Variance-aware robust reinforcement learning with linear function approximation under heavy-tailed rewards](https://arxiv.org/pdf/2303.05606)"
    },
    "Viewpoint Push Planning for Mapping of Unknown Confined Spaces": {
        "abstract": "Viewpoint planning is an important task in any application where objects or scenes need to be viewed from different angles to achieve sufficient coverage. The mapping of confined spaces such as shelves is an especially challenging task since objects occlude each other and the scene can only be observed from the front, thus with limited possible viewpoints. In this paper, we propose a deep reinforcement learning framework that generates promising views aiming at reducing the map entropy. Additionally, the pipeline extends standard viewpoint planning by predicting adequate minimally invasive push actions to uncover occluded objects and increase the visible space. Using a 2.5D occupancy height map as state representation that can be efficiently updated, our system decides whether to plan a new viewpoint or perform a push. To learn feasible pushes, we use a neural network to sample push candidates on the map and have human experts manually label them to indicate whether the sampled push is a good action to perform. As simulated and real-world experimental results with a robotic arm show, our system is able to significantly increase the mapped space compared to different baselines, while the executed push actions highly benefit the viewpoint planner with only minor changes to the object configuration.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.03126",
        "string": "[Viewpoint Push Planning for Mapping of Unknown Confined Spaces](https://arxiv.org/pdf/2303.03126)"
    },
    "Virtual Reality in Metaverse over Wireless Networks with User-centered Deep Reinforcement Learning": {
        "abstract": "The Metaverse and its promises are fast becoming reality as maturing technologies are empowering the different facets. One of the highlights of the Metaverse is that it offers the possibility for highly immersive and interactive socialization. Virtual reality (VR) technologies are the backbone for the virtual universe within the Metaverse as they enable a hyper-realistic and immersive experience, and especially so in the context of socialization. As the virtual world 3D scenes to be rendered are of high resolution and frame rate, these scenes will be offloaded to an edge server for computation. Besides, the metaverse is user-center by design, and human users are always the core. In this work, we introduce a multi-user VR computation offloading over wireless communication scenario. In addition, we devised a novel user-centered deep reinforcement learning approach to find a near-optimal solution. Extensive experiments demonstrate that our approach can lead to remarkable results under various requirements and constraints.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.04349",
        "string": "[Virtual Reality in Metaverse over Wireless Networks with User-centered Deep Reinforcement Learning](https://arxiv.org/pdf/2303.04349)"
    },
    "Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles": {
        "abstract": "In this paper, we focus on a novel optimization problem in which the objective function is a black-box and can only be evaluated through a ranking oracle. This problem is common in real-world applications, particularly in cases where the function is assessed by human judges. Reinforcement Learning with Human Feedback (RLHF) is a prominent example of such an application, which is adopted by the recent works \\cite{ouyang2022training,liu2023languages,chatgpt,bai2022training} to improve the quality of Large Language Models (LLMs) with human guidance. We propose ZO-RankSGD, a first-of-its-kind zeroth-order optimization algorithm, to solve this optimization problem with a theoretical guarantee. Specifically, our algorithm employs a new rank-based random estimator for the descent direction and is proven to converge to a stationary point. ZO-RankSGD can also be directly applied to the policy search problem in reinforcement learning when only a ranking oracle of the episode reward is available. This makes ZO-RankSGD a promising alternative to existing RLHF methods, as it optimizes in an online fashion and thus can work without any pre-collected data. Furthermore, we demonstrate the effectiveness of ZO-RankSGD in a novel application: improving the quality of images generated by a diffusion generative model with human ranking feedback. Throughout experiments, we found that ZO-RankSGD can significantly enhance the detail of generated images with only a few rounds of human feedback. Overall, our work advances the field of zeroth-order optimization by addressing the problem of optimizing functions with only ranking feedback, and offers an effective approach for aligning human and machine intentions in a wide range of domains. Our code is released here \\url{https://github.com/TZW1998/Taming-Stable-Diffusion-with-Human-Ranking-Feedback}.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.03751",
        "string": "[Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles](https://arxiv.org/pdf/2303.03751)"
    },
    "adaPARL: Adaptive Privacy-Aware Reinforcement Learning for Sequential-Decision Making Human-in-the-Loop Systems": {
        "abstract": "Reinforcement learning (RL) presents numerous benefits compared to rule-based approaches in various applications. Privacy concerns have grown with the widespread use of RL trained with privacy-sensitive data in IoT devices, especially for human-in-the-loop systems. On the one hand, RL methods enhance the user experience by trying to adapt to the highly dynamic nature of humans. On the other hand, trained policies can leak the user's private information. Recent attention has been drawn to designing privacy-aware RL algorithms while maintaining an acceptable system utility. A central challenge in designing privacy-aware RL, especially for human-in-the-loop systems, is that humans have intrinsic variability and their preferences and behavior evolve. The effect of one privacy leak mitigation can be different for the same human or across different humans over time. Hence, we can not design one fixed model for privacy-aware RL that fits all. To that end, we propose adaPARL, an adaptive approach for privacy-aware RL, especially for human-in-the-loop IoT systems. adaPARL provides a personalized privacy-utility trade-off depending on human behavior and preference. We validate the proposed adaPARL on two IoT applications, namely (i) Human-in-the-Loop Smart Home and (ii) Human-in-the-Loop Virtual Reality (VR) Smart Classroom. Results obtained on these two applications validate the generality of adaPARL and its ability to provide a personalized privacy-utility trade-off. On average, for the first application, adaPARL improves the utility by $57\\%$ over the baseline and by $43\\%$ over randomization. adaPARL also reduces the privacy leak by $23\\%$ on average. For the second application, adaPARL decreases the privacy leak to $44\\%$ before the utility drops by $15\\%$.\n        \u25b3 Less",
        "link": "https://arxiv.org/pdf/2303.04257",
        "string": "[adaPARL: Adaptive Privacy-Aware Reinforcement Learning for Sequential-Decision Making Human-in-the-Loop Systems](https://arxiv.org/pdf/2303.04257)"
    }
}